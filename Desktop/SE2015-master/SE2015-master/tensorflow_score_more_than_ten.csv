Title,Body,Labels
Meaning of inter_op_parallelism_threads and intra_op_parallelism_threads,"<p>Can somebody please explain the following TensorFlow terms</p>

<ol>
<li><p><code>inter_op_parallelism_threads</code></p></li>
<li><p><code>intra_op_parallelism_threads</code></p></li>
</ol>

<p>or, please, provide links to the right source of explanation. </p>

<p>I have conducted a few tests by changing the parameters, but the results have not been consistent to arrive at a conclusion.</p>
",
Can inception model be used for object counting in an image?,"<p>I have already gone through the image classification part in Inception model, but I require to count the objects in the image. </p>

<p>Considering the flowers data-set, one image can have multiple instances of a flower, so how can I get that count?</p>
",Choosing a model: Lack of knowledge
How to compile Tensorflow with SSE4.2 and AVX instructions?,"<p>This is the message received from running a script to check if Tensorflow is working:</p>

<pre><code>I tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcurand.so.8.0 locally
W tensorflow/core/platform/cpu_feature_guard.cc:95] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:95] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:910] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
</code></pre>

<p>I noticed that it has mentioned SSE4.2 and AVX,</p>

<p>1) What are SSE4.2 and AVX?</p>

<p>2) How do these SSE4.2 and AVX improve CPU computations for Tensorflow tasks.</p>

<p>3) How to make Tensorflow compile using the two libraries?</p>
",Related to library: Lack of knowledge
Tensorflow mean squared error loss function,"<p>I have seen a few different mean squared error loss functions in various posts for regression models in Tensorflow:</p>

<pre><code>loss = tf.reduce_sum(tf.pow(prediction - Y,2))/(n_instances)
loss = tf.reduce_mean(tf.squared_difference(prediction, Y))
loss = tf.nn.l2_loss(prediction - Y)
</code></pre>

<p>What are the differences between these?</p>
",
What is the meaning of the word logits in TensorFlow?,"<p>In the following TensorFlow function, we must feed the activation of artificial neurons in the final layer. That I understand. But I don't understand why it is called logits? Isn't that a mathematical function? </p>

<pre><code>loss_function = tf.nn.softmax_cross_entropy_with_logits(
     logits = last_layer,
     labels = target_output
)
</code></pre>
",Related to library: Lack of knowledge
How to print the value of a Tensor object in TensorFlow?,"<p>I have been using the introductory example of matrix multiplication in TensorFlow.</p>

<pre><code>matrix1 = tf.constant([[3., 3.]])
matrix2 = tf.constant([[2.],[2.]])
product = tf.matmul(matrix1, matrix2)
</code></pre>

<p>When I print the product, it is displaying it as a <code>Tensor</code> object:</p>

<pre><code>&lt;tensorflow.python.framework.ops.Tensor object at 0x10470fcd0&gt;
</code></pre>

<p>But how do I know the value of <code>product</code>?</p>

<p>The following doesn't help:</p>

<pre><code>print product
Tensor(""MatMul:0"", shape=TensorShape([Dimension(1), Dimension(1)]), dtype=float32)
</code></pre>

<p>I know that graphs run on <code>Sessions</code>, but isn't there any way I can check the output of a <code>Tensor</code> object without running the graph in a <code>session</code>?</p>
",Related to library: Lack of knowledge
How to install tensorboard,"<p>Maybe this is a stupid question, but how do I install tensorboard. 
The <a href=""http://tensorflow.org/how_tos/summaries_and_tensorboard/index.md"" rel=""nofollow noreferrer"">documentation</a> says ""If you have pip installed tensorboard"".</p>
",
TensorFlow on 32-bit Linux?,"<p>Is there a version of TensorFlow for 32-bit Linux?  I only see the 64-bit wheel available, and didn't find anything about it on the site.</p>
",
How do I start tensorflow docker jupyter notebook,"<p>I've installed the tensorflow docker container on an ubuntu machine.  The tensorflow docker <a href=""http://www.tensorflow.org/get_started/os_setup.md"" rel=""noreferrer"">setup instructions</a> specify:</p>

<pre><code>docker run -it b.gcr.io/tensorflow/tensorflow
</code></pre>

<p>This puts me into the docker container terminal, and I can run python and execute the Hello World example.  I can also manually run .\run_jupyter.sh to start the jupyter notebook. However, I can't reach the notebook from host.</p>

<p>How do I start the jupyter notebook such that I can use the notebook from the host machine? Ideally I would like to use docker to launch the container and start jupyter in a single command.</p>
",Related to library: Lack of knowledge
TensorFlow - best way to implement weight constraints,"<p>Suppose we have weights</p>

<pre><code>x = tf.Variable(np.random.random((5,10)))
cost = ...
</code></pre>

<p>And we use the GD optimizer:</p>

<pre><code>upds = tf.train.GradientDescentOptimizer(lr).minimize(cost)
session.run(upds)
</code></pre>

<p>How can we implement for example non-negativity on weights?</p>

<p>I tried clipping them:</p>

<pre><code>upds = tf.train.GradientDescentOptimizer(lr).minimize(cost)
session.run(upds)
session.run(tf.assign(x, tf.clip_by_value(x, 0, np.infty)))
</code></pre>

<p>But this slows down my training by a factor of 50.</p>

<p>Does anybody know a good way to implement such constraints on the weights in TensorFlow?</p>

<p>P.S.: in the equivalent Theano algorithm, I had</p>

<pre><code>T.clip(x, 0, np.infty)
</code></pre>

<p>and it ran smoothly.</p>
",Choosing a model: Lack of knowledge
Use attribute and target matrices for TensorFlow Linear Regression Python,"<p>I'm trying to follow <a href=""http://www.tensorflow.org/tutorials/mnist/beginners/index.md"" rel=""noreferrer"">this tutorial</a>.</p>

<p>TensorFlow just came out and I'm really trying to understand it.  I'm familiar with <em>penalized linear regression</em> like Lasso, Ridge, and ElasticNet and its usage in <code>scikit-learn</code>.  </p>

<p>For <code>scikit-learn</code> Lasso regression, all I need to input into the regression algorithm is <code>DF_X</code> [an M x N dimensional attribute matrix (pd.DataFrame)] and <code>SR_y</code> [an M dimensional target vector (pd.Series)].  The <code>Variable</code> structure in TensorFlow is a bit new to me and I'm not sure how to structure my input data into what it wants. </p>

<p>It seems as if softmax regression is for classification.  <strong>How can I restructure my <code>DF_X</code> (M x N attribute matrix) and <code>SR_y</code> (M dimensional target vector) to input into <code>tensorflow</code> for linear regression?</strong> </p>

<p>My current method for doing a Linear Regression uses pandas, numpy, and sklearn and it's shown below. I think this question will be really helpful for people getting familiar with TensorFlow:</p>

<pre><code>#!/usr/bin/python
import pandas as pd
import numpy as np
import tensorflow as tf
from sklearn.linear_model import LassoCV

#Create DataFrames for attribute and target matrices
DF_X = pd.DataFrame(np.array([[0,0,1],[2,3,1],[4,5,1],[3,4,1]]),columns=[""att1"",""att2"",""att3""],index=[""s1"",""s2"",""s3"",""s4""])
SR_y = pd.Series(np.array([3,2,5,8]),index=[""s1"",""s2"",""s3"",""s4""],name=""target"")

print DF_X
#att1  att2  att3
#s1     0     0     1
#s2     2     3     1
#s3     4     5     1
#s4     3     4     1

print SR_y
#s1    3
#s2    2
#s3    5
#s4    8
#Name: target, dtype: int64

#Create Linear Model (Lasso Regression)
model = LassoCV()
model.fit(DF_X,SR_y)

print model
#LassoCV(alphas=None, copy_X=True, cv=None, eps=0.001, fit_intercept=True,
#max_iter=1000, n_alphas=100, n_jobs=1, normalize=False, positive=False,
#precompute='auto', random_state=None, selection='cyclic', tol=0.0001,
#verbose=False)

print model.coef_
#[ 0.         0.3833346  0.       ]
</code></pre>
",Choosing a model: Lack of knowledge
Error when building seq2seq model with tensorflow,"<p>I'm trying to understand the seq2seq models defined in seq2seq.py in tensorflow. I use bits of code I copy from the translate.py example that comes with tensorflow. I keep getting the same error and really do not understand where it comes from.</p>

<p>A minimal code example to reproduce the error:</p>

<pre><code>import tensorflow as tf
from tensorflow.models.rnn import rnn_cell
from tensorflow.models.rnn import seq2seq

encoder_inputs = []
decoder_inputs = []
for i in xrange(350):  
    encoder_inputs.append(tf.placeholder(tf.int32, shape=[None],
                                              name=""encoder{0}"".format(i)))

for i in xrange(45):
    decoder_inputs.append(tf.placeholder(tf.int32, shape=[None],
                                         name=""decoder{0}"".format(i)))

model = seq2seq.basic_rnn_seq2seq(encoder_inputs,
                                  decoder_inputs,rnn_cell.BasicLSTMCell(512))
</code></pre>

<p>The error I get when evaluating the last line (I evaluated it interactively in the python interpreter):</p>

<pre><code>    &gt;&gt;&gt;  Traceback (most recent call last):
      File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
      File ""/tmp/py1053173el"", line 12, in &lt;module&gt;
      File ""/usr/local/lib/python2.7/dist-packages/tensorflow/models/rnn/seq2seq.py"", line 82, in basic_rnn_seq2seq
        _, enc_states = rnn.rnn(cell, encoder_inputs, dtype=dtype)
      File ""/usr/local/lib/python2.7/dist-packages/tensorflow/models/rnn/rnn.py"", line 85, in rnn
        output_state = cell(input_, state)
      File ""/usr/local/lib/python2.7/dist-packages/tensorflow/models/rnn/rnn_cell.py"", line 161, in __call__
        concat = linear.linear([inputs, h], 4 * self._num_units, True)
      File ""/usr/local/lib/python2.7/dist-packages/tensorflow/models/rnn/linear.py"", line 32, in linear
        raise ValueError(""Linear is expecting 2D arguments: %s"" % str(shapes))
    ValueError: Linear is expecting 2D arguments: [[None], [None, 512]]
</code></pre>

<p>I suspect the error comes from my side :)
On a sidenote. The documentation and the tutorials are really great but the example code for the sequence to sequence model (the english to french translation example) is quite dense. You also have to jump a lot between files to understand what's going on. Me at least got lost several times in the code.</p>

<p>A minimal example (perhaps on some toy data) of constructing and training a basic seq2seq model would really be helpful here. Somebody know if this already exist somewhere?</p>

<p><strong>EDIT</strong>
I have fixed the code above according @Ishamael suggestions (meaning, no errors returns) (see below), but there are still some things not clear in this fixed version. My input is a sequence of vectors of length 2 of real valued values. And my output is a sequence of binary vectors of length 22. Should my tf.placeholder code not look like the following? <strong>(EDIT yes)</strong></p>

<pre><code>tf.placeholder(tf.float32, shape=[None,2],name=""encoder{0}"".format(i))
tf.placeholder(tf.float32, shape=[None,22],name=""encoder{0}"".format(i))
</code></pre>

<p>I also had to change tf.int32 to tf.float32 above. Since my output is binary. Should this not be tf.int32 for the tf.placeholder of my decoder? But tensorflow complains again if I do this. I'm not sure what the reasoning is behind this.</p>

<p>The size of my hidden layer is 512 here.</p>

<p>the complete fixed code</p>

<pre><code>import tensorflow as tf
from tensorflow.models.rnn import rnn_cell
from tensorflow.models.rnn import seq2seq

encoder_inputs = []
decoder_inputs = []
for i in xrange(350):  
    encoder_inputs.append(tf.placeholder(tf.float32, shape=[None,512],
                                          name=""encoder{0}"".format(i)))

for i in xrange(45):
    decoder_inputs.append(tf.placeholder(tf.float32, shape=[None,512],
                                         name=""decoder{0}"".format(i)))

model = seq2seq.basic_rnn_seq2seq(encoder_inputs,
                                  decoder_inputs,rnn_cell.BasicLSTMCell(512))
</code></pre>
",Data Preparation: Shape mismatch
Remove nodes from graph or reset entire default graph,"<p>When working with the default global graph, is it possible to remove nodes after they've been added, or alternatively to reset the default graph to empty? When working with TF interactively in IPython, I find myself having to restart the kernel repeatedly. I would like to be able to experiment with graphs more easily if possible.</p>
",Related to library: Lack of knowledge
Tensorflow indexing with boolean tensor,"<p>In numpy, with two arrays of the same shape, <code>x</code> and <code>y</code>, it is possible to do slices like this <code>y[x &gt; 1]</code>. How do you achieve the same result in tensorflow? <code>y[tf.greater(x, 1)]</code> doesn't work and <code>tf.slice</code> doesn't support anything like this either. Is there a way to index with a boolean tensor right now or is that currently unsupported?</p>
",Related to library: Lack of knowledge
Hyperparameter optimization for Deep Learning Structures using Bayesian Optimization,"<p>I have constructed a CLDNN (Convolutional, LSTM, Deep Neural Network) structure for raw signal classification task.</p>

<p>Each training epoch runs for about 90 seconds and the hyperparameters seems to be very difficult to optimize.</p>

<p>I have been research various ways to optimize the hyperparameters (e.g. random or grid search) and found out about Bayesian Optimization.</p>

<p>Although I am still not fully understanding the optimization algorithm, I feed like it will help me greatly.</p>

<p>I would like to ask few questions regarding the optimization task.</p>

<ol>
<li>How do I set up the Bayesian Optimization with regards to a deep network?(What is the cost function we are trying to optimize?)</li>
<li>What is the function I am trying to optimize? Is it the cost of the validation set after N epochs?",Hyper parameter tuning
ImportError: libcudnn when running a TensorFlow program,"<p>I encountered the following error when trying to run a TensorFlow program:</p>

<pre>
ImportError: libcudnn.<i>Version</i>: cannot open shared object file: No such file or director
</pre>
",Related to library: Bugs
Miminum requirements for Google tensorflow image classifier,"<p>We are planning to build image classifiers using Google Tensorflow.</p>

<p>I wonder what are the minimum and what are the optimum requirements to train a custom image classifier using a convolutional deep neural network?</p>

<p>The questions are specifically:</p>

<ul>
<li>how many images per class should be provided at a minimum?</li>
<li>do we need to appx. provide the same amount of training images per class or can the amount per class be disparate?</li>
<li>what is the impact of wrong image data in the training data? E.g. 500 images of a tennis shoe and 50 of other shoes.  </li>
<li>is it possible to train a classifier with much more classes than the recently published inception-v3 model? Let's say: 30.000.</li>
</ul>
",Training/Choosing a model: Lack of knowledge
TypeError: only integer scalar arrays can be converted to a scalar index,"<p>I am trying a simple demo code of tensorflow from <a href=""https://github.com/llSourcell/tensorflow_demo"" rel=""noreferrer"">github link</a>.<br>
I'm currently using python version 3.5.2 <br><br></p>

<pre><code>Z:\downloads\tensorflow_demo-master\tensorflow_demo-master&gt;py Python
3.5.2 (v3.5.2:4def2a2901a5, Jun 25 2016, 22:18:55) [MSC v.1900 64 bit (AMD64)] on win32&lt;br&gt; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
</code></pre>

<p>I ran into this error when I tried board.py in command-line. I have installed all the dependencies that are required for this to run.</p>

<pre><code>def _read32(bytestream):
    dt = numpy.dtype(numpy.uint32).newbyteorder('&gt;')
    return numpy.frombuffer(bytestream.read(4), dtype=dt)

def extract_images(filename):
    """"""Extract the images into a 4D uint8 numpy array [index, y, x, depth].""""""
    print('Extracting', filename)
    with gzip.open(filename) as bytestream:
        magic = _read32(bytestream)
        if magic != 2051:
            raise ValueError(
                'Invalid magic number %d in MNIST image file: %s' %
                (magic, filename))
        num_images = _read32(bytestream)
        rows = _read32(bytestream)
        cols = _read32(bytestream)
        buf = bytestream.read(rows * cols * num_images)
        data = numpy.frombuffer(buf, dtype=numpy.uint8)
        data = data.reshape(num_images, rows, cols, 1)
    return data

Z:\downloads\tensorflow_demo-master\tensorflow_demo-master&gt;py board.py
Extracting  Z:/downloads/MNIST dataset\train-images-idx3-ubyte.gz
Traceback (most recent call last):  
File ""board.py"", line 3, in &lt;module&gt;
    mnist = input_data.read_data_sets(r'Z:/downloads/MNIST dataset', one_hot=True)  
File ""Z:\downloads\tensorflow_demo-master\tensorflow_demo-master\input_data.py"", line 150, in read_data_sets
    train_images = extract_images(local_file) 
File ""Z:\downloads\tensorflow_demo-master\tensorflow_demo-master\input_data.py"", line 40, in extract_images
    buf = bytestream.read(rows * cols * num_images) 
File ""C:\Users\surak\AppData\Local\Programs\Python\Python35\lib\gzip.py"", line 274, in read
    return self._buffer.read(size)
TypeError: only integer scalar arrays can be converted to a scalar index
</code></pre>
",Data Preparation: Shape mismatch
What does tf.gfile do in TensorFlow?,"<p>I've seen people using several functions from <code>tf.gfile</code> such as <code>tf.gfile.GFile</code> or <code>tf.gfile.Exists</code>. I have the idea that <code>tf.gfile</code> deals with files. However, I haven't been able to find the official documentation to see what else it offers. </p>

<p>It'd be great if you could help me with it.</p>
",Related to library: Lack of knowledge
Theano with Keras on Raspberry Pi,"<p>I am trying to get Theano to run with Keras on a Raspberry Pi 3 (B) without success. I tried Ubuntu MATE and Raspbian as operating systems, without success. To install Theano and Keras, I have taken following steps:</p>

<ol>
<li>Install miniconda (armv7 distribution)</li>
<li>Install all Theano dependencies (as shown <a href=""http://deeplearning.net/software/theano/install_ubuntu.html"" rel=""noreferrer"">here</a>) through Conda (if possible), <code>pip</code> and <code>apt-get</code></li>
<li>Install Theano </li>
<li>Install Keras</li>
</ol>

<p>The aforementioned steps work without any issues. In the next step, I built a little test script (test.py) which loads an already built model via </p>

<pre><code>from keras.models import load_model
model = load_model('model.hdf5')
</code></pre>

<p>When the model is being loaded, I get the following error</p>

<pre><code>Segmentation fault (core dumped)
</code></pre>

<p>Then I tried to investigate the issue further, following this answer on SO (<a href=""https://stackoverflow.com/questions/10035541/what-causes-a-python-segmentation-fault"">What causes a Python segmentation fault?</a>):</p>

<pre><code>gdb python
&gt; run test.py
</code></pre>

<p>When I run this I get:</p>

<pre><code>Program received SIGSEV, Segmentation fault.
0x76fd9822 in ?? () from /lib/ld-linux-armhf.so.3
</code></pre>

<p>In the next step I ran in the gdb shell:</p>

<pre><code>&gt; backtrace
</code></pre>

<p>and got</p>

<pre><code>#0  0x76fd9822 in ?? () from /lib/ld-linux-armhf.so.3
#1  0x76fd983a in ?? () from /lib/ld-linux-armhf.so.3
</code></pre>

<p>this is the point where I don't know any further and I would like to ask, if anyone could point me into a direction on how to fix this issue and get keras + theano to run on a Raspberry Pi.</p>

<p>(I have also tried TensorFlow as an alternative, but getting the same issue)</p>

<p>Thanks a lot.</p>

<hr>

<p>EDIT</p>

<p>I have done some more investigations. If I <a href=""https://github.com/samjabrahams/tensorflow-on-raspberry-pi"" rel=""noreferrer"">run Keras with TensorFlow</a> the problem seems to change a little bit. I ran gdb again, but the error happens now in numpy, especially in libopenblas.so.0</p>

<pre><code>Program received signal SIGSEV, Segmentation fault.
0x75ead7cc in inner_thread()
from /home/&lt;path&gt;/numpy/core/../../../../libopenblas.so.0
</code></pre>

<p>Does this help?</p>

<hr>

<p>EDIT 2</p>

<p>I have installed everything without using Miniconda and Keras works now with TensorFlow (but not with Theano yet). </p>
",Related to library: Installation
Tensorflow: How can I assign numpy pre-trained weights to subsections of graph?,"<p>This is a simple thing which I just couldn't figure out how to do.</p>

<p>I converted a pre-trained VGG caffe model to tensorflow using the github code from <a href=""https://github.com/ethereon/caffe-tensorflow"" rel=""noreferrer"">https://github.com/ethereon/caffe-tensorflow</a> and saved it to vgg16.npy...</p>

<p>I then load the network to my sess default session as ""net"" using:</p>

<pre><code>images = tf.placeholder(tf.float32, [1, 224, 224, 3])
net = VGGNet_xavier({'data': images, 'label' : 1}) 
with tf.Session() as sess:
  net.load(""vgg16.npy"", sess) 
</code></pre>

<p>After net.load, I get a graph with a list of tensors. I can access individual tensors per layer using net.layers['conv1_1']... to get weights and biases for the first VGG convolutional layer, etc.</p>

<p>Now suppose that I make another graph that has as its first layer ""h_conv1_b"":</p>

<pre><code>  W_conv1_b = weight_variable([3,3,3,64])
  b_conv1_b = bias_variable([64])
  h_conv1_b = tf.nn.relu(conv2d(im_batch, W_conv1_b) + b_conv1_b)
</code></pre>

<p>My question is -- how do you get to assign the pre-trained weights from net.layers['conv1_1'] to h_conv1_b ?? (both are now tensors)</p>
",Choosing a model: Lack of knowledge
Using pre-trained inception_resnet_v2 with Tensorflow,"<p>I have been trying to use the pre-trained inception_resnet_v2 model released by Google. I am using their model definition(<a href=""https://github.com/tensorflow/models/blob/master/slim/nets/inception_resnet_v2.py"" rel=""noreferrer"">https://github.com/tensorflow/models/blob/master/slim/nets/inception_resnet_v2.py</a>) and given checkpoint(<a href=""http://download.tensorflow.org/models/inception_resnet_v2_2016_08_30.tar.gz"" rel=""noreferrer"">http://download.tensorflow.org/models/inception_resnet_v2_2016_08_30.tar.gz</a>) to load the model in tensorflow as below [Download a extract the checkpoint file and download sample images dog.jpg and panda.jpg to test this code]-</p>

<pre><code>import tensorflow as tf
slim = tf.contrib.slim
from PIL import Image
from inception_resnet_v2 import *
import numpy as np

checkpoint_file = 'inception_resnet_v2_2016_08_30.ckpt'
sample_images = ['dog.jpg', 'panda.jpg']
#Load the model
sess = tf.Session()
arg_scope = inception_resnet_v2_arg_scope()
with slim.arg_scope(arg_scope):
  logits, end_points = inception_resnet_v2(input_tensor, is_training=False)
saver = tf.train.Saver()
saver.restore(sess, checkpoint_file)
for image in sample_images:
  im = Image.open(image).resize((299,299))
  im = np.array(im)
  im = im.reshape(-1,299,299,3)
  predict_values, logit_values = sess.run([end_points['Predictions'], logits], feed_dict={input_tensor: im})
  print (np.max(predict_values), np.max(logit_values))
  print (np.argmax(predict_values), np.argmax(logit_values))
</code></pre>

<p>However, the results from this model code does not give the expected results (class no 918 is predicted irrespective of the input image). Can someone help me understand where I am going wrong?</p>

<p>Thanks!</p>
",Evaluation: Accuracy
Why do we need TensorFlow tf.Graph?,"<p>What is the purpose of:</p>

<pre><code>with tf.Graph().as_default()
</code></pre>

<p>I have some tensorflow code that uses the above.
However, the code has only one graph, so why do we need this?</p>
",Related to library: Lack of knowledge
Ordering of batch normalization and dropout in TensorFlow?,"<p>When using batch normalization and dropout in TensorFlow (specifically using the contrib.layers) do I need to be worried about the ordering?</p>

<p>It seems possible that if I use dropout followed immediately by batch normalization there might be trouble. For example, if the shift in the batch normalization trains to the larger scale numbers of the training outputs, but then that same shift is applied to the smaller (due to the compensation for having more outputs) scale numbers without dropout during testing, then that shift may be off. Does the TensorFlow batch normalization layer automatically compensate for this? Or does this not happen for some reason I'm missing?</p>

<p>Also, are there other pitfalls to look out for in when using these two together? For example, assuming I'm using them in the correct order in regards to the above (assuming there <em>is</em> a correct order), could there be trouble with using both batch normalization and dropout on multiple successive layers? I don't immediately see a problem with that, but I might be missing something.</p>

<p>Thank you much!</p>

<p><strong>UPDATE:</strong></p>

<p>An experimental test <em>seems</em> to suggest that ordering <em>does</em> matter. I ran the same network twice with only the batch norm and dropout reverse. When the dropout is before the batch norm, validation loss seems to be going up as training loss is going down. They're both going down in the other case. But in my case the movements are slow, so things may change after more training and it's just a single test. A more definitive and informed answer would still be appreciated.</p>
",Training
Whats the difference between tensorflow dynamic_rnn and rnn?,"<p>There are several classes in <code>tf.nn</code> that relate to RNNs. In the examples I find on the net, <code>tf.nn.dynamic_rnn</code> and <code>tf.nn.rnn</code> seem to be used interchangeably or at least I cannot seem to figure out why one is used in place of the other. What is the difference?</p>
",Related to library: Lack of knowledge
Training broke with ResourceExausted error,"<p>I am new to tensorflow and Machine Learning. Recently I am working on a model. My model is like below,</p>

<ol>
<li><p>Character level Embedding Vector -> Embedding lookup -> LSTM1</p></li>
<li><p>Word level Embedding Vector->Embedding lookup -> LSTM2 </p></li>
<li><p>[LSTM1+LSTM2] -> single layer MLP-> softmax layer</p></li>
<li><p>[LSTM1+LSTM2] -> Single layer MLP-> WGAN discriminator</p></li>
<li><p>Code of he rnn model </p></li>
</ol>

<p>while I'm working on this model I got the following error. I thought My batch is too big. Thus I tried to reduce the batch size from 20 to 10 but it doesn't work. </p>

<blockquote>
  <p>ResourceExhaustedError (see above for traceback): OOM when allocating
  tensor with shape[24760,100]   [[Node:
  chars/bidirectional_rnn/bw/bw/while/bw/lstm_cell/split =
  Split[T=DT_FLOAT, num_split=4,
  _device=""/job:localhost/replica:0/task:0/device:GPU:0""](gradients_2/Add_3/y,
  chars/bidirectional_rnn/bw/bw/while/bw/lstm_cell/BiasAdd)]]    [[Node:
  bi-lstm/bidirectional_rnn/bw/bw/stack/_167 =
  _Recvclient_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"",
  send_device=""/job:localhost/replica:0/task:0/device:GPU:0"",
  send_device_incarnation=1,
  tensor_name=""edge_636_bi-lstm/bidirectional_rnn/bw/bw/stack"",
  tensor_type=DT_INT32,
  _device=""/job:localhost/replica:0/task:0/device:CPU:0""]]</p>
</blockquote>

<p>tensor with <strong><em>shape[24760,100]</em></strong> means 2476000*32/8*1024*1024 = 9.44519043 MB memory. I am running the code on a titan X(11 GB) GPU. What could go wrong? Why this type of error occurred?</p>

<p><strong>* Extra info *</strong>: the size of the LSTM1 is 100. for bidirectional LSTM it becomes 200.
The size of the LSTM2 is 300. For Bidirectional LSTM it becomes 600.</p>

<p><strong>*Note *</strong>: The error occurred after 32 epoch. My question is why after 32 epoch there is an error. Why not at the initial epoch.</p>
",Training: Error/Exception
How to apply Drop Out in Tensorflow to improve the accuracy of neural network?,"<p>Drop-Out is regularization techniques. And I want to apply it to notMNIST data to reduce over-fitting to finish my Udacity Deep Learning Course Assignment.I have read the <a href=""https://www.tensorflow.org/versions/r0.12/tutorials/mnist/pros/index.html"" rel=""noreferrer"">docs of tensorflow</a> on how to call the <code>tf.nn.dropout</code>. And here is my code</p>

<pre class=""lang-py prettyprint-override""><code># before proceeding further.
from __future__ import print_function
import numpy as np  
import tensorflow as tf
from six.moves import cPickle as pickle


pickle_file = 'notMNIST.pickle'

with open(pickle_file, 'rb') as f:
    save = pickle.load(f)
    train_dataset = save['train_dataset']
    train_labels = save['train_labels']
    valid_dataset = save['valid_dataset']
    valid_labels = save['valid_labels']
    test_dataset = save['test_dataset']
    test_labels = save['test_labels']
    del save  # hint to help gc free up memory
    print('Training set', train_dataset.shape, train_labels.shape)
    print('Validation set', valid_dataset.shape, valid_labels.shape)
    print('Test set', test_dataset.shape, test_labels.shape)


image_size = 28
num_labels = 10

def reformat(dataset, labels):
    dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)
    # Map 1 to [0.0, 1.0, 0.0 ...], 2 to [0.0, 0.0, 1.0 ...]
    labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)
    return dataset, labels

    train_dataset, train_labels = reformat(train_dataset, train_labels)
    valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)
    test_dataset, test_labels = reformat(test_dataset, test_labels)
    print('Training set', train_dataset.shape, train_labels.shape)
    print('Validation set', valid_dataset.shape, valid_labels.shape)
    print('Test set', test_dataset.shape, test_labels.shape)

    def accuracy(predictions, labels):
        return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))  / predictions.shape[0])


# ReLU neuron
# param
training_epochs = 30
batch_size = 521
display_step = 1
n_input = 784 # img shape: 28*28
n_classes = 10 # MNIST total classes (0-9 digits)

# hyper-parameter
n_hidden_1 = 256 
learning_rate = 0.05
lambda_term = 0.01


graph = tf.Graph()
with graph.as_default():
    # init weights
    weights_hiden =  tf.Variable(tf.random_normal([n_input, n_hidden_1], stddev=np.sqrt(n_input)))
    weights_out = tf.Variable(tf.random_normal([n_hidden_1, n_classes], stddev=np.sqrt(n_hidden_1)))

    biases_hidden = tf.Variable(tf.random_normal([n_hidden_1]))
    biases_out = tf.Variable(tf.random_normal([n_classes]))

    x = tf.placeholder(""float"", [None, n_input])
    y = tf.placeholder(""float"", [None, n_classes])

    def model(x, weights_hiden, weights_out, biases_hidden, biases_out):
        # hidden layer with RELU activation
        layer_1 = tf.nn.relu(tf.add(tf.matmul(x, weights_hiden), biases_hidden))
        # apply DropOut to hidden layer
        keep_prob = tf.placeholder(tf.float32)  # DROP-OUT here
        drop_out = tf.nn.dropout(layer_1, keep_prob)  # DROP-OUT here
        # output layer with linear activation
        out_layer = tf.matmul(layer_1, weights_out) + biases_out
        return out_layer

    # Construct model
    pred = model(x, weights_hiden, weights_out, biases_hidden, biases_out)

    # Define loss and optimizer
    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y) +
                          lambda_term * tf.nn.l2_loss(weights_hiden) + 
                          lambda_term * tf.nn.l2_loss(weights_out) +
                          lambda_term * tf.nn.l2_loss(biases_hidden) + 
                          lambda_term * tf.nn.l2_loss(biases_out))
    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)


# run the graph
with tf.Session(graph=graph) as sess:
    tf.initialize_all_variables().run()
    print('Initialized')
    # Training cycle
    for epoch in range(training_epochs):
        avg_cost = 0.
        total_batch = int(train_dataset.shape[0]/batch_size)
        # Loop over all batches
        for i in range(total_batch):
            batch_x = train_dataset[(i*batch_size):((i*batch_size) + batch_size), :]
            batch_y = train_labels[(i*batch_size):((i*batch_size) + batch_size), :]
            # Run optimization op (backprop) and cost op (to get loss value)
            _, c = sess.run([optimizer, cost], feed_dict={x: batch_x, y: batch_y})
            # Compute average loss
            avg_cost += c / total_batch
        # Display logs per epoch step
        if epoch % display_step == 0:
            print(""Epoch:"", '%04d' % (epoch+1), ""cost="", ""{:.9f}"".format(avg_cost))
    print(""Optimization Finished!"")

    # Test model
    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))
    # Calculate accuracy
    accuracy = tf.reduce_mean(tf.cast(correct_prediction, ""float""))
    print(""Test data accuracy:"", accuracy.eval({x: test_dataset, y: test_labels}))
    print(""Valid data accuracy:"", accuracy.eval({x: valid_dataset, y: valid_labels}))
</code></pre>

<p>The <code>tf.nn.dropout</code> is called in function <code>model()</code>, but after I applied the DropOut technique to the neural network, the accuracy did seem any change, here is the result:</p>

<pre><code>Epoch: 0001 cost= 579980.086977807
Epoch: 0002 cost= 238859.802382506
Epoch: 0003 cost= 90672.733752856
Epoch: 0004 cost= 32649.040985028
Epoch: 0005 cost= 11325.878361874
Epoch: 0006 cost= 3866.805511076
Epoch: 0007 cost= 1357.785540469
Epoch: 0008 cost= 519.381747333
Epoch: 0009 cost= 225.359804119
Epoch: 0010 cost= 110.099476707
Epoch: 0011 cost= 55.212384386
Epoch: 0012 cost= 28.469241683
Epoch: 0013 cost= 14.511494627
Epoch: 0014 cost= 6.567228943
Epoch: 0015 cost= 3.186372240
Epoch: 0016 cost= 1.701917576
Epoch: 0017 cost= 1.041632473
Epoch: 0018 cost= 0.843376874
Epoch: 0019 cost= 0.786183911
Epoch: 0020 cost= 0.775412846
Epoch: 0021 cost= 0.782965020
Epoch: 0022 cost= 0.796788171
Epoch: 0023 cost= 0.814522117
Epoch: 0024 cost= 0.832090579
Epoch: 0025 cost= 0.849197715
Epoch: 0026 cost= 0.867473578
Epoch: 0027 cost= 0.889561496
Epoch: 0028 cost= 0.921837020
Epoch: 0029 cost= 16.655304543
Epoch: 0030 cost= 1.421570476
Optimization Finished!
Test data accuracy: 0.8775
Valid data accuracy: 0.8069
</code></pre>

<p>How can I apply DropOut by Tensorflow to improve the accuracy of the network? Thank you!</p>
",Evaluation: Accuracy
How to use Batch Normalization correctly in tensorflow?,"<p>I had tried several versions of batch_normalization in tensorflow, but none of them worked! The results were all incorrect when I set batch_size = 1 at inference time.</p>

<p>Version 1: directly use the official version in tensorflow.contrib</p>

<pre><code>from tensorflow.contrib.layers.python.layers.layers import batch_norm
</code></pre>

<p>use like this:</p>

<pre><code>output = lrelu(batch_norm(tf.nn.bias_add(conv, biases), is_training), 0.5, name=scope.name)
</code></pre>

<p>is_training = True at training time and False at inference time.</p>

<p>Version 2: from <a href=""https://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow/38320613#38320613"">How could I use Batch Normalization in TensorFlow?</a></p>

<pre><code>def batch_norm_layer(x, train_phase, scope_bn='bn'):
    bn_train = batch_norm(x, decay=0.999, epsilon=1e-3, center=True, scale=True,
            updates_collections=None,
            is_training=True,
            reuse=None, # is this right?
            trainable=True,
            scope=scope_bn)
    bn_inference = batch_norm(x, decay=0.999, epsilon=1e-3, center=True, scale=True,
            updates_collections=None,
            is_training=False,
            reuse=True, # is this right?
            trainable=True,
            scope=scope_bn)
    z = tf.cond(train_phase, lambda: bn_train, lambda: bn_inference)
    return z
</code></pre>

<p>use like this:</p>

<pre><code>output = lrelu(batch_norm_layer(tf.nn.bias_add(conv, biases), is_training), 0.5, name=scope.name)
</code></pre>

<p>is_training is a placeholder at training time is True and False at inference time.</p>

<p>version 3: from slim <a href=""https://github.com/tensorflow/models/blob/master/inception/inception/slim/ops.py"" rel=""noreferrer"">https://github.com/tensorflow/models/blob/master/inception/inception/slim/ops.py</a></p>

<pre><code>def batch_norm_layer(inputs,
           is_training=True,
           scope='bn'):
  decay=0.999
  epsilon=0.001
  inputs_shape = inputs.get_shape()
  with tf.variable_scope(scope) as t_scope:
    axis = list(range(len(inputs_shape) - 1))
    params_shape = inputs_shape[-1:]
    # Allocate parameters for the beta and gamma of the normalization.
    beta, gamma = None, None
    beta = tf.Variable(tf.zeros_initializer(params_shape),
        name='beta',
        trainable=True)
    gamma = tf.Variable(tf.ones_initializer(params_shape),
        name='gamma',
        trainable=True)
    moving_mean = tf.Variable(tf.zeros_initializer(params_shape),
        name='moving_mean',
        trainable=False)
    moving_variance = tf.Variable(tf.ones_initializer(params_shape),
        name='moving_variance',
        trainable=False)
    if is_training:
      # Calculate the moments based on the individual batch.
      mean, variance = tf.nn.moments(inputs, axis)

      update_moving_mean = moving_averages.assign_moving_average(
          moving_mean, mean, decay)
      update_moving_variance = moving_averages.assign_moving_average(
          moving_variance, variance, decay)
    else:
      # Just use the moving_mean and moving_variance.
      mean = moving_mean
      variance = moving_variance
      # Normalize the activations.
    outputs = tf.nn.batch_normalization(
       inputs, mean, variance, beta, gamma, epsilon)
    outputs.set_shape(inputs.get_shape())
    return outputs
</code></pre>

<p>use like this:</p>

<pre><code>output = lrelu(batch_norm_layer(tf.nn.bias_add(conv, biases), is_training), 0.5, name=scope.name)
</code></pre>

<p>is_training = True at training time and False at inference time.</p>

<p>version 4: like version3, but add  tf.control_dependencies</p>

<pre><code>def batch_norm_layer(inputs,
           decay=0.999,
           center=True,
           scale=True,
           epsilon=0.001,
           moving_vars='moving_vars',
           activation=None,
           is_training=True,
           trainable=True,
           restore=True,
           scope='bn',
           reuse=None):
  inputs_shape = inputs.get_shape()
  with tf.variable_op_scope([inputs], scope, 'BatchNorm', reuse=reuse):
      axis = list(range(len(inputs_shape) - 1))
      params_shape = inputs_shape[-1:]
      # Allocate parameters for the beta and gamma of the normalization.
      beta = tf.Variable(tf.zeros(params_shape), name='beta')
      gamma = tf.Variable(tf.ones(params_shape), name='gamma')
      # Create moving_mean and moving_variance add them to
      # GraphKeys.MOVING_AVERAGE_VARIABLES collections.
      moving_mean = tf.Variable(tf.zeros(params_shape), name='moving_mean',
            trainable=False)
      moving_variance = tf.Variable(tf.ones(params_shape),   name='moving_variance', 
            trainable=False)
  control_inputs = []
  if is_training:
      # Calculate the moments based on the individual batch.
      mean, variance = tf.nn.moments(inputs, axis)

      update_moving_mean = moving_averages.assign_moving_average(
          moving_mean, mean, decay)
      update_moving_variance = moving_averages.assign_moving_average(
          moving_variance, variance, decay)
      control_inputs = [update_moving_mean, update_moving_variance]
  else:
      # Just use the moving_mean and moving_variance.
      mean = moving_mean
      variance = moving_variance
  # Normalize the activations. 
  with tf.control_dependencies(control_inputs):
      return tf.nn.batch_normalization(
        inputs, mean, variance, beta, gamma, epsilon)
</code></pre>

<p>use like this:</p>

<pre><code>output = lrelu(batch_norm(tf.nn.bias_add(conv, biases), is_training), 0.5, name=scope.name)
</code></pre>

<p>is_training = True at training time and False at inference time.</p>

<p>The 4 versions of Batch_normalization are all not correct. So, how to use batch normalization correctly?</p>

<p>Another strange phenomenon is if I set batch_norm_layer to null like this, the inference result are all same.</p>

<pre><code>def batch_norm_layer(inputs, is_training):
    return inputs
</code></pre>
",Training
Installing TensorFlow on Windows (Python 3.6.x),"<p>I'm trying to install <a href=""https://www.tensorflow.org/versions/r0.12/get_started/os_setup.html#pip-installation-on-windows"" rel=""noreferrer"">TensorFlow on Windows</a>.</p>

<p>I tried to install it with <code>pip</code>, but I always get the same error message:</p>

<pre><code>... is not a supported wheel on this platform.
</code></pre>

<p>I first tried it with Python 3.5.1, now I upgraded to <strong>3.6.0b4</strong>, but it makes no difference.</p>

<hr />

<p>Python:</p>

<pre><code>Python 3.6.0b4 (default, Nov 22 2016, 05:30:12) [MSC v.1900 64 bit (AMD64)] on win32
</code></pre>

<p>pip:</p>

<pre><code>pip 9.0.1 from ...\python\lib\site-packages (python 3.6)
</code></pre>

<hr />

<p>To be exact, I tried the following two commands:</p>

<pre><code>pip install --upgrade https://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-0.12.0rc0-cp35-cp35m-win_amd64.whl
pip install --upgrade https://storage.googleapis.com/tensorflow/windows/gpu/tensorflow_gpu-0.12.0rc0-cp35-cp35m-win_amd64.whl
</code></pre>

<p>they output the following:</p>

<pre><code>&gt; tensorflow-0.12.0rc0-cp35-cp35m-win_amd64.whl is not a supported wheel on this platform.
&gt; tensorflow_gpu-0.12.0rc0-cp35-cp35m-win_amd64.whl is not a supported wheel on this platform.
</code></pre>

<p>Does anyone know how to solve this problem? I'm not sure where I'm making a mistake.</p>

<p>Thanks!</p>

<p><hr />
<strong>Edit 1</strong></p>

<p>Btw, I also tried <code>pip install tensorflow</code> and <code>pip install tensorflow-gpu</code> like suggested <a href=""https://stackoverflow.com/a/38900276/6459948"">here</a>. I got the following output:</p>

<pre><code>&gt; Could not find a version that satisfies the requirement tensorflow (from versions: ) No matching distribution found for tensorflow
&gt; Could not find a version that satisfies the requirement tensorflow-gpu (from versions: ) No matching distribution found for tensorflow-gpu
</code></pre>
",Related to library: Installation
Tensorflow Queues - Switching between train and validation data,"<p>I am trying to make use of queues for loading data from files in Tensorflow.</p>

<p>I would like to to run the graph with validation data at the end of each epoch to get a better feel for how the training is going.</p>

<p>That is where i am running into problems. I cant seem to figure out how to
make the switch between training data and validation data when using queues.</p>

<p>I have stripped down my code to a bare minimum toy example to make it easier to
get help. Instead of including all the code that loads the image files, performs inference, and training, I have chopped it off at the
point where the filenames are loaded into the queue.</p>

<pre><code>import tensorflow as tf

#  DATA
train_items = [""train_file_{}"".format(i) for i in range(6)]
valid_items = [""valid_file_{}"".format(i) for i in range(3)]

# SETTINGS
batch_size = 3
batches_per_epoch = 2
epochs = 2

# CREATE GRAPH
graph = tf.Graph()
with graph.as_default():
    file_list = tf.placeholder(dtype=tf.string, shape=None)

    # Create a queue consisting of the strings in `file_list`
    q = tf.train.string_input_producer(train_items, shuffle=False, num_epochs=None)

    # Create batch of items.
    x = q.dequeue_many(batch_size)

    # Inference, train op, and accuracy calculation after this point
    # ...


# RUN SESSION
with tf.Session(graph=graph) as sess:
    # Initialize variables
    sess.run(tf.global_variables_initializer())
    sess.run(tf.local_variables_initializer())

    # Start populating the queue.
    coord = tf.train.Coordinator()
    threads = tf.train.start_queue_runners(sess=sess, coord=coord)

    try:
        for epoch in range(epochs):
            print(""-""*60)
            for step in range(batches_per_epoch):
                if coord.should_stop():
                    break
                train_batch = sess.run(x, feed_dict={file_list: train_items})
                print(""TRAIN_BATCH: {}"".format(train_batch))

            valid_batch = sess.run(x, feed_dict={file_list: valid_items})
            print(""\nVALID_BATCH : {} \n"".format(valid_batch))

    except Exception, e:
        coord.request_stop(e)
    finally:
        coord.request_stop()
        coord.join(threads)
</code></pre>

<h1>Variations and experiments</h1>

<h2>Trying different values for <code>num_epochs</code></h2>

<h3>num_epochs=None</h3>

<p>If i set the <code>num_epochs</code> argument in <code>tf.train.string_input_producer()</code>to
<code>None</code> it gives be the following output,
which shows that it is running two epochs as intended, but it is using data
from the training set when running evaluation.</p>

<pre><code>------------------------------------------------------------
TRAIN_BATCH: ['train_file_0' 'train_file_1' 'train_file_2']
TRAIN_BATCH: ['train_file_3' 'train_file_4' 'train_file_5']

VALID_BATCH : ['train_file_0' 'train_file_1' 'train_file_2']

------------------------------------------------------------
TRAIN_BATCH: ['train_file_3' 'train_file_4' 'train_file_5']
TRAIN_BATCH: ['train_file_0' 'train_file_1' 'train_file_2']

VALID_BATCH : ['train_file_3' 'train_file_4' 'train_file_5']
</code></pre>

<h3>num_epochs=2</h3>

<p>If i set the <code>num_epochs</code> argument in <code>tf.train.string_input_producer()</code> to <code>2</code>
it gives be the following output,
which shows that it is not even running the full two batches at all
(and evaliation is still using training data)</p>

<pre><code>------------------------------------------------------------
TRAIN_BATCH: ['train_file_0' 'train_file_1' 'train_file_2']
TRAIN_BATCH: ['train_file_3' 'train_file_4' 'train_file_5']

VALID_BATCH : ['train_file_0' 'train_file_1' 'train_file_2']

------------------------------------------------------------
TRAIN_BATCH: ['train_file_3' 'train_file_4' 'train_file_5']
</code></pre>

<h3>num_epochs=1</h3>

<p>If i set the <code>num_epochs</code> argument in <code>tf.train.string_input_producer()</code> to <code>1</code>
in the hopes that it will flush out
any aditional training data from the queue so it can make use of the validation
data, i get the following output, which shows that it is terminating as soon as
it gets through one epoch of training data, and does not get to go through
loading evaluation data.</p>

<pre><code>------------------------------------------------------------
TRAIN_BATCH: ['train_file_0' 'train_file_1' 'train_file_2']
TRAIN_BATCH: ['train_file_3' 'train_file_4' 'train_file_5']
</code></pre>

<h2>Setting <code>capacity</code> argument to various values</h2>

<p>I have also tried setting the <code>capacity</code> argument in
<code>tf.train.string_input_producer()</code> to small values, such as 3, and 1. But these
had no effect on the results.</p>

<h1>What other approach should I take?</h1>

<p>What other approach could i take to switch between training and validation data?
Would i have to create separate queues? I am at a loss as to how to get that to
work. Would i have to create additional coordinators and queue runners as well?</p>
",Evaluation: Validation strategy/Training
What does global_step mean in Tensorflow?,"<p>In this is <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/mnist/mnist.py"" rel=""nofollow noreferrer"">tutorial code</a> from TensorFlow website, </p>

<ol>
<li><p>could anyone help explain what does <code>global_step</code> mean? </p>

<p>I found on the Tensorflow website written that <em>global step is used count training steps</em>, but I don't quite get what exactly it means.</p></li>
<li><p>Also, what does the number 0 mean when setting up <code>global_step</code>? </p>

<pre><code>def training(loss,learning_rate):
    tf.summary.scalar('loss',loss)
    optimizer = tf.train.GradientDescentOptimizer(learning_rate)

    # Why 0 as the first parameter of the global_step tf.Variable?
    global_step = tf.Variable(0, name='global_step',trainable=False)

    train_op = optimizer.minimize(loss, global_step=global_step)

    return train_op
</code></pre>

<p>According to Tensorflow doc <em>global_step: increment by one after the variables have been updated</em>. Does that mean after one update <code>global_step</code> becomes 1?</p></li>
</ol>
",Related to library: Lack of knowledge
TensorBoard Embedding Example?,"<p>I'm looking for a tensorboard embedding example, with iris data for example like the embedding projector <a href=""http://projector.tensorflow.org/"" rel=""noreferrer"">http://projector.tensorflow.org/</a></p>

<p>But unfortunately i couldn't find one. Just a little bit information about how to do it in <a href=""https://www.tensorflow.org/how_tos/embedding_viz/"" rel=""noreferrer"">https://www.tensorflow.org/how_tos/embedding_viz/</a></p>

<p>Does someone knows a basic tutorial for this functionality?</p>

<p>Basics:</p>

<p>1) Setup a 2D tensor variable(s) that holds your embedding(s).</p>

<pre><code>embedding_var = tf.Variable(....)
</code></pre>

<p>2) Periodically save your embeddings in a LOG_DIR.</p>

<p>3) Associate metadata with your embedding.</p>
",Related to library: Lack of knowledge
How to initialize only Optimizer variables in Tensorflow?,"<p>I want to use <code>MomentumOptimizer</code> in Tensorflow. However, since this optimizer uses some internal variable, attempting to use it without initializing this variable yields an error: </p>

<p><code>FailedPreconditionError (see above for traceback): Attempting to use
 uninitialized value Variable_2/Momentum</code></p>

<p>This can be easily solved by initializing all variables, e.g:</p>

<p><code>tf.global_variables_initializer().run()</code></p>

<p>However, I do not want to initialize <strong>all</strong> the variables - only those of Optimizer. Is there any way to do this?</p>
",
Can Keras with Tensorflow backend be forced to use CPU or GPU at will?,"<p>I have Keras installed with the Tensorflow backend and CUDA.  I'd like to sometimes on demand force Keras to use CPU.  Can this be done without say installing a separate CPU-only Tensorflow in a virtual environment?  If so how?  If the backend were Theano, the flags could be set, but I have not heard of Tensorflow flags accessible via Keras.  </p>
",Related to library: Installation
"TensorFlow on Windows: ""not a supported wheel on this platform"" error","<p>Was happy to know Tensorflow is made available for Windows and we don't have to use Docker. </p>

<p>I tried to install as per instructions but I get this error.</p>

<blockquote>
  <p>pip install --upgrade <a href=""https://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-0.12.0rc0-cp35-cp35m-win_amd64.whl"" rel=""noreferrer"">https://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-0.12.0rc0-cp35-cp35m-win_amd64.whl</a>
  tensorflow-0.12.0rc0-cp35-cp35m-win_amd64.whl is not a supported wheel on this platform.</p>
</blockquote>

<p>What does that error mean?</p>

<p>I am running latest version of Python. </p>

<blockquote>
  <p>python --version
  Python 3.5.2</p>
</blockquote>
",Related to library: Installation
Is there a way to suppress the messages TensorFlow prints?,"<p>I think that those messages are really important for the first few times but then it is just useless.
It is actually making things worse to read and debug.</p>

<blockquote>
  <p>I tensorflow/stream_executor/dso_loader.cc:128] successfully opened
  CUDA library libcublas.so.8.0 locally I
  tensorflow/stream_executor/dso_loader.cc:119] Couldn't open CUDA
  library libcudnn.so. LD_LIBRARY_PATH:  I
  tensorflow/stream_executor/cuda/cuda_dnn.cc:3459] Unable to load cuDNN
  DSO I tensorflow/stream_executor/dso_loader.cc:128] successfully
  opened CUDA library libcufft.so.8.0 locally I
  tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA
  library libcuda.so.1 locally I
  tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA
  library libcurand.so.8.0 locally</p>
</blockquote>

<p>Is there a way to suppress the ones that just say it was successful?</p>
",Related to library: Lack of knowledge
"TensorFlow ValueError: Cannot feed value of shape (64, 64, 3) for Tensor u'Placeholder:0', which has shape '(?, 64, 64, 3)'","<p>I am new to TensorFlow and machine learning. I am trying to classify two objects a cup and a pendrive (jpeg images). I have trained and exported a model.ckpt successfully. Now I am trying to restore the saved model.ckpt for prediction. Here is the script: </p>

<pre><code>import tensorflow as tf
import math
import numpy as np
from PIL import Image
from numpy import array


# image parameters
IMAGE_SIZE = 64
IMAGE_CHANNELS = 3
NUM_CLASSES = 2

def main():
    image = np.zeros((64, 64, 3))
    img = Image.open('./IMG_0849.JPG')

    img = img.resize((64, 64))
    image = array(img).reshape(64,64,3)

    k = int(math.ceil(IMAGE_SIZE / 2.0 / 2.0 / 2.0 / 2.0)) 
    # Store weights for our convolution and fully-connected layers
    with tf.name_scope('weights'):
        weights = {
            # 5x5 conv, 3 input channel, 32 outputs each
            'wc1': tf.Variable(tf.random_normal([5, 5, 1 * IMAGE_CHANNELS, 32])),
            # 5x5 conv, 32 inputs, 64 outputs
            'wc2': tf.Variable(tf.random_normal([5, 5, 32, 64])),
            # 5x5 conv, 64 inputs, 128 outputs
            'wc3': tf.Variable(tf.random_normal([5, 5, 64, 128])),
            # 5x5 conv, 128 inputs, 256 outputs
            'wc4': tf.Variable(tf.random_normal([5, 5, 128, 256])),
            # fully connected, k * k * 256 inputs, 1024 outputs
            'wd1': tf.Variable(tf.random_normal([k * k * 256, 1024])),
            # 1024 inputs, 2 class labels (prediction)
            'out': tf.Variable(tf.random_normal([1024, NUM_CLASSES]))
        }

    # Store biases for our convolution and fully-connected layers
    with tf.name_scope('biases'):
        biases = {
            'bc1': tf.Variable(tf.random_normal([32])),
            'bc2': tf.Variable(tf.random_normal([64])),
            'bc3': tf.Variable(tf.random_normal([128])),
            'bc4': tf.Variable(tf.random_normal([256])),
            'bd1': tf.Variable(tf.random_normal([1024])),
            'out': tf.Variable(tf.random_normal([NUM_CLASSES]))
        }

   saver = tf.train.Saver()
   with tf.Session() as sess:
       saver.restore(sess, ""./model.ckpt"")
       print ""...Model Loaded...""   
       x_ = tf.placeholder(tf.float32, shape=[None, IMAGE_SIZE , IMAGE_SIZE , IMAGE_CHANNELS])
       y_ = tf.placeholder(tf.float32, shape=[None, NUM_CLASSES])
       keep_prob = tf.placeholder(tf.float32)

       init = tf.initialize_all_variables()

       sess.run(init)
       my_classification = sess.run(tf.argmax(y_, 1), feed_dict={x_:image})
       print 'Neural Network predicted', my_classification[0], ""for your image""


if __name__ == '__main__':
     main()
</code></pre>

<p>When I run the above script for prediction I get the following error:</p>

<pre><code>ValueError: Cannot feed value of shape (64, 64, 3) for Tensor u'Placeholder:0', which has shape '(?, 64, 64, 3)' 
</code></pre>

<p>What am I doing wrong? And how do I fix the shape of numpy array?</p>
",Data Preparation: Shape mismatch/Training
Tensorflow AttributeError: 'NoneType' object has no attribute 'TF_DeleteStatus',"<p>Tensorflow is giving me this unresolved error:</p>

<pre><code>Exception ignored in: &lt;bound method BaseSession.__del__ of &lt;tensorflow.python.client.session.Session object at 0x7f68d14b6668&gt;&gt;
Traceback (most recent call last):
  File ""/opt/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 532, in __del__
AttributeError: 'NoneType' object has no attribute 'TF_DeleteStatus'
</code></pre>

<p>The error has been discussed <a href=""https://github.com/tensorflow/tensorflow/issues/3388"">here</a>. The problem is it is not showing up consistently. However, it is showing up in my terminal frequently. Has anybody managed to get around it.Thanks.</p>
",
TensorFlow 'module' object has no attribute 'global_variables_initializer',"<p>I'm new to Tensorflow
I'm running a Deep learning Assignment from Udacity on iPython notebook.
<a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/udacity/5_word2vec.ipynb"">link</a></p>

<p>And it has an error.</p>

<pre><code>AttributeError                            Traceback (most recent call last)
`&lt;ipython-input-18-3446420b5935&gt;` in `&lt;module&gt;`()
  2 
  3 with tf.Session(graph=graph) as session:
----&gt; 4   tf.global_variables_initializer().run()

AttributeError: 'module' object has no attribute 'global_variables_initializer'
</code></pre>

<p>Please help! How can I fix this? Thank you.</p>
",Related to library: Bugs
"TensorFlow, why there are 3 files after saving the model?","<p>Having read the <a href=""https://www.tensorflow.org/how_tos/variables/#saving_and_restoring"" rel=""noreferrer"">docs</a>, I saved a model in <code>TensorFlow</code>, here is my demo code:</p>

<pre class=""lang-py prettyprint-override""><code># Create some variables.
v1 = tf.Variable(..., name=""v1"")
v2 = tf.Variable(..., name=""v2"")
...
# Add an op to initialize the variables.
init_op = tf.global_variables_initializer()

# Add ops to save and restore all the variables.
saver = tf.train.Saver()

# Later, launch the model, initialize the variables, do some work, save the
# variables to disk.
with tf.Session() as sess:
  sess.run(init_op)
  # Do some work with the model.
  ..
  # Save the variables to disk.
  save_path = saver.save(sess, ""/tmp/model.ckpt"")
  print(""Model saved in file: %s"" % save_path)
</code></pre>

<p>but after that, I found there are 3 files</p>

<pre><code>model.ckpt.data-00000-of-00001
model.ckpt.index
model.ckpt.meta
</code></pre>

<p>And I can't restore the model by restore the <code>model.ckpt</code> file, since there is no such file. Here is my code</p>

<pre class=""lang-py prettyprint-override""><code>with tf.Session() as sess:
  # Restore variables from disk.
  saver.restore(sess, ""/tmp/model.ckpt"")
</code></pre>

<p>So, why there are 3 files?</p>
",Output problem
How to get Tensorflow tensor dimensions (shape) as int values?,"<p>Suppose I have a Tensorflow tensor. How do I get the dimensions (shape) of the tensor as integer values? I know there are two methods, <code>tensor.get_shape()</code> and <code>tf.shape(tensor)</code>, but I can't get the shape values as integer <code>int32</code> values.</p>

<p>For example, below I've created a 2-D tensor, and I need to get the number of rows and columns as <code>int32</code> so that I can call <code>reshape()</code> to create a tensor of shape <code>(num_rows * num_cols, 1)</code>. However, the method <code>tensor.get_shape()</code> returns values as <code>Dimension</code> type, not <code>int32</code>.</p>

<pre><code>import tensorflow as tf
import numpy as np

sess = tf.Session()    
tensor = tf.convert_to_tensor(np.array([[1001,1002,1003],[3,4,5]]), dtype=tf.float32)

sess.run(tensor)    
# array([[ 1001.,  1002.,  1003.],
#        [    3.,     4.,     5.]], dtype=float32)

tensor_shape = tensor.get_shape()    
tensor_shape
# TensorShape([Dimension(2), Dimension(3)])    
print tensor_shape    
# (2, 3)

num_rows = tensor_shape[0] # ???
num_cols = tensor_shape[1] # ???

tensor2 = tf.reshape(tensor, (num_rows*num_cols, 1))    
# Traceback (most recent call last):
#   File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
#   File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 1750, in reshape
#     name=name)
#   File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 454, in apply_op
#     as_ref=input_arg.is_ref)
#   File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 621, in convert_to_tensor
#     ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
#   File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/constant_op.py"", line 180, in _constant_tensor_conversion_function
#     return constant(v, dtype=dtype, name=name)
#   File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/constant_op.py"", line 163, in constant
#     tensor_util.make_tensor_proto(value, dtype=dtype, shape=shape))
#   File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/tensor_util.py"", line 353, in make_tensor_proto
#     _AssertCompatible(values, dtype)
#   File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/tensor_util.py"", line 290, in _AssertCompatible
#     (dtype.name, repr(mismatch), type(mismatch).__name__))
# TypeError: Expected int32, got Dimension(6) of type 'Dimension' instead.
</code></pre>
",Output problem
Tensorflow vocabularyprocessor,"<p>I am following the wildml blog on text classification using tensorflow. I am not able to understand the purpose of max_document_length in the code statement :</p>

<pre><code>vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)
</code></pre>

<p>Also how can i extract vocabulary from the vocab_processor</p>
",Related to library: Lack of knowledge
Understanding `tf.nn.nce_loss()` in tensorflow,"<p>I am trying to understand the NCE loss function in Tensorflow. NCE loss is employed for a word2vec task, for instance:</p>

<pre><code># Look up embeddings for inputs.
embeddings = tf.Variable(
    tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))
embed = tf.nn.embedding_lookup(embeddings, train_inputs)

# Construct the variables for the NCE loss
nce_weights = tf.Variable(
    tf.truncated_normal([vocabulary_size, embedding_size],
                        stddev=1.0 / math.sqrt(embedding_size)))
nce_biases = tf.Variable(tf.zeros([vocabulary_size]))

# Compute the average NCE loss for the batch.
# tf.nce_loss automatically draws a new sample of the negative labels each
# time we evaluate the loss.
loss = tf.reduce_mean(
    tf.nn.nce_loss(weights=nce_weights,
                   biases=nce_biases,
                   labels=train_labels,
                   inputs=embed,
                   num_sampled=num_sampled,
                   num_classes=vocabulary_size))
</code></pre>

<p>more details, please reference Tensorflow <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/word2vec/word2vec_basic.py"" rel=""noreferrer"">word2vec_basic.py</a></p>

<ol>
<li>What are the input and output matrices in the NCE function? </li>
</ol>

<p>In a word2vec model, we are interested in building representations for words. In the training process, given a slid window, every word will have two embeddings: 1) when the word is a centre word; 2) when the word is a context word. These two embeddings are called input and output vectors, respectively. (<a href=""http://www-personal.umich.edu/~ronxin/pdf/w2vexp.pdf"" rel=""noreferrer"">more explanations of input and output matrices</a>)</p>

<p>In my opinion, the input matrix is <code>embeddings</code> and the output matrix is <code>nce_weights</code>. Is it right?</p>

<ol start=""2"">
<li>What is the final embedding?</li>
</ol>

<p>According to a <a href=""https://stackoverflow.com/questions/37982478/what-is-the-purpose-of-weights-and-biases-in-tensorflow-word2vec-example"">post</a> by s0urcer also relating to <code>nce</code>, it says the final embedding matrix is just the input matrix. While, <a href=""https://stats.stackexchange.com/questions/177667/input-vector-representation-vs-output-vector-representation-in-word2vec"">some others saying</a>, the <code>final_embedding=input_matrix+output_matrix</code>. Which is right/more common?</p>
",Related to library: Lack of knowledge
The print of string constant is always attached with 'b' inTensorFlow,"<p>Durng the test of TensorFlow r0.12(CPU) installed on Windows 10, I found that the printed string contant is always with an 'b' in the end. The print of python is normal. I cannot figure out the reason so came here for help. The code is as follows:</p>

<pre><code>&gt;&gt;&gt;import tensorflow as tf
&gt;&gt;&gt;hello = tf.constant('Hello, TensorFlow!')
&gt;&gt;&gt;sess = tf.Session()
&gt;&gt;&gt;print(sess.run(hello))
b'Hello, TensorFlow!'
</code></pre>
",Related to library: Lack of knowledge
How to do Xavier initialization on TensorFlow,"<p>I'm porting my Caffe network over to TensorFlow but it doesn't seem to have xavier initialization. I'm using <code>truncated_normal</code> but this seems to be making it a lot harder to train.</p>
",Choosing a model: Lack of knowledge
Why does TensorFlow example fail when increasing batch size?,"<p>I was looking at the <a href=""http://tensorflow.org/tutorials/mnist/beginners/index.md"">Tensorflow MNIST example for beginners</a> and found that in this part:</p>

<pre><code>for i in range(1000):
  batch_xs, batch_ys = mnist.train.next_batch(100)
  sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})
</code></pre>

<p>changing the batch size from 100 to be above 204 causes the model to fail to converge. It works up to 204, but at 205 and any higher number I tried, the accuracy would end up &lt; 10%. Is this a bug, something about the algorithm, something else?</p>

<p>This is running their binary installation for OS X, seems to be version 0.5.0.</p>
",Training
Distributed tensorflow: the difference between In-graph replication and Between-graph replication,"<p>I got confused about the two concepts: <code>In-graph replication</code> and <code>Between-graph replication</code> when reading the <a href=""https://www.tensorflow.org/versions/master/how_tos/distributed/#replicated_training"" rel=""noreferrer"">Replicated training</a> in tensorflow's official How-to. </p>

<ol>
<li><p>It's said in above link that </p>

<blockquote>
  <p><strong>In-graph replication.</strong> In this approach, the client builds a single
  tf.Graph that contains one set of parameters (in tf.Variable nodes
  pinned to /job:ps); ...</p>
</blockquote>

<p>Does this mean there are <strong>multiple</strong> <code>tf.Graph</code>s in <code>Between-graph
replication</code> approach? If yes, where are the corresponding codes in
the provided examples?</p></li>
<li><p>While there is already a <code>Between-graph replication</code> example in above link, could anyone provide a <code>In-graph replication</code>
implementation (pseudo code is fine)  and highlight its main
differences from <code>Between-graph replication</code>?</p>

<p>Thanks in advance! </p>

<hr></li>
</ol>

<h2><strong>Edit_1: more questions</strong></h2>

<p>Thanks a lot for your detailed explanations and gist code @mrry @YaroslavBulatov ! After looking
    your responses, I have the following two questions:</p>

<ol start=""3"">
<li><p>There is the following statement in <a href=""https://www.tensorflow.org/versions/master/how_tos/distributed/#replicated_training"" rel=""noreferrer"">Replicated training</a>:</p>

<blockquote>
  <p><strong>Between-graph replication.</strong> In this approach, there is a separate
  client for each /job:worker task, typically in the same process as the
  worker task. Each client builds a <strong>similar graph</strong> containing the
  parameters (pinned to /job:ps as before using
  tf.train.replica_device_setter() to map them deterministically to the
  same tasks); and a <strong>single copy</strong> of the compute-intensive part of the
  model, pinned to the local task in /job:worker.</p>
</blockquote>

<p>I have two sub-questions related to above words in bold.</p>

<p>(A) Why do we say each client builds <strong>similar graph</strong>, but not <strong>same graph</strong>?
I wonder the graph built in each client in the example of <a href=""https://www.tensorflow.org/versions/master/how_tos/distributed/#replicated_training"" rel=""noreferrer"">Replicated training</a>
should be the same because below graph construction codes are shared within all <code>worker</code>s.:</p>

<p><code># Build model...</code></p>

<p><code>loss = ...</code></p>

<p><code>global_step = tf.Variable(0)</code></p>

<p>(B) Shouldn't it be <strong>multiple copies</strong> of compute-intensive part of
the model, since we have multiple <code>workers</code>?</p></li>
<li><p>Does the example in <a href=""https://www.tensorflow.org/versions/master/how_tos/distributed/#replicated_training"" rel=""noreferrer"">Replicated training</a> support training on multiple machines, each of which has multiple GPUs? If not, can we
use simultaneously both the <code>In-graph replication</code> to support training on multiple
GPUs on each machine and <code>Between-graph replication</code> for
cross-machine training? I ask this question because
@mrry indicated that the <code>In-graph replication</code> is essentially same to the way
used in <a href=""https://github.com/tensorflow/models/blob/91c7b91f834a5a857e8168b96d6db3b93d7b9c2a/tutorials/image/cifar10/cifar10_multi_gpu_train.py"" rel=""noreferrer"">CIFAR-10 example model for multiple GPUs</a>.</p></li>
</ol>
",Training
How does tf.app.run() work?,"<p>How does <code>tf.app.run()</code> work in Tensorflow translate demo? </p>

<p>In <code>tensorflow/models/rnn/translate/translate.py</code>, there is a call to <code>tf.app.run()</code>. How is it being handled?</p>

<pre><code>if __name__ == ""__main__"":
    tf.app.run() 
</code></pre>
",Related to library: Lack of knowledge
"TensorFlow on Windows: ""Couldn't open CUDA library cudnn64_5.dll""","<p>Tensorflow just released windows support. I installed the gpu version and CUDA 8.0 and python 3.5. However, after I import the tensorflow I got the following error:</p>

<pre><code>&gt;&gt;&gt; import tensorflow
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc:128] successfully opened CUDA library cublas64_80.dll locally
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc:119] Couldn't open CUDA library cudnn64_5.dll
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\cuda\cuda_dnn.cc:3459] Unable to load cuDNN DSO
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc:128] successfully opened CUDA library cufft64_80.dll locally
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc:128] successfully opened CUDA library nvcuda.dll locally
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc:128] successfully opened CUDA library curand64_80.dll locally
</code></pre>

<p>Can someone help? Thanks!</p>
",Related to library: Lack of knowledge
Add Tensorflow pre-processing to existing Keras model (for use in Tensorflow Serving),"<p>I would like to include my custom pre-processing logic in my exported Keras model for use in Tensorflow Serving.</p>

<p><em>My pre-processing performs string tokenization and uses an external dictionary to convert each token to an index for input to the Embedding layer:</em></p>

<pre><code>from keras.preprocessing import sequence

token_to_idx_dict = ... #read from file

# Custom Pythonic pre-processing steps on input_data
tokens = [tokenize(s) for s in input_data]
token_idxs = [[token_to_idx_dict[t] for t in ts] for ts in tokens]
tokens_padded = sequence.pad_sequences(token_idxs, maxlen=maxlen)
</code></pre>

<p><em>Model architecture and training:</em></p>

<pre><code>model = Sequential()
model.add(Embedding(max_features, 128, input_length=maxlen))
model.add(LSTM(128, activation='sigmoid'))
model.add(Dense(n_classes, activation='softmax'))
model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')

model.fit(x_train, y_train)
</code></pre>

<p>Since the model will be used in Tensorflow Serving, I want to incorporate all pre-processing logic into the model itself (encoded in the exported model file).</p>

<p><strong>Q: How can I do so using the Keras library only?</strong></p>

<p>I found <a href=""https://blog.keras.io/keras-as-a-simplified-interface-to-tensorflow-tutorial.html"" rel=""noreferrer"">this guide</a> explains how to combine Keras and Tensorflow. But I'm still unsure how to export everything as one model.</p>

<p>I know Tensorflow has built-in string splitting, <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lookup/lookup_ops.py#L320"" rel=""noreferrer"">file I/O</a>, and <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lookup/lookup_ops.py#L195"" rel=""noreferrer"">dictionary lookup operations</a>.</p>

<p><em>Pre-processing logic using Tensorflow operations:</em></p>

<pre><code># Get input text
input_string_tensor = tf.placeholder(tf.string, shape={1})
# Split input text by whitespace
splitted_string = tf.string_split(input_string_tensor, "" "")
# Read index lookup dictionary
token_to_idx_dict = tf.contrib.lookup.HashTable(tf.contrib.lookup.TextFileInitializer(""vocab.txt"", tf.string, 0, tf.int64, 1, delimiter="",""), -1)
# Convert tokens to indexes
token_idxs = token_to_idx_dict.lookup(splitted_string)
# Pad zeros to fixed length
token_idxs_padded = tf.pad(token_idxs, ...)
</code></pre>

<p><strong>Q: How can I use these Tensorflow pre-defined pre-processing operations and my Keras layers together to both train and then export the model as a ""black box"" for use in Tensorflow Serving?</strong></p>
",Related to library: Lack of knowledge/Training
Tensorflow: 'module' object has no attribute 'scalar_summary',"<p>I tried to run the following code to test my TensorBoard, however, when I ran the program, there is an error said:</p>

<pre><code>'module' object has no attribute 'scalar_summary'
</code></pre>

<p>I want to know how can I fix this issue, thanks.</p>

<p>The following is the system info:</p>

<ul>
<li>Operating System: Ubuntu 16.04 LTS</li>
<li>Tensorflow version: 0.12rc (master)</li>
<li>Running environment: Jupyter Notebook</li>
</ul>

<p><strong>Test program and Output:</strong>
<a href=""https://i.stack.imgur.com/qgt9G.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/qgt9G.png"" alt=""enter image description here""></a></p>
",Related to library: Lack of knowledge
How to properly manage memory and batch size with TensorFlow,"<p>I am using TensorFlow to build a simple feed-forward neural network, and I am using variable size batches. I am not using the GPU, I have 8GB RAM, and running on Python 3.5.2.</p>

<p>My problem is that I have some batches that are too big and are generating the typical out of memory error. I understand that, it is not a problem. However, if I use Keras with TF backend I don't have that issue. I have built an example (with fixed size batches) bellow that illustrates this.</p>

<p>Is there a problem with my implementation? How should I handle batches that are too big?</p>

<h3>TensorFlow example (exhausts memory)</h3>



<pre class=""lang-py prettyprint-override""><code>
import numpy as np
import tensorflow as tf

n_observations = 100000
n_input = 6
batch_size = 20000
X = np.random.rand(n_observations, n_input)
Y = X[:,0] ** 3 + X[:,1] ** 2 + X[:,2] + X[:,3] + X[:,4] + X[:,5]+ np.random.rand(n_observations)

n_hidden = 16
n_output = 1

def generatebatch(n_observations, batch_size):
    for batch_i in range(n_observations // batch_size):
        start = batch_i*batch_size
        end = start + batch_size
        batch_xs = X[start:end, :]
        batch_ys = Y[start:end]
        yield batch_xs, batch_ys

with tf.Session() as sess:
    # placeholders for input and target
    net_input = tf.placeholder(tf.float32, [None, n_input])
    y_true = tf.placeholder(tf.float32)

    # Hidden Layer
    W1 = tf.Variable(tf.random_normal([n_input, n_hidden]))
    b1 = tf.Variable(tf.random_normal([n_hidden]))
    net_output1 = tf.nn.relu(tf.matmul(net_input, W1) + b1)

    # Yet another Hidden Layer
    yaW1 = tf.Variable(tf.random_normal([n_hidden, n_hidden]))
    yab1 = tf.Variable(tf.random_normal([n_hidden]))
    yanet_output1 = tf.nn.relu(tf.matmul(net_output1, yaW1) + yab1)

    # Output Layer
    W2 = tf.Variable(tf.random_normal([n_hidden, n_output]))
    b2 = tf.Variable(tf.random_normal([n_output]))
    net_output2 = tf.nn.relu(tf.matmul(yanet_output1, W2) + b2)

    # The loss function
    cost = tf.reduce_mean(tf.pow(y_true - net_output2, 2))

    # Configure the optimizer
    optimizer = tf.train.AdamOptimizer().minimize(cost)

    # Initialize variables
    sess.run(tf.global_variables_initializer())

    n_epochs = 100
    for epoch_i in range(n_epochs):
        batchloss = []
        for batch_xs, batch_ys in generatebatch(n_observations, batch_size):
            _, loss = sess.run(
                [optimizer, cost],
                feed_dict={
                    net_input: batch_xs,
                    y_true: batch_ys
            })
            batchloss.append(loss)
        print(np.mean(batchloss))
</code></pre>

<h3>Keras Example (handles the batch size somehow)</h3>

<pre class=""lang-py prettyprint-override""><code>
import numpy as np
from keras.models import Sequential
from keras.layers import Dense
import logging

#just to hide the deprecation warnings
logging.basicConfig(level=logging.CRITICAL)

n_input = 6
n_observations = 100000
n_hidden = 16
n_epochs = 10
batch_size = 35000

# input data
X = np.random.rand(n_observations, n_input)
Y = X[:,0] ** 3 + X[:,1] ** 2 + X[:,2] + X[:,3] + X[:,4] + X[:,5]+ np.random.rand(n_observations)

# create and fit Multilayer Perceptron model
model = Sequential()
model.add(Dense(n_hidden, input_dim=n_input, activation='relu'))
model.add(Dense(n_hidden, activation='relu'))
model.add(Dense(1, activation='sigmoid'))
model.compile(loss='mse', optimizer='adam')
model.fit(X, Y, nb_epoch=n_epochs, batch_size=batch_size, verbose=1)
</code></pre>
",Choosing a model: Creating model errors
Predicting next word using the language model tensorflow example,"<p>The <a href=""http://tensorflow.org/tutorials/recurrent/index.md#language_modeling"" rel=""noreferrer"">tensorflow tutorial</a> on language model allows  to compute the probability of sentences :</p>

<pre><code>probabilities = tf.nn.softmax(logits)
</code></pre>

<p>in the comments below it also specifies a way of predicting the next word instead of probabilities but does not specify how this can be done. So how to output a word instead of probability using this example?</p>

<pre><code>lstm = rnn_cell.BasicLSTMCell(lstm_size)
# Initial state of the LSTM memory.
state = tf.zeros([batch_size, lstm.state_size])

loss = 0.0
for current_batch_of_words in words_in_dataset:
    # The value of state is updated after processing each batch of words.
    output, state = lstm(current_batch_of_words, state)

    # The LSTM output can be used to make next word predictions
    logits = tf.matmul(output, softmax_w) + softmax_b
    probabilities = tf.nn.softmax(logits)
    loss += loss_function(probabilities, target_words)
</code></pre>
",Prediction
How do you read Tensorboard files programmatically?,"<p>How can you write a python script to read Tensorboard log files, extracting the loss and accuracy and other numerical data, without launching the GUI <code>tensorboard --logdir=...</code>?</p>
",
numpy random choice in Tensorflow,"<p>Is there an equivalent function to numpy random choice in Tensorflow. 
In numpy we can get an item randomly from the given list with its weights. </p>

<pre><code> np.random.choice([1,2,3,5], 1, p=[0.1, 0, 0.3, 0.6, 0])
</code></pre>

<p>This code will select an item from the given list with p weights. </p>
",Related to library: Lack of knowledge
Convert Tensorflow model to Caffe model,"<p>I would like to be able to convert a Tensorflow model to Caffe model.</p>

<p>I searched on google but I was able to find only converters from caffe to tensorflow but not the opposite.</p>

<p>Does anyone have an idea on how to do it?</p>

<p>Thanks,
Evi</p>
",Choosing a model: Lack of knowledge
TensorFlow: How can I evaluate a validation data queue multiple times during training?,"<h2>tl;dr</h2>

<p>How can I evaluate a validation set after every K training iterations, using separate queues for training and validation data, without resorting to separate <code>tf.Sessions</code> in multiple processes? There doesn't seem to be a clean way to achieve this, given my particular problem, and my current workaround (which I thought would work) gives me undefined behavior. Help!</p>

<h2>The whole story</h2>

<p>I want to evaluate a validation set every K training iterations, and I cannot figure out how to implement this properly in TensorFlow. This should be one of the most common operations, yet it feels that TensorFlow's API/architecture is working against me here or is at least making things unnecessarily difficult.</p>

<p>My assumptions are:</p>

<ul>
<li>[A1] The multi-process model for training/validation as described here <a href=""https://www.tensorflow.org/how_tos/reading_data/#multiple_input_pipelines"" rel=""noreferrer"">https://www.tensorflow.org/how_tos/reading_data/#multiple_input_pipelines</a> is not applicable to my problem, as I have to assume there is not enough GPU memory available to load the variables twice.</li>
<li>[A2] I want to evaluate on the validation set every K training iterations.</li>
<li>[A3] Both training and validation data cannot be simply read from disk, but are generated on-the-fly. This makes it impossible to reliably pre-compute the size of the validation set in advance.</li>
<li>[A4] The validation set is too large to pre-compute and store onto disk.</li>
<li>[A5] The effective validation set size is not necessarily a multiple of the batch size.</li>
</ul>

<p>The training input pipeline is set up as follows:</p>

<ul>
<li>A <code>tf.train.slice_input_producer()</code> generates a (shuffled) list of filenames, each referring to raw input data.</li>
<li>A custom data generation function generates a variable number of training exemplars/labels from each chunk of raw input data.</li>
<li>The generated training exemplars/labels are queued via <code>tf.train.shuffle_batch()</code> before being fed into the network.</li>
</ul>

<p>Due to [A3], [A4], [A5], the validation input pipeline is set up in an almost identical way, except that the final input queue is generated via <code>tf.train.batch()</code>, since shuffling is not desirable. Due to the above assumptions, a feed_dict based approach is also infeasible, and also seemingly incompatible with using a higher level function such as <code>tf.train.batch</code>.</p>

<p>However, a straightforward implementation using two different sets of queues for training and validation does not work. As far as I understand, I have two options:</p>

<ul>
<li><p>[B1] Set the <code>num_epochs</code> argument of the validation <code>tf.train.slice_input_producer</code> to <code>None</code>.</p>

<p>In this case, the validation set is cycled through endlessly, but I would need to know the size of the validation set in advance to explicitly limit the number of batches to evaluate per run through the validation set. Furthermore, if the validation set size is not divisible by the batch size, I will always pull a bit more in the last batch. As this would shift the order of evaluation of the validation data each time, this is not acceptable.</p></li>
<li><p>[B2] Set the <code>num_epochs</code> argument of the validation <code>tf.train.slice_input_producer</code> to <code>1</code>, and additionally set the <code>allow_smaller_final_batch</code> argument of the <code>tf.train.batch</code> function to <code>True</code>.</p>

<p>In this case, the validation set is cycled through exactly once, after which the respective queue is closed forever. By default, this will make evaluating the validation set two or more times impossible. Since I do not know of a good way to reopen a queue in TensorFlow, I need to work around this limitation. </p></li>
</ul>

<p>Due to the greater limitations of option [B1], I chose to work around the issues of option [B2] instead. The (pseudo-)code outlining my current approach is as follows:</p>

<p>The training loop should be fairly canonical. Every K iterations, a function to evaluate the validation set is called.
Note that I only start the queues that have a name starting with ""train_""; these is the queue set up for collecting generated training data. In order to do this, I created two helper functions, <code>get_queues_by_name</code> and <code>start_queue_runners</code>. </p>

<pre><code>def train_loop(train_ops, vali_ops, ...):
    with tf.Session() as sess:
        coord = tf.train.Coordinator()
        sess.run([tf.initialize_all_variables(), tf.initialize_local_variables()])
        load_latest_snapshot(sess, loader, snapshot_file)

        # Launch the queue runners
        queues = get_queues_by_name(""train"")
        threads = start_queue_runners(sess, coord, queues)

        try:
            for step in range(start_iteration, num_train_iterations):
                # Runs the session on validation set
                if step % K == 0:
                    validation_results = run_validation(vali_ops, snapshot_file)

                # TRAINING:
                # ...

        except Exception as e:
            coord.request_stop(e)
        finally:
            coord.request_stop()
            coord.join(threads)
</code></pre>

<p>The helper functions look like this:</p>

<pre><code>def get_queues_by_name(name):
    """"""Retrieves all queues that contain the string given by 'name'""""""
    all_queues = tf.get_collection(tf.GraphKeys.QUEUE_RUNNERS)
    return [q for q in all_queues if name in q.name]


def start_queue_runners(session, coordinator, queues):
    """"""Similar to tf.train.start_queue_runners but now accepts a list of queues instead of a graph collection""""""
    with session.graph.as_default():
        threads = []
        for queue in queues:
            log(""Queue"", ""Starting queue '%s'"" % queue.name, level=2)
            threads.extend(queue.create_threads(session, coordinator, daemon=True, start=True))
    return threads
</code></pre>

<p>In the <code>run_validation</code> function, my chosen workaround against the issue of a closed queue is to create a new <code>tf.Session</code>. I also only start the threads associated with the queue collecting validation set data.</p>

<pre><code>def run_validation(ops, snapshot_file):  # Called inside train_loop()
    results = None
    loader = tf.train.Saver()

    with tf.Session() as sess:
        coord = tf.train.Coordinator()
        sess.run([tf.initialize_local_variables()])
        load_latest_snapshot(sess, loader, snapshot_file)

        # Launch the queue runners
        queues = get_queues_by_name(""eval"")
        threads = start_queue_runners(sess, coord, queues)

        # Performs the inference in batches
        try:
            # Evaluate validation set:
            results = eval_in_batches(ops, sess)
        except Exception as e:
            coord.request_stop(e)
        finally:
            coord.request_stop()
            coord.join(threads)

    return results
</code></pre>

<p>I do not know whether creating a new <code>tf.Session</code> here is a good idea, but it seems like the only way to accomplish restarting the validation queue. Ideally, I also wouldn't want to re-load the model snapshot, as this seems conceptually unnecessary.</p>

<p>The issue with this code is that I see erratic/undefined behavior during running, such as NaN's or Inf's appearing inside the network during validation set evaluation. This seems to occur predominantly when the validation set queue is being filled at the same time as the training set queue is still being filled (since the training queue is open during validation set evaluation). For example, this very often happens if I evaluate the validation set at iteration 0 (when both queues still need to be filled). It almost seems as if the training/validation queues share some global state, although they are running in a different session.</p>

<p>Can someone explain why this is happening, and how I can solve this more cleanly while taking my above assumptions [A1]-[A5] into account?</p>
",Evaluation: Validation strategy/Training
Tensorflow 1.0 Windows + 64-bit Anaconda 4.3.0 error,"<p>Following the instructions under ""Installing with Anaconda"" at <a href=""https://www.tensorflow.org/install/install_windows"" rel=""noreferrer"">https://www.tensorflow.org/install/install_windows</a>, I get to this point and get a failure.  </p>

<p><code>(tensorflow) C:\Users\rallen\Documents\Devel\python\tensorflow&gt;pip install --ignore-installed --upgrade https://storage.googleapis.com/tensorflow/windows/gpu/tensorflow_gpu-1.0.0-cp35-cp35m-win_x86_64.whl
tensorflow_gpu-1.0.0-cp35-cp35m-win_x86_64.whl is not a supported wheel on this platform.
</code></p>

<p>This is my environment 64-bit Anaconda3 4.3.0</p>

<p><code>&gt;python
Python 3.6.0 |Anaconda 4.3.0 (64-bit)| (default, Dec 23 2016, 11:57:41) [MSC v.1900 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
</code></p>

<p>I previously successfully installed pre-1.0 tensorflow from pip.</p>
",Related to library: Installation
How do I resolve these tensorflow warnings?,"<p>I just installed Tensorflow 1.0.0 using pip. When running, I get warnings like the one shown below.</p>

<p><code>W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.</code></p>

<p>I get 5 more similar warning for SSE4.1, SSE4.2, AVX, AVX2, FMA.</p>

<p>Despite these warnings the program seems to run fine.</p>
",Related to library: Installation
"How to ""reset"" tensorboard data after killing tensorflow instance","<p>I'm testing different hyperparameters for a cnn model I built, but I'm having a small annoyance when viewing the summaries in Tensorboard. The problem seems to be that the data is just ""added"" in consecutive runs, so the functions result in a weird superposition unless I see the information as ""relative"" instead of ""by step"". See here:</p>

<p><a href=""https://i.stack.imgur.com/hZDiS.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/hZDiS.png"" alt=""X Type: Step""></a></p>

<p><a href=""https://i.stack.imgur.com/kef2k.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/kef2k.png"" alt=""X Type: Relative""></a></p>

<p>I've tried killing tensorboard's process and erasing the log files, but it seems it is not enough.</p>

<p><strong>So the question is, how do I reset this information?</strong></p>

<p>Thanks!!</p>
",Related to library: Lack of knowledge
"TensorFlow 0.12 tutorials produce warning: ""Rank of input Tensor should be the same as output_rank for column","<p>I have some experience with writing machine learning programs in python, but I'm new to TensorFlow and am checking it out. My dev environment is a lubuntu 14.04 64-bit virtual machine. I've created a python 3.5 conda environment from miniconda and installed TensorFlow 0.12 and its dependencies. I began trying to run some example code from TensorFlow's tutorials and encountered this warning when calling <code>fit()</code> in the boston.py example for input functions: <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/input_fn/boston.py"" rel=""noreferrer"">source</a>.</p>

<blockquote>
  <p>WARNING:tensorflow:Rank of input Tensor (1) should be the same as
  output_rank (2) for column. Will attempt to expand dims. It is highly
  recommended that you resize your input, as this behavior may change.</p>
</blockquote>

<p>After some searching in Google, I found other people encountered this same warning:</p>

<ul>
<li><a href=""https://github.com/tensorflow/tensorflow/issues/6184"" rel=""noreferrer"">https://github.com/tensorflow/tensorflow/issues/6184</a></li>
<li><a href=""https://github.com/tensorflow/tensorflow/issues/5098"" rel=""noreferrer"">https://github.com/tensorflow/tensorflow/issues/5098</a></li>
<li><a href=""https://stackoverflow.com/questions/41068915/tensorflow-boston-housing-data-tutorial-errors"">Tensorflow - Boston Housing Data Tutorial Errors</a></li>
</ul>

<p>However, they also experienced errors which prevent code execution from completing. In my case, the code executes with the above warning. Unfortunately, I couldn't find a single answer in those links regarding what caused the warning and how to fix the warning. They all focused on the error. How does one remove the warning? Or is the warning safe to ignore? </p>

<p>Cheers!</p>

<p>Extra info, I also see the following warnings when running the aforementioned boston.py example.  </p>

<blockquote>
  <p>WARNING:tensorflow:*******************************************************
  WARNING:tensorflow:TensorFlow's V1 checkpoint format has been
  deprecated. WARNING:tensorflow:Consider switching to the more
  efficient V2 format: WARNING:tensorflow:<br>
  'tf.train.Saver(write_version=tf.train.SaverDef.V2)'
  WARNING:tensorflow:now on by default.
  WARNING:tensorflow:*******************************************************</p>
</blockquote>

<p>and</p>

<blockquote>
  <p>WARNING:tensorflow:From
  /home/kade/miniconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/dnn_linear_combined.py:1053
  in predict.: calling BaseEstimator.predict (from
  tensorflow.contrib.learn.python.learn.estimators.estimator) with x is
  deprecated and will be removed after 2016-12-01. Instructions for
  updating: Estimator is decoupled from Scikit Learn interface by moving
  into separate class SKCompat. Arguments x, y and batch_size are only
  available in the SKCompat class, Estimator will only accept input_fn.
  Example conversion:   est = Estimator(...) -> est =
  SKCompat(Estimator(...))</p>
</blockquote>

<p><strong>UPDATE (2016-12-22):</strong>
I've tracked the warning to this file:
<a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/layers/python/layers/feature_column_ops.py"" rel=""noreferrer"">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/layers/python/layers/feature_column_ops.py</a></p>

<p>and this code block:</p>

<pre><code>except NotImplementedError:
    with variable_scope.variable_scope(
        None,
        default_name=column.name,
        values=columns_to_tensors.values()):
      tensor = column._to_dense_tensor(transformed_tensor)
      tensor = fc._reshape_real_valued_tensor(tensor, 2, column.name)
      variable = [
          contrib_variables.model_variable(
              name='weight',
              shape=[tensor.get_shape()[1], num_outputs],
              initializer=init_ops.zeros_initializer(),
              trainable=trainable,
              collections=weight_collections)
      ]
      predictions = math_ops.matmul(tensor, variable[0], name='matmul')
</code></pre>

<p>Note the line: <code>tensor = fc._reshape_real_valued_tensor(tensor, 2, column.name)</code></p>

<p>The method signature is: <code>_reshape_real_valued_tensor(input_tensor, output_rank, column_name=None)</code></p>

<p>The value <code>2</code> is hardcoded as the value of output_rank, but the boston.py example is passing in an <code>input_tensor</code> of rank 1. I will continue to investigate.</p>
",Data Preparation: Shape mismatch
Logging training and validation loss in tensorboard,"<p>I'm trying to learn how to use tensorflow and tensorboard. I have a test project based on the MNIST neural net tutorial (<a href=""https://www.tensorflow.org/versions/master/tutorials/mnist/tf/index.html"">https://www.tensorflow.org/versions/master/tutorials/mnist/tf/index.html</a>).</p>

<p>In my code, I construct a node that calculates the fraction of digits in a data set that are correctly classified, like this:</p>

<pre><code>correct = tf.nn.in_top_k(self._logits, labels, 1)
correct = tf.to_float(correct)
accuracy = tf.reduce_mean(correct)
</code></pre>

<p>Here, <code>self._logits</code>is the inference part of the graph, and <code>labels</code> is a placeholder that contains the correct labels.</p>

<p>Now, what I would like to do is evaluate the accuracy for both the training set and the validation set as training proceeds. I can do this by running the accuracy node twice, with different feed_dicts:</p>

<pre><code>train_acc = tf.run(accuracy, feed_dict={images : training_set.images, labels : training_set.labels})
valid_acc = tf.run(accuracy, feed_dict={images : validation_set.images, labels : validation_set.labels})
</code></pre>

<p>This works as intended. I can print the values, and I can see that initially, the two accuracies will both increase, and eventually the validation accuracy will flatten out while the training accuracy keeps increasing.</p>

<p>However, I would also like to get graphs of these values in tensorboard, and I can not figure out how to do this. If I simply add a <code>scalar_summary</code> to <code>accuracy</code>, the logged values will not distinguish between training set and validation set. </p>

<p>I also tried creating two identical <code>accuracy</code> nodes with different names and running one on the training set and one on the validation set. I then add a <code>scalar_summary</code> to each of these nodes. This does give me two graphs in tensorboard, but instead of one graph showing the training set accuracy and one showing the validation set accuracy, they are both showing identical values that do not match either of the ones printed to the terminal.</p>

<p>I am probably misunderstanding how to solve this problem. What is the recommended way of separately logging the output from a single node for different inputs?</p>
",Related to library: Lack of knowledge/Evaluation: Validation strategy/Training
Holding variables constant during optimizer,"<p>I have a TensorFlow computational graph for a loss tensor L that depends on 2 tf.Variables, A and B. </p>

<p>I'd like to run gradient ascent on variable A (A+=gradient of L wrt A) while holding B fixed, and vice versa - running gradient ascent on B (B+=gradient of L wrt B) while holding A fixed. How do I do this?</p>
",Training
Why is Tensorflow 100x slower than convnetjs in this simple NN example?,"<p>I've been working with convnetjs for 1 year and now I want to move on to more powerful and fast libraries. I thought Tensorflow would be orders of magnitude faster than a JS library, so I wrote a simple neural network for both libraries and did some tests. It is a 3-5-5-1 neural network, trained on one single example for a certain number of epochs with SGD and RELU layers.</p>

<p>Tensorflow code:</p>

<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf
import numpy
import time

NUM_CORES = 1  # Choose how many cores to use.
sess = tf.Session(config=tf.ConfigProto(inter_op_parallelism_threads=NUM_CORES, intra_op_parallelism_threads=NUM_CORES))

# Parameters
learning_rate = 0.001
training_epochs = 1000
batch_size = 1

# Network Parameters
n_input = 3 # Data input
n_hidden_1 = 5 # 1st layer num features
n_hidden_2 = 5 # 2nd layer num features
n_output = 1 # Data output

# tf Graph input
x = tf.placeholder(""float"", [None, n_input], ""a"")
y = tf.placeholder(""float"", [None, n_output], ""b"")

# Create model
def multilayer_perceptron(_X, _weights, _biases):
    layer_1 = tf.nn.relu(tf.add(tf.matmul(_X, _weights['h1']), _biases['b1'])) #Hidden layer with RELU activation
    layer_2 = tf.nn.relu(tf.add(tf.matmul(layer_1, _weights['h2']), _biases['b2'])) #Hidden layer with RELU activation
    return tf.matmul(layer_2, _weights['out']) + _biases['out']

# Store layers weight &amp; bias
weights = {
    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),
    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),
    'out': tf.Variable(tf.random_normal([n_hidden_2, n_output]))
}
biases = {
    'b1': tf.Variable(tf.random_normal([n_hidden_1])),
    'b2': tf.Variable(tf.random_normal([n_hidden_2])),
    'out': tf.Variable(tf.random_normal([n_output]))
}

# Construct model
pred = multilayer_perceptron(x, weights, biases)

# Define loss and optimizer
cost = tf.reduce_sum(tf.nn.l2_loss(pred-y)) / batch_size # L2 loss
optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost) # Adam Optimizer

# Initializing the variables
init = tf.initialize_all_variables()

# Launch the graph
sess.run(init)

# Training Data
train_X = numpy.asarray([[0.1,0.2,0.3]])
train_Y = numpy.asarray([[0.5]])

# Training cycle
start = time.clock()
for epoch in range(training_epochs):
    # Fit training using batch data
    sess.run(optimizer, feed_dict={x: train_X, y: train_Y})
end = time.clock()

print end - start #2.5 seconds -&gt; 400 epochs per second 
print ""Optimization Finished!""
</code></pre>

<p>JS code:</p>

<pre class=""lang-js prettyprint-override""><code>&lt;!DOCTYPE html&gt;

&lt;html lang=""en""&gt;
&lt;head&gt;
    &lt;meta charset=""utf-8"" /&gt;
    &lt;title&gt;Regression example convnetjs&lt;/title&gt;
    &lt;script src=""http://cs.stanford.edu/people/karpathy/convnetjs/build/convnet.js""&gt;&lt;/script&gt;
    &lt;script src=""http://cs.stanford.edu/people/karpathy/convnetjs/build/util.js""&gt;&lt;/script&gt;
    &lt;script&gt;
        var layer_defs, net, trainer;
        function start() {
            layer_defs = [];
            layer_defs.push({ type: 'input', out_sx: 1, out_sy: 1, out_depth: 3 });
            layer_defs.push({ type: 'fc', num_neurons: 5, activation: 'relu' });
            layer_defs.push({ type: 'fc', num_neurons: 5, activation: 'relu' });
            layer_defs.push({ type: 'regression', num_neurons: 1 });
            net = new convnetjs.Net();
            net.makeLayers(layer_defs);
            trainer = new convnetjs.SGDTrainer(net, { learning_rate: 0.001, method: 'sgd', batch_size: 1, l2_decay: 0.001, l1_decay: 0.001 });

        var start = performance.now();
            for(var i = 0; i &lt; 100000; i++) {
                var x = new convnetjs.Vol([0.1, 0.2, 0.3]);
                trainer.train(x, [0.5]);
            }
        var end = performance.now();
        console.log(end-start); //3 seconds -&gt; 33333 epochs per second
        var predicted_values = net.forward(x);
            console.log(predicted_values.w[0]);
        }

    &lt;/script&gt;
&lt;/head&gt;
&lt;body&gt;
    &lt;button onclick=""start()""&gt;Start&lt;/button&gt;
&lt;/body&gt;
&lt;/html&gt;
</code></pre>

<p>The results are that convnetjs trains for 100'000 epochs in 3 seconds, while Tensorflow trains for 1000 epochs in 2.5 seconds. Is this expected?</p>
",Training: Performance
Feeding image data in tensorflow for transfer learning,"<p>I am trying to use tensorflow for transfer learning. I downloaded the pre-trained model inception3 from the tutorial. In the code, for prediction: </p>

<pre><code>prediction = sess.run(softmax_tensor,{'DecodeJpeg/contents:0'}:image_data})
</code></pre>

<p>Is there a way to feed the png image. I tried changing <code>DecodeJpeg</code> to <code>DecodePng</code> but it did not work. Beside, what should I change if I want to feed decoded image file like a numpy array or a batch of arrays?</p>

<p>Thanks!!</p>
",
Storing tensorflow models in memory,"<p>The program I'm writing involves switching between models during run-time.</p>

<p>I am currently using Saver to save/load models from the disk as specified here: <a href=""https://www.tensorflow.org/api_docs/python/state_ops/saving_and_restoring_variables#Saver"">https://www.tensorflow.org/api_docs/python/state_ops/saving_and_restoring_variables#Saver</a>.</p>

<p>The models are fairly small and can be stored in memory, so I was wondering if anyone knows of a way to store and restore these models in-memory instead of saving them to disk.</p>

<p>I tried to modify the tensorflow source to save the model to memory however <code>gen_io_ops</code> seems to be generated during compile time. Another possible way is to use memory mapped files. Does anyone know of an easier way?</p>
",
How to stack multiple lstm in keras?,"<p>I am using deep learning library keras and trying to stack multiple LSTM with no luck.
Below is my code</p>

<pre><code>model = Sequential()
model.add(LSTM(100,input_shape =(time_steps,vector_size)))
model.add(LSTM(100))
</code></pre>

<p>The above code returns error in the third line <code>Exception: Input 0 is incompatible with layer lstm_28: expected ndim=3, found ndim=2
</code></p>

<p>The input X is a tensor of shape (100,250,50). I am running keras on tensorflow backend</p>
",Choosing a model: Lack of knowledge
module' object has no attribute 'SummaryWriter',"<p>I'm using Tensorflow version 0.12.head with Python 2.7 on a linux CentOS 7 and when I run this:</p>

<pre><code>import tensorflow as tf

a = tf.constant(5, name=""input_a"")
b = tf.constant(3, name=""input_b"")
c = tf.mul(a, b, name=""mul_c"")
d = tf.add(a, b, name=""add_d"")
e = tf.add(c, d, name=""add_e"")
sess = tf.Session()
output = sess.run(e)
writer = tf.train.SummaryWriter('./my_graph', sess.graph)
</code></pre>

<p>I get this error:</p>

<pre><code>AttributeError                            Traceback (most recent call last) &lt;ipython-input-6-29c037e85eec&gt; in &lt;module&gt;()
----&gt; 1 writer = tf.train.SummaryWriter('./my_graph', sess.graph)

AttributeError: 'module' object has no attribute 'SummaryWriter'
</code></pre>

<p>I have run these two commands because there is bug <a href=""https://github.com/tensorflow/tensorflow/issues/1645"" rel=""noreferrer"">issue</a> on Github for the same problem:</p>

<pre><code>&gt;&gt;&gt; import six
&gt;&gt;&gt; print(six.__version__)
1.10.0
&gt;&gt;&gt; print(dir(six.moves.queue)) ['Empty', 'Full', 'LifoQueue', 'PriorityQueue', 'Queue', '__all__', '__builtins__', '__doc__', '__file__', '__name__', '__package__', '_threading', '_time', 'deque', 'heapq']
&gt;&gt;&gt; print(six.moves.queue.__file__) /usr/lib64/python2.7/Queue.pyc
</code></pre>

<p>I'm new in Python and in Tensorflow. Do you know how can I fix this error?</p>

<p>I have changed <code>SummaryWriter</code> with <code>FileWriter</code>:</p>

<pre><code>writer = tf.train.FileWriter('./my_graph', sess.graph)
</code></pre>

<p>And I get the same error but with <code>FileWriter</code> function:</p>

<pre><code>AttributeError                            Traceback (most recent call last)
&lt;ipython-input-8-daa50ea2b8f9&gt; in &lt;module&gt;()
----&gt; 1 writer = tf.train.FileWriter('./my_graph', sess.graph)

AttributeError: 'module' object has no attribute 'FileWriter'
</code></pre>

<p>I have also run it in a terminal and I get the same result:</p>

<pre><code>[VansFannel@localhost ~]$ python
Python 2.7.5 (default, Nov  6 2016, 00:28:07) 
[GCC 4.8.5 20150623 (Red Hat 4.8.5-11)] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
&gt;&gt;&gt; import tensorflow as tf
W tensorflow/core/platform/cpu_feature_guard.cc:95] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:95] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
&gt;&gt;&gt; a = tf.constant(5, name=""input_a"")
&gt;&gt;&gt; b = tf.constant(3, name=""input_b"")
&gt;&gt;&gt; c = tf.mul(a, b, name=""mul_c"")
&gt;&gt;&gt; d = tf.add(a, b, name=""add_d"")
&gt;&gt;&gt; e = tf.add(c, d, name=""add_e"")
&gt;&gt;&gt; sess = tf.Session()
&gt;&gt;&gt; output = sess.run(e)
&gt;&gt;&gt; writer = tf.train.FileWriter('./my_graph', sess.graph)
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
AttributeError: 'module' object has no attribute 'FileWriter'
&gt;&gt;&gt; 
</code></pre>
",Related to library: Lack of knowledge
Add weights to .pb file exported by TensorFlow,"<p>My project uses Python to train a MLP on TensorFlow and then I export the graph and the weights in that way:</p>

<pre><code>tf.train.write_graph(sess.graph_def, ""./"", ""inp.txt"", True) 
saver.save(sess, 'variables/model.ckpt', global_step=1)
</code></pre>

<p>Now, although it is fine to use both files to import it back to Python it seems impossible to use it for Android or C++ since it cannot inport the checkpoint .ckpt.</p>

<p>Right now, I'm using the script <code>freeze_graph.py</code> provided by google to join both files into one by doing:</p>

<pre><code>bazel-bin/tensorflow/python/tools/freeze_graph --input_graph=inp.txt --input_checkpoint=variables/model.ckpt-1 --output_graph=newoutput.pb --output_node_names=output
</code></pre>

<p>My question is, is there a way to use another function instead of <code>tf.train.write_graph</code> to export it with the weights included?</p>
",
About names of variable scope in tensorflow,"

<p>Recently I have been trying to learn to use TensorFlow, and I do not understand how variable scopes work exactly. In particular, I have the following problem:</p>

<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf
from tensorflow.models.rnn import rnn_cell
from tensorflow.models.rnn import rnn

inputs = [tf.placeholder(tf.float32,shape=[10,10]) for _ in range(5)]
cell = rnn_cell.BasicLSTMCell(10)
outpts, states = rnn.rnn(cell, inputs, dtype=tf.float32)

print outpts[2].name
# ==&gt; u'RNN/BasicLSTMCell_2/mul_2:0'
</code></pre>

<p>Where does the  <code>'_2'</code> in <code>'BasicLSTMCell_2'</code> come from? How does it work when later using <code>tf.get_variable(reuse=True)</code> to get the same variable again?</p>

<p>edit: I think I find a related problem:</p>

<pre class=""lang-py prettyprint-override""><code>def creating(s):
    with tf.variable_scope('test'):
        with tf.variable_scope('inner'):
            a=tf.get_variable(s,[1])
    return a

def creating_mod(s):
    with tf.variable_scope('test'):
        with tf.variable_scope('inner'):
            a=tf.Variable(0.0, name=s)
    return a

tf.ops.reset_default_graph()
a=creating('a')
b=creating_mod('b')
c=creating('c')
d=creating_mod('d')

print a.name, '\n', b.name,'\n', c.name,'\n', d.name
</code></pre>

<p>The output is</p>

<pre class=""lang-py prettyprint-override""><code>test/inner/a:0 
test_1/inner/b:0 
test/inner/c:0 
test_3/inner/d:0
</code></pre>

<p>I'm confused...</p>
",
How to prefetch data using a custom python function in tensorflow,"<p>I am trying to prefetch training data to hide I/O latency. I would like to write custom Python code that loads data from disk and preprocesses the data (e.g. by adding a context window). In other words, one thread does data preprocessing and the other does training. Is this possible in TensorFlow?</p>

<p>Update: I have a working example based on @mrry's example.</p>

<pre><code>import numpy as np
import tensorflow as tf
import threading

BATCH_SIZE = 5
TRAINING_ITERS = 4100

feature_input = tf.placeholder(tf.float32, shape=[128])
label_input = tf.placeholder(tf.float32, shape=[128])

q = tf.FIFOQueue(200, [tf.float32, tf.float32], shapes=[[128], [128]])
enqueue_op = q.enqueue([label_input, feature_input])

label_batch, feature_batch = q.dequeue_many(BATCH_SIZE)
c = tf.reshape(feature_batch, [BATCH_SIZE, 128]) + tf.reshape(label_batch, [BATCH_SIZE, 128])

sess = tf.Session()

def load_and_enqueue(sess, enqueue_op, coord):
  with open('dummy_data/features.bin') as feature_file, open('dummy_data/labels.bin') as label_file:
    while not coord.should_stop():
      feature_array = np.fromfile(feature_file, np.float32, 128)
      if feature_array.shape[0] == 0:
        print('reach end of file, reset using seek(0,0)')
        feature_file.seek(0,0)
        label_file.seek(0,0)
        continue
      label_value = np.fromfile(label_file, np.float32, 128)

      sess.run(enqueue_op, feed_dict={feature_input: feature_array,
                                      label_input: label_value})

coord = tf.train.Coordinator()
t = threading.Thread(target=load_and_enqueue, args=(sess,enqueue_op, coord))
t.start()

for i in range(TRAINING_ITERS):
  sum = sess.run(c)
  print('train_iter='+str(i))
  print(sum)

coord.request_stop()
coord.join([t])
</code></pre>
",Data Preparation: Lack of knowledge
Why input is scaled in tf.nn.dropout in tensorflow?,"<p>I can't understand why dropout works like this in tensorflow. The blog of <a href=""http://cs231n.github.io/neural-networks-2/"">CS231n</a> says that, <code>""dropout is implemented by only keeping a neuron active with some probability p (a hyperparameter), or setting it to zero otherwise.""</code> Also you can see this from picture(Taken from the same site)
<a href=""https://i.stack.imgur.com/SbXq1.jpg""><img src=""https://i.stack.imgur.com/SbXq1.jpg"" alt=""enter image description here""></a></p>

<p>From tensorflow site, <code>With probability keep_prob, outputs the input element scaled up by 1 / keep_prob, otherwise outputs 0.</code></p>

<p>Now, why the input element is scaled up by <code>1/keep_prob</code>? Why not keep the input element as it is with probability and not scale it with <code>1/keep_prob</code>? </p>
",Training
Adjust Single Value within Tensor -- TensorFlow,"<p>I feel embarrassed asking this, but how do you adjust a single value within a tensor? Suppose you want to add '1' to only one value within your tensor?</p>

<p>Doing it by indexing doesn't work:</p>

<pre><code>TypeError: 'Tensor' object does not support item assignment
</code></pre>

<p>One approach would be to build an identically shaped tensor of 0's. And then adjusting a 1 at the position you want. Then you would add the two tensors together. Again this runs into the same problem as before.</p>

<p>I've read through the API docs several times and can't seem to figure out how to do this. Thanks in advance! </p>
",Related to library: Lack of knowledge
Tensorflow and Anaconda on Ubuntu?,"<p>On my Ubuntu 14.04, I have installed tensorflow, using ""pip"", as specified in the <a href=""http://tensorflow.org/get_started/os_setup.md"" rel=""noreferrer"">Tensorflow Installation instructions</a> and I made sure it was working by importing it in python and it did work.</p>

<p>Then, I installed Anaconda and it changed my .bashrc file by adding the following line to it:</p>

<pre><code>export PATH=""/home/sonny/anaconda2/bin:$PATH""
</code></pre>

<p>But because of this change, now it looks into the PATH above, which doesn't contain tensorflow. now I can't import tensorflow in my python code.</p>

<p>What is the proper way to extend the $PATH environment variable so that it stays using everything from anaconda2 but it becomes able to import ""tensorflow""?</p>
",Related to library: Lack of knowledge
Why do we name variables in Tensorflow?,"<p>In some of the places, I saw the syntax, where variables are initialized with names, sometimes without names. For example:</p>

<pre><code># With name
var = tf.Variable(0, name=""counter"")

# Without
one = tf.constant(1)
</code></pre>

<p>What is the point of naming the variable <code>var</code> <code>""counter""</code>?</p>
",Related to library: Lack of knowledge
Tensorflow image reading & display,"<p>I've got a bunch of images in a format similar to Cifar10 (binary file, <code>size = 96*96*3</code> bytes per image), one image after another (<a href=""http://cs.stanford.edu/~acoates/stl10/"" rel=""noreferrer"">STL-10 dataset</a>). The file I'm opening has 138MB.</p>

<p>I tried to read &amp; check the contents of the Tensors containing the images to be sure that the reading is done right, however I have two questions - </p>

<ol>
<li>Does the <code>FixedLengthRecordReader</code> load the whole file, however just provide inputs one at a time? Since reading the first <code>size</code> bytes should be relatively fast. However, the code takes about two minutes to run. </li>
<li>How to get the actual image contents in a displayable format, or display them internally to validate that the images are read well? I did <code>sess.run(uint8image)</code>, however the result is empty.</li>
</ol>

<p>The code is below:</p>

<pre><code>import tensorflow as tf
def read_stl10(filename_queue):
  class STL10Record(object):
    pass
  result = STL10Record()

  result.height = 96
  result.width = 96
  result.depth = 3
  image_bytes = result.height * result.width * result.depth
  record_bytes = image_bytes

  reader = tf.FixedLengthRecordReader(record_bytes=record_bytes)
  result.key, value = reader.read(filename_queue)
  print value
  record_bytes = tf.decode_raw(value, tf.uint8)

  depth_major = tf.reshape(tf.slice(record_bytes, [0], [image_bytes]),
                       [result.depth, result.height, result.width])
  result.uint8image = tf.transpose(depth_major, [1, 2, 0])
  return result
# probably a hack since I should've provided a string tensor

filename_queue = tf.train.string_input_producer(['./data/train_X'])
image = read_stl10(filename_queue)

print image.uint8image
with tf.Session() as sess:
  result = sess.run(image.uint8image)
  print result, type(result)
</code></pre>

<p><strong>Output:</strong></p>

<pre><code>Tensor(""ReaderRead:1"", shape=TensorShape([]), dtype=string)
Tensor(""transpose:0"", shape=TensorShape([Dimension(96), Dimension(96), Dimension(3)]), dtype=uint8)
I tensorflow/core/common_runtime/local_device.cc:25] Local device intra op parallelism threads: 4
I tensorflow/core/common_runtime/local_session.cc:45] Local session inter op parallelism threads: 4
[empty line for last print]
Process finished with exit code 137
</code></pre>

<p>I'm running this on my CPU, if that adds anything.</p>

<p>EDIT: I found the pure TensorFlow solution thanks to Rosa. Apparently, when using the <code>string_input_producer</code>, in order to see the results, you need to initialize the queue runners. 
The only required thing to add to the code above is the second line from below:</p>

<pre><code>...
with tf.Session() as sess:
    tf.train.start_queue_runners(sess=sess)
...
</code></pre>

<p>Afterwards, the image in the <code>result</code> can be displayed with <code>matplotlib.pyplot.imshow(result)</code>. I hope this helps someone. If you have any further questions, feel free to ask me or check the link in Rosa's answer.</p>
",Data Preparation
Is TensorFlow suitable for Recommendation Systems,"<p>I have read blogpost about TensorFlow is being open sourced.</p>

<p>In the tutorials and the examples on the TensorFlow website, I see that they are mostly classification problems. (e.g. given an image, classify the number written in it)</p>

<p>I am curious about it the software also suitable for solving problems in recommendation system?
For example, is it good for solving problems on collaborative filtering / content-based filtering?</p>
",Choosing a model: Lack of knowledge
Making predictions with a TensorFlow model,"<p>I followed the given mnist tutorials and was able to train a model and evaluate its accuracy. However, the tutorials don't show how to make predictions given a model. I'm not interested in accuracy, I just want to use the model to predict a new example and in the output see all the results (labels), each with its assigned score (sorted or not).</p>
",Prediction
Tensorflow NaN bug?,"<p>I'm using TensorFlow and I modified the <a href=""http://tensorflow.org/tutorials/mnist/pros/index.md"">tutorial</a> example to take my RGB images.</p>

<p>The algorithm works flawlessly out of the box on the new image set, until suddenly (still converging, it's around 92% accuracy usually), it crashes with the error that ReluGrad received non-finite values. Debugging shows that nothing unusual happens with the numbers until very suddenly, for unknown reason, the error is thrown. Adding</p>

<pre><code>print ""max W vales: %g %g %g %g""%(tf.reduce_max(tf.abs(W_conv1)).eval(),tf.reduce_max(tf.abs(W_conv2)).eval(),tf.reduce_max(tf.abs(W_fc1)).eval(),tf.reduce_max(tf.abs(W_fc2)).eval())
print ""max b vales: %g %g %g %g""%(tf.reduce_max(tf.abs(b_conv1)).eval(),tf.reduce_max(tf.abs(b_conv2)).eval(),tf.reduce_max(tf.abs(b_fc1)).eval(),tf.reduce_max(tf.abs(b_fc2)).eval())
</code></pre>

<p>as debug code to each loop, yields the following output:</p>

<pre><code>Step 8600
max W vales: 0.759422 0.295087 0.344725 0.583884
max b vales: 0.110509 0.111748 0.115327 0.124324
Step 8601
max W vales: 0.75947 0.295084 0.344723 0.583893
max b vales: 0.110516 0.111753 0.115322 0.124332
Step 8602
max W vales: 0.759521 0.295101 0.34472 0.5839
max b vales: 0.110521 0.111747 0.115312 0.124365
Step 8603
max W vales: -3.40282e+38 -3.40282e+38 -3.40282e+38 -3.40282e+38
max b vales: -3.40282e+38 -3.40282e+38 -3.40282e+38 -3.40282e+38
</code></pre>

<p>Since none of my values is very high, the only way a NaN can happen is by a badly handled 0/0, but since this tutorial code doesn't do any divisions or similar operations, I see no other explanation than that this comes from the internal TF code. </p>

<p>I'm clueless on what to do with this. Any suggestions? The algorithm is converging nicely, its accuracy on my validation set was steadily climbing and just reached 92.5% at iteration 8600.</p>
",Data Preparation
How can I visualize the weights(variables) in cnn in Tensorflow?,"<p>After training the cnn model, I want to visualize the weight or print out the weights, what can I do?
I cannot even print out the variables after training.
Thank you!</p>
",Output problem
TensorFlow Error found in Tutorial,"<p>Dare I even ask? This is such a new technology at this point that I can't find a way to solve this seemingly simple error. The tutorial I'm going over can be found here- <a href=""http://www.tensorflow.org/tutorials/mnist/pros/index.html#deep-mnist-for-experts"">http://www.tensorflow.org/tutorials/mnist/pros/index.html#deep-mnist-for-experts</a></p>

<p>I literally copied and pasted all of the code into IPython Notebook and at the very last chunk of code I get an error.</p>

<pre><code># To train and evaluate it we will use code that is nearly identical to that for the simple one layer SoftMax network above.
# The differences are that: we will replace the steepest gradient descent optimizer with the more sophisticated ADAM optimizer.

cross_entropy = -tf.reduce_sum(y_*tf.log(y_conv))
train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)
correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))
accuracy = tf.reduce_mean(tf.cast(correct_prediction, ""float""))
sess.run(tf.initialize_all_variables())
for i in range(20000):
    batch = mnist.train.next_batch(50)
    if i%100 == 0:
        train_accuracy = accuracy.eval(feed_dict={x:batch[0], y_: batch[1], keep_prob: 1.0})
    print ""step %d, training accuracy %g""%(i, train_accuracy)
    train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})

print ""test accuracy %g""%accuracy.eval(feed_dict={
    x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0})
</code></pre>

<p>After running this code, I receive this error.</p>

<pre><code>---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-46-a5d1ab5c0ca8&gt; in &lt;module&gt;()
     15 
     16 print ""test accuracy %g""%accuracy.eval(feed_dict={
---&gt; 17     x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0})

/root/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc in eval(self, feed_dict, session)
    403 
    404     """"""
--&gt; 405     return _eval_using_default_session(self, feed_dict, self.graph, session)
    406 
    407 

/root/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc in _eval_using_default_session(tensors, feed_dict, graph, session)
   2712     session = get_default_session()
   2713     if session is None:
-&gt; 2714       raise ValueError(""Cannot evaluate tensor using eval(): No default ""
   2715                        ""session is registered. Use 'with ""
   2716                        ""DefaultSession(sess)' or pass an explicit session to ""

ValueError: Cannot evaluate tensor using eval(): No default session is registered. Use 'with DefaultSession(sess)' or pass an explicit session to eval(session=sess)
</code></pre>

<p>I thought that I may need to install or reinstall TensorFlow via conda install <a href=""https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.5.0-cp27-none-linux_x86_64.whl"">https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.5.0-cp27-none-linux_x86_64.whl</a> but conda doesn't even know how to install it.</p>

<p>Does anyone have any idea of how to work around this error?</p>
",
Tensorflow: Using Adam optimizer,"<p>I am experimenting with some simple models in tensorflow, including one that looks very similar to the first <a href=""http://www.tensorflow.org/tutorials/mnist/beginners/index.md"">MNIST for ML Beginners example</a>, but with a somewhat larger dimensionality. I am able to use the gradient descent optimizer with no problems, getting good enough convergence. When I try to use the ADAM optimizer, I get errors like this:</p>

<pre><code>tensorflow.python.framework.errors.FailedPreconditionError: Attempting to use uninitialized value Variable_21/Adam
     [[Node: Adam_2/update_Variable_21/ApplyAdam = ApplyAdam[T=DT_FLOAT, use_locking=false, _device=""/job:localhost/replica:0/task:0/cpu:0""](Variable_21, Variable_21/Adam, Variable_21/Adam_1, beta1_power_2, beta2_power_2, Adam_2/learning_rate, Adam_2/beta1, Adam_2/beta2, Adam_2/epsilon, gradients_11/add_10_grad/tuple/control_dependency_1)]]
</code></pre>

<p>where the specific variable that complains about being uninitialized changes depending on the run. What does this error mean? And what does it suggest is wrong? It seems to occur regardless of the learning rate I use.</p>
",Data Preparation: Shape mismatch
Dot product of two vectors in tensorflow,"<p>I was wondering if there is an easy way to calculate the dot product of two vectors (i.e. 1-d tensors) and return a scalar value in tensorflow.</p>

<p>Given two vectors X=(x1,...,xn) and Y=(y1,...,yn), the dot product is
dot(X,Y) = x1 * y1 + ... + xn * yn </p>

<p>I know that it is possible to achieve this by first broadcasting the vectors X and Y to a 2-d tensor and then using tf.matmul. However, the result is a matrix, and I am after a scalar.</p>

<p>Is there an operator like tf.matmul that is specific to vectors?</p>
",Related to library: Lack of knowledge
The minimum required Cuda capability is 3.5,"<p>After installing TensorFlow and its dependencies on a g2.2xlarge EC2 instance I tried to run an MNIST example from the getting started page: </p>

<pre><code>python tensorflow/models/image/mnist/convolutional.py
</code></pre>

<p>But I get the following warning:</p>

<pre><code>I tensorflow/core/common_runtime/gpu/gpu_device.cc:611] Ignoring gpu device 
(device: 0, name: GRID K520, pci bus id: 0000:00:03.0) with Cuda compute 
capability 3.0. The minimum required Cuda capability is 3.5.
</code></pre>

<p>Is this a hard requirement? Any chance I could comment that check out in a fork of TensorFlow? It would be super nice to be able to train models in AWS.</p>
",Related to library: Installation
How can I change the shape of a variable in TensorFlow?,"<p><a href=""http://www.tensorflow.org/how_tos/variables/index.md#creation"">TensorFlow tutorial</a> says that at creation time we need to specify the shape of tensors.  That shape automatically becomes the shape of the tensor.  It also says that TensorFlow provides advanced mechanisms to reshape variables. How can I do that? Any code example?</p>
",Related to library: Lack of knowledge
Error while importing Tensorflow in python2.7 in Ubuntu 12.04. 'GLIBC_2.17 not found',"<p>I have installed the Tensorflow bindings with python successfully. But when I try to import Tensorflow, I get the follwoing error.</p>

<blockquote>
  <p>ImportError: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_2.17' not
  found (required by
  /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow.so)</p>
</blockquote>

<p>I have tried to update GLIBC_2.15 to 2.17, but no luck.</p>
",Related to library: Lack of knowledge
"Unable to import Tensorflow ""No module named copyreg""","<p>El Capitan OS here. I've been trying to find a workaround with import Tensorflow into my ipython notebook, but so far no luck.</p>

<p>Like many people in the forums, I've also had issues with install tensorflow because of the six package. I was able to install after some fidgeting with brew</p>

<pre><code>brew link gdbm
brew install python
rew linkapps python
sudo pip install https://storage.googleapis.com/tensorflow/mac/tensorflow-0.5.0-py2-none-any.whl
</code></pre>

<p>I got a message that tensorflow was installed correctly. Even when I did <code>sudo pip install tensorflow</code> I get the message:</p>

<pre><code>Requirement already satisfied (use --upgrade to upgrade): tensorflow in /usr/local/lib/python2.7/site-packages
Requirement already satisfied (use --upgrade to upgrade): six&gt;=1.10.0 in /Library/Python/2.7/site-packages (from tensorflow)
Requirement already satisfied (use --upgrade to upgrade): numpy&gt;=1.9.2 in /usr/local/lib/python2.7/site-packages (from tensorflow)
</code></pre>

<p>However, when I'm on my ipython notebook and I did an <code>import tensorflow</code> I get the message: <code>ImportError: No module named tensorflow</code></p>

<p>I've dug further and found this error on the import as well:</p>

<pre><code>In [1]: import tensorflow
---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
&lt;ipython-input-1-a649b509054f&gt; in &lt;module&gt;()
----&gt; 1 import tensorflow

/usr/local/lib/python2.7/site-packages/tensorflow/__init__.py in &lt;module&gt;()
      2 # module.
      3 # pylint: disable=wildcard-import
----&gt; 4 from tensorflow.python import *

/usr/local/lib/python2.7/site-packages/tensorflow/python/__init__.py in &lt;module&gt;()
     11 
     12 import tensorflow.python.platform
---&gt; 13 from tensorflow.core.framework.graph_pb2 import *
     14 from tensorflow.core.framework.summary_pb2 import *
     15 from tensorflow.core.framework.config_pb2 import *

/usr/local/lib/python2.7/site-packages/tensorflow/core/framework/graph_pb2.py in &lt;module&gt;()
      6 from google.protobuf import descriptor as _descriptor
      7 from google.protobuf import message as _message
----&gt; 8 from google.protobuf import reflection as _reflection
      9 from google.protobuf import symbol_database as _symbol_database
     10 from google.protobuf import descriptor_pb2

/usr/local/lib/python2.7/site-packages/google/protobuf/reflection.py in &lt;module&gt;()
     56   from google.protobuf.pyext import cpp_message as message_impl
     57 else:
---&gt; 58   from google.protobuf.internal import python_message as message_impl
     59 
     60 # The type of all Message classes.

/usr/local/lib/python2.7/site-packages/google/protobuf/internal/python_message.py in &lt;module&gt;()
     57 
     58 import six
---&gt; 59 import six.moves.copyreg as copyreg
     60 
     61 # We use ""as"" to avoid name collisions with variables.

ImportError: No module named copyreg
</code></pre>
",Related to library: Lack of knowledge
where is the ./configure of TensorFlow and how to enable the GPU support?,"<p>When installing TensorFlow on my Ubuntu, I would like to use GPU with CUDA. </p>

<p>But I am stopped at this step in the <a href=""http://www.tensorflow.org/get_started/os_setup.md"" rel=""nofollow noreferrer"">Official Tutorial</a> :</p>

<p><a href=""https://i.stack.imgur.com/dmwOC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dmwOC.png"" alt=""enter image description here""></a></p>

<p>Where exactly is this <code>./configure</code> ? Or where is my root of source tree.</p>

<p>My TensorFlow is located here <code>/usr/local/lib/python2.7/dist-packages/tensorflow</code>.   But I still did not find <code>./configure</code>.  </p>

<p><strong>EDIT</strong></p>

<p>I have found the <code>./configure</code> according to <a href=""https://stackoverflow.com/a/33659119/4802797"">Salvador Dali's answer</a>. But when doing the example code, I got the following error:</p>

<pre><code>&gt;&gt;&gt; import tensorflow as tf
&gt;&gt;&gt; hello = tf.constant('Hello, TensorFlow!')
&gt;&gt;&gt; sess = tf.Session()
I tensorflow/core/common_runtime/local_device.cc:25] Local device intra op parallelism threads: 8
E tensorflow/stream_executor/cuda/cuda_driver.cc:466] failed call to cuInit: CUDA_ERROR_NO_DEVICE
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:86] kernel driver does not appear to be running on this host (cliu-ubuntu): /proc/driver/nvidia/version does not exist
I tensorflow/core/common_runtime/gpu/gpu_init.cc:112] DMA: 
I tensorflow/core/common_runtime/local_session.cc:45] Local session inter op parallelism threads: 8
</code></pre>

<p>The cuda device cannot be found. </p>

<p><strong>Answer</strong></p>

<p>See the answer about how did I enable GPU support <a href=""https://stackoverflow.com/a/36089757/4802797"">here</a>.</p>
",Related to library: Lack of knowledge
TensorFlow MNIST example not running with fully_connected_feed.py,"<p>I am able to run the <code>Deep MNIST Example</code> fine, but when running <code>fully_connected_feed.py</code>, I am getting the following error:</p>

<pre><code>File ""fully_connected_feed.py"", line 19, in &lt;module&gt;
from tensorflow.g3doc.tutorials.mnist import input_data ImportError: No module named
g3doc.tutorials.mnist
</code></pre>

<p>I am new to Python so could also just be a general setup problem.</p>
",Related to library: Lack of knowledge
Sorting an Array in TensorFlow,"<p>Let's assume I have an array in TensorFlow:</p>

<pre><code>[ 0.12300211,  0.51767069,  0.13886075,  0.55363625],
[ 0.47279349,  0.50432992,  0.48080254,  0.51576483],
[ 0.84347934,  0.44505221,  0.88839239,  0.48857492],
[ 0.93650454,  0.43652734,  0.96464157,  0.47236174], ..
</code></pre>

<p>I would like to sort this array by the third column. How do I do this? I am able to sort each column individually using <code>tf.nn.top_k()</code>, which gives me the sorted values and the respective indices. I could use the indices of this third column to reorder the others, but I cannot find a reordering Op.</p>

<p>Assuming I want to keep things in-graph (no Python shenanigans):</p>

<ul>
<li>How do I sort/order (the above array) in TensorFlow?</li>
<li>How do I re-order in TensorFlow when I have indices for re-ordering?</li>
</ul>
",Related to library: Lack of knowledge
How to create a Tensorflow Tensorboard Empty Graph,"<p>launch tensorboard with <code>tensorboard --logdir=/home/vagrant/notebook</code></p>

<p>at tensorboard:6006 > graph, it says No graph definition files were found.</p>

<p>To store a graph, create a tf.python.training.summary_io.SummaryWriter and pass the graph either via the constructor, or by calling its add_graph() method.</p>

<pre><code>import tensorflow as tf

sess = tf.Session()
writer = tf.python.training.summary_io.SummaryWriter(""/home/vagrant/notebook"", sess.graph_def)
</code></pre>

<p>However the page is still empty, how can I start playing with tensorboard?</p>

<h1>current tensorboard</h1>

<p><a href=""https://i.stack.imgur.com/UB8dX.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/UB8dX.png"" alt=""Current Tensorboard""></a></p>

<h1>result wanted</h1>

<p>An empty graph that can add nodes, editable.</p>

<h1>update</h1>

<p>Seems like tensorboard is unable to create a graph to add nodes, drag and edit etc ( I am confused by the official video ).</p>

<p>running <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/tutorials/mnist/fully_connected_feed.py"" rel=""noreferrer"">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/tutorials/mnist/fully_connected_feed.py</a> and then <code>tensorboard --logdir=/home/vagrant/notebook/data</code> is able to view the graph</p>

<p>However seems like tensorflow only provide ability to view summary, nothing much different to make it standout</p>
",Related to library: Lack of knowledge
import input_data MNIST tensorflow not working,"<p><a href=""https://stackoverflow.com/questions/33659424/tensorflow-mnist-example-not-running"">TensorFlow MNIST example not running with fully_connected_feed.py</a></p>

<p>I checked this out and realized that <code>input_data</code> was not built-in.  So I downloaded the whole folder from <a href=""https://tensorflow.googlesource.com/tensorflow/+/master/tensorflow/g3doc/tutorials/mnist/input_data.py"" rel=""nofollow noreferrer"">here</a>. How can I start the tutorial:</p>

<pre><code>import input_data
mnist = input_data.read_data_sets(""MNIST_data/"", one_hot=True)


---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
&lt;ipython-input-6-a5af65173c89&gt; in &lt;module&gt;()
----&gt; 1 import input_data
      2 mnist = tf.input_data.read_data_sets(""MNIST_data/"", one_hot=True)

ImportError: No module named input_data
</code></pre>

<p>I'm using iPython (Jupyter) so do I need to change my working directory to this folder I downloaded? or can I add this to my <code>tensorflow</code> directory? If so, where do I add the files? I installed <code>tensorflow</code> with <code>pip</code> (on my OSX) and the current location is <code>~/anaconda/lib/python2.7/site-packages/tensorflow/__init__.py</code></p>

<p>Are these files meant to be accessed directly through <code>tensorflow</code> like <code>sklearn</code> datasets? or am I just supposed to cd into the directory and work from there? The example is not clear. </p>
",Related to library: Lack of knowledge
'Library not loaded: @rpath/libcudart.7.5.dylib' TensorFlow Error on Mac,"<p>I'm using OS X El Capitan (10.11.4).</p>

<p>I just downloaded TensorFlow using the pip install instructions <a href=""https://www.tensorflow.org/versions/r0.10/get_started/os_setup.html"">here</a>.</p>

<p>Everything went pretty smoothly, though I did get a few warning messages like:</p>

<p><code>The directory '/Users/myusername/Library/Caches/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want the -H flag.</code></p>

<p>and</p>

<p><code>You are using pip version 6.0.8, however version 8.1.2 is available.</code> Even though I just installed pip.</p>

<p>Then, when I tested TensorFlow in Python, I got the error:</p>

<pre><code>&gt;&gt;&gt; import tensorflow as tf
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/tensorflow/__init__.py"", line 23, in &lt;module&gt;
    from tensorflow.python import *
  File ""/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/tensorflow/python/__init__.py"", line 48, in &lt;module&gt;
    from tensorflow.python import pywrap_tensorflow
  File ""/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 28, in &lt;module&gt;
    _pywrap_tensorflow = swig_import_helper()
  File ""/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)
  File ""/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
ImportError: dlopen(/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/tensorflow/python/_pywrap_tensorflow.so, 10): Library not loaded: @rpath/libcudart.7.5.dylib
  Referenced from: /Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/tensorflow/python/_pywrap_tensorflow.so
  Reason: image not found
</code></pre>

<p>Now, when I try to do <code>pip uninstall tensorflow-0.10.0rc0</code> it tells me that it's not installed.</p>

<p>The closest thing I've found to resembling this problem is <a href=""https://github.com/tensorflow/tensorflow/issues/2278"">this issue</a> in the TensorFlow GitHub docs (which I have not tried).</p>

<p>How can I uninstall whatever it did install and get TensorFlow up and running correctly?</p>
",Related to library: Lack of knowledge
Are there any examples of anomaly detection algorithms implemented with TensorFlow?,"<p>I'm fairly new to this subject and I am working on a project that deals with detecting anomalies in time-series data. I want to use TensorFlow so that I could potentially deploy the model onto a mobile device. I'm having a difficult time finding relevant material and examples of anomaly detection algorithms implemented in TensorFlow. </p>

<p>Some algorithms I'm looking into are clustering algorithms for classifying windowed samples and Holt-Winters for streaming data. </p>

<p>Any example would help me tremendously! </p>
",Related to library: Lack of knowledge
tensorflow deep neural network for regression always predict same results in one batch,"<p>I use a tensorflow to implement a simple multi-layer perceptron for regression. The code is modified from standard mnist classifier, that I only changed the output cost to MSE (use <code>tf.reduce_mean(tf.square(pred-y))</code>), and some input, output size settings. However, if I train the network using regression, after several epochs, the output batch are totally the same. for example:</p>

<pre><code>target: 48.129, estimated: 42.634
target: 46.590, estimated: 42.634
target: 34.209, estimated: 42.634
target: 69.677, estimated: 42.634
......
</code></pre>

<p>I have tried different batch size, different initialization, input normalization using sklearn.preprocessing.scale (my inputs range are quite different). However, none of them worked. I have also tried one of sklearn example from Tensorflow (<a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/skflow/boston.py"">Deep Neural Network Regression with Boston Data</a>). But I got another error in line 40:</p>

<p>'module' object has no attribute 'infer_real_valued_columns_from_input'</p>

<p>Anyone has clues on where the problem is? Thank you</p>

<p>My code is listed below, may be a little bit long, but very straghtforward:</p>

<pre><code>from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import tensorflow as tf
from tensorflow.contrib import learn
import matplotlib.pyplot as plt

from sklearn.pipeline import Pipeline
from sklearn import datasets, linear_model
from sklearn import cross_validation
import numpy as np

boston = learn.datasets.load_dataset('boston')
x, y = boston.data, boston.target
X_train, X_test, Y_train, Y_test = cross_validation.train_test_split(
x, y, test_size=0.2, random_state=42)

total_len = X_train.shape[0]

# Parameters
learning_rate = 0.001
training_epochs = 500
batch_size = 10
display_step = 1
dropout_rate = 0.9
# Network Parameters
n_hidden_1 = 32 # 1st layer number of features
n_hidden_2 = 200 # 2nd layer number of features
n_hidden_3 = 200
n_hidden_4 = 256
n_input = X_train.shape[1]
n_classes = 1

# tf Graph input
x = tf.placeholder(""float"", [None, 13])
y = tf.placeholder(""float"", [None])

# Create model
def multilayer_perceptron(x, weights, biases):
    # Hidden layer with RELU activation
    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])
    layer_1 = tf.nn.relu(layer_1)

    # Hidden layer with RELU activation
    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])
    layer_2 = tf.nn.relu(layer_2)

    # Hidden layer with RELU activation
    layer_3 = tf.add(tf.matmul(layer_2, weights['h3']), biases['b3'])
    layer_3 = tf.nn.relu(layer_3)

    # Hidden layer with RELU activation
    layer_4 = tf.add(tf.matmul(layer_3, weights['h4']), biases['b4'])
    layer_4 = tf.nn.relu(layer_4)

    # Output layer with linear activation
    out_layer = tf.matmul(layer_4, weights['out']) + biases['out']
    return out_layer

# Store layers weight &amp; bias
weights = {
    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1], 0, 0.1)),
    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2], 0, 0.1)),
    'h3': tf.Variable(tf.random_normal([n_hidden_2, n_hidden_3], 0, 0.1)),
    'h4': tf.Variable(tf.random_normal([n_hidden_3, n_hidden_4], 0, 0.1)),
    'out': tf.Variable(tf.random_normal([n_hidden_4, n_classes], 0, 0.1))
}
biases = {
    'b1': tf.Variable(tf.random_normal([n_hidden_1], 0, 0.1)),
    'b2': tf.Variable(tf.random_normal([n_hidden_2], 0, 0.1)),
    'b3': tf.Variable(tf.random_normal([n_hidden_3], 0, 0.1)),
    'b4': tf.Variable(tf.random_normal([n_hidden_4], 0, 0.1)),
    'out': tf.Variable(tf.random_normal([n_classes], 0, 0.1))
}

# Construct model
pred = multilayer_perceptron(x, weights, biases)

# Define loss and optimizer
cost = tf.reduce_mean(tf.square(pred-y))
optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)

# Launch the graph
with tf.Session() as sess:
    sess.run(tf.initialize_all_variables())

    # Training cycle
    for epoch in range(training_epochs):
        avg_cost = 0.
        total_batch = int(total_len/batch_size)
        # Loop over all batches
        for i in range(total_batch-1):
            batch_x = X_train[i*batch_size:(i+1)*batch_size]
            batch_y = Y_train[i*batch_size:(i+1)*batch_size]
            # Run optimization op (backprop) and cost op (to get loss value)
            _, c, p = sess.run([optimizer, cost, pred], feed_dict={x: batch_x,
                                                          y: batch_y})
            # Compute average loss
            avg_cost += c / total_batch

        # sample prediction
        label_value = batch_y
        estimate = p
        err = label_value-estimate
        print (""num batch:"", total_batch)

        # Display logs per epoch step
        if epoch % display_step == 0:
            print (""Epoch:"", '%04d' % (epoch+1), ""cost="", \
                ""{:.9f}"".format(avg_cost))
            print (""[*]----------------------------"")
            for i in xrange(3):
                print (""label value:"", label_value[i], \
                    ""estimated value:"", estimate[i])
            print (""[*]============================"")

    print (""Optimization Finished!"")

    # Test model
    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))
    # Calculate accuracy
    accuracy = tf.reduce_mean(tf.cast(correct_prediction, ""float""))
    print (""Accuracy:"", accuracy.eval({x: X_test, y: Y_test}))
</code></pre>
",Training
"Tensorflow install fails with ""compiletime version 3.5 of module does not match runtime version 3.6""","<p>I tried installing from pip:</p>

<pre><code>pip3 install --user --no-cache https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.4.0-cp36-cp36m-linux_x86_64.whl
</code></pre>

<p>Then tried importing and got:</p>

<pre><code> Using TensorFlow backend.
  /usr/lib64/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: 
  compiletime version 3.5 of module 
  'tensorflow.python.framework.fast_tensor_util' does not match runtime 
  version 3.6
    return f(*args, **kwds)

  2017-11-10 09:35:01.206112: I 
  tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports 
  instructions that this TensorFlow binary was not compiled to use: SSE4.1 
  SSE4.2 AVX
</code></pre>

<p>Questions:</p>

<ol>
<li><p>I don't understand why the wheel says 3.6, but I get the warning about 3.5</p></li>
<li><p>I want to compile to optimize for my cpu, so can I use pip to install from source rather than from binary wheel?</p></li>
</ol>
",Related to library: Installation
How do I find out the version of TensorFlow on my computer?,"<p>What's the command to find out the version of TensorFlow on my computer? I installed TensorFlow on my computer some time ago and want to make sure that I have the latest version.</p>
",Related to library: Installation
How does one debug NaN values in TensorFlow?,"<p>I was running TensorFlow and I happen to have something yielding a NaN. I'd like to know what it is but I do not know how to do this. The main issue is that in a ""normal"" procedural program I would just write a print statement just before the operation is executed. The issue with TensorFlow is that I cannot do that because I first declare (or define) the graph, so adding print statements to the graph definition does not help. Are there any rules, advice, heuristics, anything to track down what might be causing the NaN?</p>

<hr>

<p>In this case I know more precisely what line to look at because I have the following:</p>

<pre><code>Delta_tilde = 2.0*tf.matmul(x,W) - tf.add(WW, XX) #note this quantity should always be positive because its pair-wise euclidian distance
Z = tf.sqrt(Delta_tilde)
Z = Transform(Z) # potentially some transform, currently I have it to return Z for debugging (the identity)
Z = tf.pow(Z, 2.0)
A = tf.exp(Z) 
</code></pre>

<p>when this line is present I have it that it returns NaN as declared by my summary writers. Why is this? Is there a way to at least explore what value Z has after its being square rooted?</p>

<hr>

<p>For the specific example I posted, I tried <code>tf.Print(0,Z)</code> but with no success it printed nothing. As in:</p>

<pre><code>Delta_tilde = 2.0*tf.matmul(x,W) - tf.add(WW, XX) #note this quantity should always be positive because its pair-wise euclidian distance
Z = tf.sqrt(Delta_tilde)
tf.Print(0,[Z]) # &lt;-------- TF PRINT STATMENT
Z = Transform(Z) # potentially some transform, currently I have it to return Z for debugging (the identity)
Z = tf.pow(Z, 2.0)
A = tf.exp(Z) 
</code></pre>

<p>I actually don't understand what <code>tf.Print</code> is suppose to do. Why does it need two arguments? If I want to print 1 tensor why would I need to pass 2? Seems bizarre to me.</p>

<hr>

<p>I was looking at the function <a href=""https://www.tensorflow.org/versions/r0.9/api_docs/python/control_flow_ops.html#add_check_numerics_ops"" rel=""noreferrer"">tf.add_check_numerics_ops()</a> but it doesn't say how to use it (plus the docs seem to not be super helpful). Does anyone know how to use this?</p>

<hr>

<p>Since I've had comments addressing the data might be bad, I am using standard MNIST. However, I am computing a quantity that is positive (pair-wise eucledian distance) and then square rooting it. Thus, I wouldn't see how the data specifically would be an issue.</p>
",Related to library: Lack of knowledge
tensorflow.train.import_meta_graph does not work?,"<p>I try to simply save and restore a graph, but the simplest example does not work as expected (this is done using version 0.9.0 or 0.10.0 on Linux 64 without CUDA using python 2.7 or 3.5.2)</p>

<p>First I save the graph like this:</p>

<pre><code>import tensorflow as tf
v1 = tf.placeholder('float32') 
v2 = tf.placeholder('float32')
v3 = tf.mul(v1,v2)
c1 = tf.constant(22.0)
v4 = tf.add(v3,c1)
sess = tf.Session()
result = sess.run(v4,feed_dict={v1:12.0, v2:3.3})
g1 = tf.train.export_meta_graph(""file"")
## alternately I also tried:
## g1 = tf.train.export_meta_graph(""file"",collection_list=[""v4""])
</code></pre>

<p>This creates a file ""file"" that is non-empty and also sets g1 to something that looks like a proper graph definition.</p>

<p>Then I try to restore this graph:</p>

<pre><code>import tensorflow as tf
g=tf.train.import_meta_graph(""file"")
</code></pre>

<p>This works without an error, but does not return anything at all. </p>

<p>Can anyone provide the necessary code to simply just save the graph for ""v4"" and completely restore it so that running this in a new session will produce the same result?</p>
",Evaluation
How do I pass a scalar via a TensorFlow feed dictionary,"<p>My TensorFlow model uses <code>tf.random_uniform</code> to initialize a variable. I would like to specify the range when I begin training, so I created a placeholder for the initialization value.</p>

<pre><code>init = tf.placeholder(tf.float32, name=""init"")
v = tf.Variable(tf.random_uniform((100, 300), -init, init), dtype=tf.float32)
initialize = tf.initialize_all_variables()
</code></pre>

<p>I initialize variables at the start of training like so.</p>

<pre><code>session.run(initialize, feed_dict={init: 0.5})
</code></pre>

<p>This gives me the following error:</p>

<pre><code>ValueError: initial_value must have a shape specified: Tensor(""Embedding/random_uniform:0"", dtype=float32)
</code></pre>

<p>I cannot figure out the correct <code>shape</code> parameter to pass to <code>tf.placeholder</code>. I would think for a scalar I should do <code>init = tf.placeholder(tf.float32, shape=0, name=""init"")</code> but this gives the following error:</p>

<pre><code>ValueError: Incompatible shapes for broadcasting: (100, 300) and (0,)
</code></pre>

<p>If I replace <code>init</code> with the literal value <code>0.5</code> in the call to <code>tf.random_uniform</code> it works.</p>

<p>How do I pass this scalar initial value via the feed dictionary?</p>
",Evaluation
How do I set TensorFlow RNN state when state_is_tuple=True?,"<p>I have written an <a href=""https://github.com/wpm/tfrnnlm"" rel=""nofollow noreferrer"">RNN language model using TensorFlow</a>. The model is implemented as an <code>RNN</code> class. The graph structure is built in the constructor, while <code>RNN.train</code> and <code>RNN.test</code> methods run it.</p>

<p>I want to be able to reset the RNN state when I move to a new document in the training set, or when I want to run a validation set during training. I do this by managing the state inside the training loop, passing it into the graph via a feed dictionary.</p>

<p>In the constructor I define the the RNN like so</p>

<pre><code>    cell = tf.nn.rnn_cell.LSTMCell(hidden_units)
    rnn_layers = tf.nn.rnn_cell.MultiRNNCell([cell] * layers)
    self.reset_state = rnn_layers.zero_state(batch_size, dtype=tf.float32)
    self.state = tf.placeholder(tf.float32, self.reset_state.get_shape(), ""state"")
    self.outputs, self.next_state = tf.nn.dynamic_rnn(rnn_layers, self.embedded_input, time_major=True,
                                                  initial_state=self.state)
</code></pre>

<p>The training loop looks like this</p>

<pre><code> for document in document:
     state = session.run(self.reset_state)
     for x, y in document:
          _, state = session.run([self.train_step, self.next_state], 
                                 feed_dict={self.x:x, self.y:y, self.state:state})
</code></pre>

<p><code>x</code> and <code>y</code> are batches of training data in a document. The idea is that I pass the latest state along after each batch, except when I start a new document, when I zero out the state by running <code>self.reset_state</code>.</p>

<p>This all works.  Now I want to change my RNN to use the recommended <code>state_is_tuple=True</code>. However, I don't know how to pass the more complicated LSTM state object via a feed dictionary. Also I don't know what arguments to pass to the <code>self.state = tf.placeholder(...)</code> line in my constructor.</p>

<p>What is the correct strategy here? There still isn't much example code or documentation for <code>dynamic_rnn</code> available.</p>

<hr>

<p>TensorFlow issues <a href=""https://github.com/tensorflow/tensorflow/issues/2695"" rel=""nofollow noreferrer"">2695</a> and <a href=""https://github.com/tensorflow/tensorflow/issues/2838"" rel=""nofollow noreferrer"">2838</a> appear relevant.</p>

<p>A <a href=""http://www.wildml.com/2016/08/rnns-in-tensorflow-a-practical-guide-and-undocumented-features/"" rel=""nofollow noreferrer"">blog post</a> on WILDML addresses these issues but doesn't directly spell out the answer.</p>

<p>See also <a href=""https://stackoverflow.com/questions/38241410/tensorflow-remember-lstm-state-for-next-batch-stateful-lstm"">TensorFlow: Remember LSTM state for next batch (stateful LSTM)</a>.</p>
",Choosing a model: Lack of knowledge
system auto reboot when tensorflow model is too large,"<p>I'm using a nvidia GTX1080 gpu(8GB) to run <a href=""https://github.com/tensorflow/models/tree/master/inception"" rel=""nofollow noreferrer"">Inception model</a> on tensorflow, when I set batch_size = 16 and image_size = 400, then after I start the program, my ubuntu14.04 will auto reboot.</p>
",Choosing a model: Lack of knowledge
Tensorflow Documentation,"<p>I am increasingly irritated and frustrated by the <code>Tensorflow</code> documentation. I searched on google for documentation regarding </p>

<pre><code>tf.reshape
</code></pre>

<p>I'm getting directed to a generic page like <a href=""https://www.tensorflow.org/versions/r0.9/api_docs/python/array_ops.html"" rel=""nofollow noreferrer"">here</a>. I want to see the details of <code>tf.reshape</code> and not the entirety of the documentation.</p>

<p>Am I doing something wrong here?</p>
",Related to library: Lack of knowledge
What is the difference between TF Learn (aka Scikit Flow) and TFLearn (aka TFLearn.org),"<p>There are two TFLearn projects</p>

<pre><code>TF Learn (aka Scikit Flow)
https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/learn/python/learn  
</code></pre>

<p>and</p>

<pre><code>TFLearn: Deep learning library featuring a higher-level API for TensorFlow.
https://github.com/tflearn/tflearn
</code></pre>

<p>what is the status of these projects, are they going to stay separate or they going to merged together?</p>
",
tensorflow: efficient feeding of eval/train data using queue runners,"<p>I'm trying to run a tensorflow graph to train a model and periodically evaluate using a separate evaluation dataset. Both training and evaluation data is implemented using queue runners.</p>

<p>My current solution is to create both inputs in the same graph and use a <code>tf.cond</code> dependent on an <code>is_training</code> placeholder. My issue is highlighted by the following code:</p>

<pre><code>import tensorflow as tf
from tensorflow.models.image.cifar10 import cifar10
from time import time


def get_train_inputs(is_training):
    return cifar10.inputs(False)


def get_eval_inputs(is_training):
    return cifar10.inputs(True)


def get_mixed_inputs(is_training):
    train_inputs = get_train_inputs(None)
    eval_inputs = get_eval_inputs(None)

    return tf.cond(is_training, lambda: train_inputs, lambda: eval_inputs)


def time_inputs(inputs_fn, n_runs=10):
    graph = tf.Graph()
    with graph.as_default():
        is_training = tf.placeholder(dtype=tf.bool, shape=(),
                                     name='is_training')
        images, labels = inputs_fn(is_training)

    with tf.Session(graph=graph) as sess:
        coordinator = tf.train.Coordinator()
        threads = tf.train.start_queue_runners(sess=sess, coord=coordinator)
        t = time()
        for i in range(n_runs):
            im, l = sess.run([images, labels], feed_dict={is_training: True})
        dt = time() - t
        coordinator.request_stop()
        coordinator.join(threads)

    return dt / n_runs

print('Train inputs: %.3f' % time_inputs(get_train_inputs))
print('Eval inputs: %.3f' % time_inputs(get_eval_inputs))
print('Mixed inputs: %.3f' % time_inputs(get_mixed_inputs))
</code></pre>

<p>I also had to comment out the <code>image_summary</code> line <code>133</code> of <code>tensorflow/models/image/cifar10/cifar10_inputs.py</code>.</p>

<p>This yielded the following results:</p>

<pre><code>Train inputs: 0.055
Eval inputs: 0.050
Mixed inputs: 0.105
</code></pre>

<p>It would seem in the mixed case both inputs are being read/parsed, even though only 1 is used. Is there a way of avoiding this redundant computation? Or is there a nicer way of switching between training/evaluation data that still leverages the queue-runner setup?</p>
",Related to library: Lack of knowledge
"In TensorFlow, how can I get nonzero values and their indices from a tensor with python?","<p>I want to do something like this.<br>
Let's say we have a tensor A.  </p>

<pre><code>A = [[1,0],[0,4]]
</code></pre>

<p>And I want to get nonzero values and their indices from it.  </p>

<pre><code>Nonzero values: [1,4]  
Nonzero indices: [[0,0],[1,1]]
</code></pre>

<p>There are similar operations in Numpy.<br>
<code>np.flatnonzero(A)</code> return indices that are non-zero in the flattened A.<br>
<code>x.ravel()[np.flatnonzero(x)]</code> extract elements according to non-zero indices.<br>
Here's <a href=""http://docs.scipy.org/doc/numpy-1.10.1/reference/generated/numpy.flatnonzero.html"" rel=""noreferrer"">a link</a> for these operations.</p>

<p>How can I do somthing like above Numpy operations in Tensorflow with python?<br>
(Whether a matrix is flattened or not doesn't really matter.)</p>
",
tensorflow not found in pip,"<p>I'm trying to intstall tensorflow</p>

<pre><code>    pip install tensorflow --user
Collecting tensorflow
  Could not find a version that satisfies the requirement tensorflow (from versions: )
No matching distribution found for tensorflow
</code></pre>

<p>What am I doing wrong? So far I've used Python and pip and with no issues.</p>
",Related to library: Lack of knowledge
TensorFlow strings: what they are and how to work with them,"<p>When I read file with <code>tf.read_file</code> I get something with type <code>tf.string</code>. Documentation says only that it is ""Variable length byte arrays. Each element of a Tensor is a byte array."" (<a href=""https://www.tensorflow.org/versions/r0.10/resources/dims_types.html"" rel=""nofollow noreferrer"">https://www.tensorflow.org/versions/r0.10/resources/dims_types.html</a>). I have no idea how to interpret this.</p>

<p>I can do nothing with this type. In usual python you can get elements by index like <code>my_string[:4]</code>, but when I run following code I get an error.</p>

<pre><code>import tensorflow as tf
import numpy as np

x = tf.constant(""This is string"")
y = x[:4]


init = tf.initialize_all_variables()
sess = tf.Session()
sess.run(init)
result = sess.run(y)
print result
</code></pre>

<p>It says </p>

<pre>  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_shape.py"", line 621, in assert_has_rank
    raise ValueError(""Shape %s must have rank %d"" % (self, rank))
ValueError: Shape () must have rank 1
</pre>

<p>Also I cannot convert my string to <code>tf.float32</code> tensor. It is <code>.flo</code> file and it has magic header ""PIEH"". This numpy code successfuly convert such header into number (see example here <a href=""https://stackoverflow.com/a/28016469/4744283"">https://stackoverflow.com/a/28016469/4744283</a>) but I can't do that with tensorflow. I tried <code>tf.string_to_number(string, out_type=tf.float32)</code> but it says </p>

<pre>tensorflow.python.framework.errors.InvalidArgumentError: StringToNumberOp could not correctly convert string: PIEH
</pre>

<p>So, what string is? What it's shape is? How can I at least get part of the string? I suppose that if I can get part of it I can just skip ""PIEH"" part.</p>

<p><strong>UPD</strong>: I forgot to say that <code>tf.slice(string, [0], [4])</code> also doesn't work with same error.</p>
",Related to library: Lack of knowledge
What is a local variable in tensorflow?,"<p>Tensorflow has this API defined:</p>

<blockquote>
  <p><code>tf.local_variables()</code></p>
  
  <p>Returns all variables created with <code>collection=[LOCAL_VARIABLES]</code>.</p>
  
  <p>Returns:</p>
  
  <p>A list of local Variable objects.</p>
</blockquote>

<p>What exactly is a local variable in TensorFlow? Can someone give me an example?</p>
",Related to library: Lack of knowledge
TensorFlow image operations for batches,"<p>There are a number of image operations in TensorFlow used for distorting input images during training, e.g. <code>tf.image.random_flip_left_right(image, seed=None)</code> and <code>tf.image.random_brightness(image, max_delta, seed=None)</code> and several others.</p>

<p>These functions are made for single images (i.e. 3-D tensors with shape [height, width, color-channel]). How can I make them work on a batch of images (i.e. 4-D tensors with shape [batch, height, width, color-channel])?</p>

<p>A working example would be greatly appreciated!</p>
",Related to library: Lack of knowledge
TensorFlow REST Frontend but not TensorFlow Serving,"<p>I want to deploy a simple TensorFlow model and run it in REST service like Flask.
Did not find so far good example on github or here.</p>

<p>I am not ready to use TF Serving as suggested in other posts, it is perfect solution for Google but it overkill for my tasks with gRPC, bazel, C++ coding, protobuf...</p>
",Related to library: Lack of knowledge
Keras - How are batches and epochs used in fit_generator()?,"<p>I have a video of 8000 frames, and I'd like to train a Keras model on batches of 200 frames each. I have a frame generator that loops through the video frame-by-frame and accumulates the (3 x 480 x 640) frames into a numpy matrix <code>X</code> of shape <code>(200, 3, 480, 640)</code> -- (batch size, rgb, frame height, frame width) -- and yields <code>X</code> and <code>Y</code> every 200th frame:</p>

<pre><code>import cv2
...
def _frameGenerator(videoPath, dataPath, batchSize):
    """"""
    Yield X and Y data when the batch is filled.
    """"""
    camera = cv2.VideoCapture(videoPath)
    width = camera.get(3)
    height = camera.get(4)
    frameCount = int(camera.get(7))  # Number of frames in the video file.

    truthData = _prepData(dataPath, frameCount)

    X = np.zeros((batchSize, 3, height, width))
    Y = np.zeros((batchSize, 1))

    batch = 0
    for frameIdx, truth in enumerate(truthData):
        ret, frame = camera.read()
        if ret is False: continue

        batchIndex = frameIdx%batchSize

        X[batchIndex] = frame
        Y[batchIndex] = truth

        if batchIndex == 0 and frameIdx != 0:
            batch += 1
            print ""now yielding batch"", batch
            yield X, Y
</code></pre>

<p>Here's how run <a href=""https://keras.io/models/model/"" rel=""noreferrer""><code>fit_generator()</code></a>:</p>

<pre><code>        batchSize = 200
        print ""Starting training...""
        model.fit_generator(
            _frameGenerator(videoPath, dataPath, batchSize),
            samples_per_epoch=8000,
            nb_epoch=10,
            verbose=args.verbosity
        )
</code></pre>

<p>My understanding is an epoch finishes when <code>samples_per_epoch</code> samples have been seen by the model, and <code>samples_per_epoch</code> = batch size * number of batches = 200 * 40. So after training for an epoch on frames 0-7999, the next epoch will start training again from frame 0. Is this correct?</p>

<p>With this setup <strong>I expect 40 batches (of 200 frames each) to be passed from the generator to <code>fit_generator</code>, per epoch; this would be 8000 total frames per epoch</strong> -- i.e., <code>samples_per_epoch=8000</code>. Then for subsequent epochs, <code>fit_generator</code> would reinitialize the generator such that we begin training again from the start of the video. Yet this is not the case. <strong>After the first epoch is complete (after the model logs batches 0-24), the generator picks up where it left off. Shouldn't the new epoch start again from the beginning of the training dataset?</strong></p>

<p>If there is something incorrect in my understanding of <code>fit_generator</code> please explain. I've gone through the documentation, this <a href=""https://github.com/fchollet/keras/blob/master/examples/cifar10_cnn.py"" rel=""noreferrer"">example</a>, and these <a href=""https://github.com/fchollet/keras/issues/1627"" rel=""noreferrer"">related</a> <a href=""https://github.com/fchollet/keras/issues/107"" rel=""noreferrer"">issues</a>. I'm using Keras v1.0.7 with the TensorFlow backend. This issue is also posted in the <a href=""https://github.com/fchollet/keras/issues/3461"" rel=""noreferrer"">Keras repo</a>.</p>
",Related to library: Lack of knowledge
What does it mean to unroll a RNN dynamically?,"<p>What does it mean to ""unroll a RNN dynamically"". I've seen this specifically mentioned in the Tensorflow source code, but I'm looking for a conceptual explanation that extends to RNN in general. </p>

<p>In the tensorflow <code>rnn</code> method, it is documented:</p>

<blockquote>
  <p>If the <code>sequence_length</code> vector is provided, dynamic calculation is
  performed. This method of calculation does not compute the RNN steps
  past the maximum sequence length of the minibatch (thus saving
  computational time),</p>
</blockquote>

<p>But in the <code>dynamic_rnn</code> method it mentions:</p>

<blockquote>
  <p>The parameter <code>sequence_length</code> is optional and is used to
  copy-through state and zero-out outputs when past a batch element's
  sequence length. So it's more for correctness than performance,
  unlike in <code>rnn()</code>.</p>
</blockquote>

<p>So does this mean <code>rnn</code> is more performant for variable length sequences? What is the conceptual difference between <code>dynamic_rnn</code> and <code>rnn</code>?</p>
",Hyper parameter tuning
TensorFlow saving into/loading a graph from a file,"<p>From what I've gathered so far, there are several different ways of dumping a TensorFlow graph into a file and then loading it into another program, but I haven't been able to find clear examples/information on how they work. What I already know is this:</p>

<ol>
<li>Save the model's variables into a checkpoint file (.ckpt) using a <code>tf.train.Saver()</code> and restore them later (<a href=""https://www.tensorflow.org/versions/r0.10/how_tos/variables/index.html"" rel=""noreferrer"">source</a>)</li>
<li>Save a model into a .pb file and load it back in using <code>tf.train.write_graph()</code> and <code>tf.import_graph_def()</code> (<a href=""https://github.com/tensorflow/tensorflow/issues/616"" rel=""noreferrer"">source</a>)</li>
<li>Load in a model from a .pb file, retrain it, and dump it into a new .pb file using Bazel (<a href=""https://petewarden.com/2016/02/28/tensorflow-for-poets/"" rel=""noreferrer"">source</a>)</li>
<li>Freeze the graph to save the graph and weights together (<a href=""https://www.tensorflow.org/versions/r0.9/how_tos/tool_developers/index.html#freezing"" rel=""noreferrer"">source</a>)</li>
<li>Use <code>as_graph_def()</code> to save the model, and for weights/variables, map them into constants (<a href=""https://stackoverflow.com/questions/34343259/is-there-an-example-on-how-to-generate-protobuf-files-holding-trained-tensorflow"">source</a>)</li>
</ol>

<p>However, I haven't been able to clear up several questions regarding these different methods:</p>

<ol>
<li>Regarding checkpoint files, do they only save the trained weights of a model? Could checkpoint files be loaded into a new program, and be used to run the model, or do they simply serve as ways to save the weights in a model at a certain time/stage?</li>
<li>Regarding <code>tf.train.write_graph()</code>, are the weights/variables saved as well?</li>
<li>Regarding Bazel, can it only save into/load from .pb files for retraining? Is there a simple Bazel command just to dump a graph into a .pb?</li>
<li>Regarding freezing, can a frozen graph be loaded in using <code>tf.import_graph_def()</code>?</li>
<li>The Android demo for TensorFlow loads in Google's Inception model from a .pb file. If I wanted to substitute my own .pb file, how would I go about doing that? Would I need to change any native code/methods?</li>
<li>In general, what exactly is the difference between all these methods? Or more broadly, what is the difference between <code>as_graph_def()</code>/.ckpt/.pb?</li>
</ol>

<p>In short, what I'm looking for is a method to save both a graph (as in, the various operations and such) and its weights/variables into a file, which can then be used to load the graph and weights into another program, for use (not necessarily continuing/retraining).</p>

<p>Documentation about this topic isn't very straightforward, so any answers/information would be greatly appreciated.</p>
",Choosing a model
Is Intel based graphic card compatible with tensorflow/GPU?,"<p>Is this graphic card compatible with tensorflow/GPU ?</p>

<pre><code>*-display               
   description: VGA compatible controller
   product: Haswell-ULT Integrated Graphics Controller
   vendor: Intel Corporation
   physical id: 2
   bus info: pci@0000:00:02.0
   version: 09
   width: 64 bits
   clock: 33MHz
   capabilities: msi pm vga_controller bus_master cap_list rom
   configuration: driver=i915 latency=0
   resources: irq:44 memory:c2000000-c23fffff memory:b0000000-bfffffff ioport:7000(size=64)
</code></pre>
",Related to library: Lack of knowledge
Tensorflow while_loop for training,"<p>In my problem I need run GD with 1 example from data on each training step. It's known problem that session.run() has overhead and therefore it is too long to train model. 
In attempt to avoid overhead I tried to use while_loop and train model on all data with one run() call. But it approach don't work and train_op don't execute even ones. Below simple example of what I'm doing:</p>

<pre><code>data = [k*1. for k in range(10)]
tf.reset_default_graph()

i = tf.Variable(0, name='loop_i')
q_x = tf.FIFOQueue(100000, tf.float32)
q_y = tf.FIFOQueue(100000, tf.float32)

x = q_x.dequeue()
y = q_y.dequeue()
w = tf.Variable(0.)
b = tf.Variable(0.)
loss = (tf.add(tf.mul(x, w), b) - y)**2

gs = tf.Variable(0)

train_op = tf.train.GradientDescentOptimizer(0.05).minimize(loss, global_step=gs)

s = tf.Session()
s.run(tf.initialize_all_variables())

def cond(i):
    return i &lt; 10

def body(i):
    return tf.tuple([tf.add(i, 1)], control_inputs=[train_op])


loop = tf.while_loop(cond, body, [i])

for _ in range(1):
    s.run(q_x.enqueue_many((data, )))
    s.run(q_y.enqueue_many((data, )))

s.run(loop)
s.close()
</code></pre>

<p>What I'm doing wrong? Or there is another solution of this problem with too expensive overhead?</p>

<p>Thanks!</p>
",Related to library: Lack of knowledge
"Keras + tensorflow gives the error ""no attribute 'control_flow_ops'""","<p>I am trying to run keras for the first time.  I installed the modules with:</p>

<pre><code>pip install keras --user
pip install tensorflow --user
</code></pre>

<p>and then tried to run <a href=""https://github.com/fchollet/keras/blob/master/examples/mnist_cnn.py"">https://github.com/fchollet/keras/blob/master/examples/mnist_cnn.py</a>.</p>

<p>However it gives me:</p>

<pre><code>AttributeError: 'module' object has no attribute 'control_flow_ops'
</code></pre>

<p>These are the versions I am using.</p>

<pre><code>print tensorflow.__version__
0.11.0rc0
print keras.__version__
1.1.0
</code></pre>

<blockquote>
  <p>What can I do to get keras to run with tensorflow?</p>
</blockquote>
",Traning
Regarding the use of tf.train.shuffle_batch() to create batches,"<p>In <a href=""https://www.tensorflow.org/versions/r0.10/api_docs/python/io_ops.html#shuffle_batch"" rel=""noreferrer"">Tensorflow tutorial</a>, it gives the following example regarding <code>tf.train.shuffle_batch()</code>:</p>

<pre><code># Creates batches of 32 images and 32 labels.
image_batch, label_batch = tf.train.shuffle_batch(
     [single_image, single_label],
     batch_size=32,
     num_threads=4,
     capacity=50000,
     min_after_dequeue=10000)
</code></pre>

<p>I am not very clear about the meaning of <code>capacity</code> and <code>min_after_dequeue</code>. In this example, it is set as <code>50000</code> and <code>10000</code> respectively. What is the logic for this kind of setup, or what does that mean. If input has 200 images and 200 labels, what will happen?</p>
",Related to library: Lack of knowledge
Show training and validation accuracy in TensorFlow using same graph,"<p>I have a TensorFlow model, and one part of this model evaluates the accuracy. The <code>accuracy</code> is just another node in the tensorflow graph, that takes in <code>logits</code> and <code>labels</code>.</p>

<p>When I want to plot the training accuracy, this is simple: I have something like:</p>

<pre><code>tf.scalar_summary(""Training Accuracy"", accuracy)
tf.scalar_summary(""SomethingElse"", foo)
summary_op = tf.merge_all_summaries()
writer = tf.train.SummaryWriter('/me/mydir/', graph=sess.graph)
</code></pre>

<p>Then, during my training loop, I have something like:</p>

<pre><code>for n in xrange(1000):
  ...
  summary, ..., ... = sess.run([summary_op, ..., ...], feed_dict)
  writer.add_summary(summary, n)
  ...
</code></pre>

<p>Also inside that for loop, every say, 100 iterations, I want to evaluate the <strong>validation</strong> accuracy. I have a separate feed_dict for this, and I am able to evaluate the validation accuracy very nicely in python. </p>

<p>However, here is my problem: I want to make another <em>summary for the validation accuracy</em>, by using the <code>accuracy</code> node. I am not clear on how to do this though. Since I have the <code>accuracy</code> node it makes sense that I should be able to re-use it, but I am unsure how to do this exactly, such that I can also get the validation accuracy written out as a separate scalar_summary... </p>

<p>How might this be possible?</p>
",Related to library: Lack of knowledge
Unable to feed value for placeholder tensor,"<p>I have written a simple version bidirectional lstm for sentence classification. But it keeps giving me ""You must feed a value for placeholder tensor 'train_x'"" error and it seems this come from the variable initialization step. </p>

<pre><code>data = load_data(FLAGS.data)
model = RNNClassifier(FLAGS)
init = tf.initialize_all_variables()

with tf.Session() as sess:
    coord = tf.train.Coordinator()
    threads = tf.train.start_queue_runners(sess=sess, coord=coord)
    sess.run(init)
    print(""Graph initialized.."")
    print()
    np.random.seed(FLAGS.random_state)
    for epoch in range(FLAGS.max_max_epoch):

        loss = sess.run(model.cost, feed_dict={model.train_x: data.train_x, model.train_y: data.train_y, 
                                        model.embedding_placeholder: data.glove_vec})
        print(""Epoch {:2d}: Loss = {:.6f} = {:.5f}"".format(epoch+1, loss))
    coord.request_stop()
    coord.join(threads)
</code></pre>

<p>And the <code>RNNClassifier</code> class code (in a different directory):</p>

<pre><code>class RNNClassifier:

    def __init__(self, FLAGS):
        self.params = FLAGS
        with tf.device(""/cpu:0""):
            self.train_x = tf.placeholder(tf.int32, [6248, 42], name='train_x')
            self.train_y = tf.placeholder(tf.int32, [6248, 3], name='train_y')
            self.embedding_placeholder = tf.placeholder(tf.float32, [1193515, 100])

        with tf.variable_scope('forward_lstm'):
            lstm_fw_cell = tf.nn.rnn_cell.LSTMCell(num_units=self.params.num_hidden, use_peepholes=False, 
                                                activation=tf.nn.relu, forget_bias=0.0, 
                                                state_is_tuple=True)
        with tf.variable_scope('backward_lstm'):
            lstm_bw_cell = tf.nn.rnn_cell.LSTMCell(num_units=self.params.num_hidden, use_peepholes=False, 
                                                activation=tf.nn.relu, forget_bias=0.0, 
                                                state_is_tuple=True)

        fw_initial_state = lstm_fw_cell.zero_state(self.params.batch_size, tf.float32)
        bw_initial_state = lstm_bw_cell.zero_state(self.params.batch_size, tf.float32)
        self._initial_state = [fw_initial_state, bw_initial_state]

        with tf.device(""/cpu:0""), tf.variable_scope('softmax'):
            self.W = tf.get_variable('W', [self.params.num_hidden*2, self.params.num_classes])
            self.b = tf.get_variable('b', [self.params.num_classes], initializer=tf.constant_initializer(0.0))

        batched_inputs, batched_labels = self.batch_data()
        embed_inputs = self.use_embedding(batched_inputs)


        rnn_outputs, output_state_fw, output_state_bw  = tf.nn.bidirectional_rnn(
            cell_fw=lstm_fw_cell,
            cell_bw=lstm_bw_cell,
            inputs=embed_inputs,
            initial_state_fw=fw_initial_state,
            initial_state_bw=bw_initial_state
            )


        logits = tf.matmul(rnn_outputs[-1], self.W) + self.b

        self._cost = cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf.cast(batched_labels, tf.float32)))
        optimizer = tf.train.AdamOptimizer(learning_rate=0.05).minimize(cost)


    def batch_data(self):
        # inputs = tf.convert_to_tensor(train_x, dtype=tf.int32)
        # labels = tf.convert_to_tensor(train_y, dtype=tf.int32)
        batched_inputs, batched_labels = tf.train.batch(
            tensors=[self._train_x, self._train_y],
            batch_size=self.params.batch_size,
            dynamic_pad=True,
            enqueue_many=True,
            name='batching'
    )
    return batched_inputs, batched_labels


    def use_embedding(self, batched_inputs):
        with tf.device(""/cpu:0""), tf.name_scope(""input_embedding""):
            embedding = tf.get_variable(""embedding"", shape=[1193515, 100], trainable=False)
            embedding_init = embedding.assign(self.embedding_placeholder)
            embed_inputs = tf.split(1, self.params.seq_len, tf.nn.embedding_lookup(embedding_init, batched_inputs))
            embed_inputs = [tf.squeeze(input_, [1]) for input_ in embed_inputs]
    return embed_inputs

    @property
    def cost(self):
        return self._cost
</code></pre>

<p><strong>The output (including the error):</strong></p>

<pre><code>I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
I tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: 
name: GeForce GTX 750 Ti
major: 5 minor: 0 memoryClockRate (GHz) 1.0845
pciBusID 0000:01:00.0
Total memory: 2.00GiB
Free memory: 1.41GiB
I tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 
I tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:839] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: GeForce GTX 750 Ti, pci bus id: 0000:01:00.0)
E tensorflow/core/client/tensor_c_api.cc:485] You must feed a value for placeholder tensor 'train_x' with dtype int32 and shape [6248,42]
     [[Node: train_x = Placeholder[dtype=DT_INT32, shape=[6248,42], _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]

Graph initialized..

W tensorflow/core/framework/op_kernel.cc:936] Out of range: PaddingFIFOQueue '_0_batching/padding_fifo_queue' is closed and has insufficient elements (requested 50, current size 0)
     [[Node: batching = QueueDequeueMany[_class=[""loc:@batching/padding_fifo_queue""], component_types=[DT_INT32, DT_INT32], timeout_ms=-1, _device=""/job:localhost/replica:0/task:0/cpu:0""](batching/padding_fifo_queue, batching/n)]]
W tensorflow/core/framework/op_kernel.cc:936] Out of range: PaddingFIFOQueue '_0_batching/padding_fifo_queue' is closed and has insufficient elements (requested 50, current size 0)
     [[Node: batching = QueueDequeueMany[_class=[""loc:@batching/padding_fifo_queue""], component_types=[DT_INT32, DT_INT32], timeout_ms=-1, _device=""/job:localhost/replica:0/task:0/cpu:0""](batching/padding_fifo_queue, batching/n)]]
E tensorflow/core/client/tensor_c_api.cc:485] PaddingFIFOQueue '_0_batching/padding_fifo_queue' is closed and has insufficient elements (requested 50, current size 0)
     [[Node: batching = QueueDequeueMany[_class=[""loc:@batching/padding_fifo_queue""], component_types=[DT_INT32, DT_INT32], timeout_ms=-1, _device=""/job:localhost/replica:0/task:0/cpu:0""](batching/padding_fifo_queue, batching/n)]]
     [[Node: batching/_9 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=1, tensor_name=""edge_1191_batching"", tensor_type=DT_INT32, _device=""/job:localhost/replica:0/task:0/gpu:0""]()]]
Traceback (most recent call last):
  File ""train_lstm.py"", line 66, in &lt;module&gt;
    model.embedding_placeholder: data.glove_vec})
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 382, in run
    run_metadata_ptr)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 655, in _run
    feed_dict_string, options, run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 723, in _do_run
    target_list, options, run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 743, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors.OutOfRangeError: PaddingFIFOQueue '_0_batching/padding_fifo_queue' is closed and has insufficient elements (requested 50, current size 0)
     [[Node: batching = QueueDequeueMany[_class=[""loc:@batching/padding_fifo_queue""], component_types=[DT_INT32, DT_INT32], timeout_ms=-1, _device=""/job:localhost/replica:0/task:0/cpu:0""](batching/padding_fifo_queue, batching/n)]]
     [[Node: batching/_9 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=1, tensor_name=""edge_1191_batching"", tensor_type=DT_INT32, _device=""/job:localhost/replica:0/task:0/gpu:0""]()]]
Caused by op u'batching', defined at:
  File ""train_lstm.py"", line 49, in &lt;module&gt;
    model = RNNClassifier(FLAGS)
  File ""/home/ccrmad/Code/TDLSTM/models/rnn_classifier.py"", line 34, in __init__
    batched_inputs, batched_labels = self.batch_data()
  File ""/home/ccrmad/Code/TDLSTM/models/rnn_classifier.py"", line 74, in batch_data
    name='batching'
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/input.py"", line 595, in batch
    dequeued = queue.dequeue_many(batch_size, name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/data_flow_ops.py"", line 435, in dequeue_many
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_data_flow_ops.py"", line 867, in _queue_dequeue_many
    timeout_ms=timeout_ms, name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py"", line 703, in apply_op
    op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 2310, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1232, in __init__
    self._traceback = _extract_stack()
</code></pre>

<p>I have tried move the <code>train_x</code> and <code>train_y</code> placeholder initialization before <code>init = tf.initialize_all_variables()</code> and feed them to RNNClassifier() as two args but it still give the same error. Why?</p>
",
Get the value of some weights in a model trained by TensorFlow,"<p>I have trained a ConvNet model with TensorFlow, and I want to get a particular weight in layer. For example in torch7 I would simply access <code>model.modules[2].weights</code>. to get the weights of layer 2. How would I do the same thing in TensorFlow?</p>
",Related to library: Lack of knowledge
What is the TensorFlow checkpoint meta file?,"<p>When saving a checkpoint, TensorFlow often saves a meta file: <code>my_model.ckpt.meta</code>. What is in that file, can we still restore a model even if we delete it and what kind of info did we lose if we restore a model without the meta file?</p>
",Evaluation
How to train TensorFlow network using a generator to produce inputs?,"<p>The TensorFlow <a href=""https://www.tensorflow.org/versions/r0.10/how_tos/reading_data/index.html"" rel=""noreferrer"">docs</a> describe a bunch of ways to read data using TFRecordReader, TextLineReader, QueueRunner etc and queues.</p>

<p>What I would like to do is much, much simpler:  I have a python generator function that produces an infinite sequence of training data as (X, y) tuples (both are numpy arrays, and the first dimension is the batch size).  I just want to train a network using that data as inputs.  </p>

<p>Is there a simple self-contained example of training a TensorFlow network using a generator which produces the data?  (along the lines of the MNIST or CIFAR examples)</p>
",Output problem
Understanding Tensorflow LSTM Input shape,"<p>I have a dataset X which consists <strong>N = 4000 samples</strong>, each sample consists of <strong>d = 2 features</strong> (continuous values) spanning back <strong>t = 10 time steps</strong>. I also have the  corresponding 'labels' of each sample which are also continuous values, at time step 11. </p>

<p>At the moment my dataset is in the shape X: [4000,20], Y: [4000].</p>

<p>I want to train an LSTM using TensorFlow to predict the value of Y (regression), given the 10 previous inputs of d features, but I am having a tough time implementing this in TensorFlow.</p>

<p>The main problem I have at the moment is understanding how TensorFlow is expecting the input to be formatted. I have seen various examples such as <a href=""http://mourafiq.com/2016/05/15/predicting-sequences-using-rnn-in-tensorflow.html"" rel=""nofollow noreferrer"">this</a>, but these examples deal with one big string of continuous time series data. My data is different samples, each an independent time series.</p>
",Training
What is the difference between Keras and tf.contrib.keras in TensorFlow 1.1+?,"<p>Now that TensorFlow 1.1 supports the Keras API under <code>tf.contrib.keras</code>, which one should I use if I intend to use Keras with a TF backend?</p>

<p>Is the <code>tf.contrib.keras</code> version different in any way than a regular Keras distribution? (TF specific optimizations of internal data structures come to mind). Is there any benefit in terms of using Keras and TensorFlow Core together if I use one or the other?</p>

<p>Or is <code>tf.contrib.keras</code> simply a copy of the same codebase as Keras but under a different namespace?</p>
",Data Preparation: Shape mismatch
How can I implement a custom RNN (specifically an ESN) in Tensorflow?,"<p>I am trying to define my own RNNCell (Echo State Network) in Tensorflow, according to below definition.</p>

<p>x(t + 1) = tanh(Win*u(t) + W*x(t) + Wfb*y(t))</p>

<p>y(t) = Wout*z(t)</p>

<p>z(t) = [x(t), u(t)]</p>

<p>x is state, u is input, y is output. Win, W, and Wfb are not trainable. All weights are randomly initialized, but W is modified like this: ""Set a certain percentage of elements of W to 0, scale W to keep its spectral radius below 1.0</p>

<p>I have this code to generate the equation.</p>

<pre><code>x = tf.Variable(tf.reshape(tf.zeros([N]), [-1, N]), trainable=False, name=""state_vector"")
W = tf.Variable(tf.random_normal([N, N], 0.0, 0.05), trainable=False)
# TODO: setup W according to the ESN paper
W_x = tf.matmul(x, W)

u = tf.placeholder(""float"", [None, K], name=""input_vector"")
W_in = tf.Variable(tf.random_normal([K, N], 0.0, 0.05), trainable=False)
W_in_u = tf.matmul(u, W_in)

z = tf.concat(1, [x, u])
W_out = tf.Variable(tf.random_normal([K + N, L], 0.0, 0.05))
y = tf.matmul(z, W_out)
W_fb = tf.Variable(tf.random_normal([L, N], 0.0, 0.05), trainable=False)
W_fb_y = tf.matmul(y, W_fb)

x_next = tf.tanh(W_in_u + W_x + W_fb_y)

y_ = tf.placeholder(""float"", [None, L], name=""train_output"")
</code></pre>

<p>My problem is two-fold. First I don't know how to implement this as a superclass of RNNCell. Second I don't know how to generate a W tensor according to above specification.</p>

<p>Any help about any of these question is greatly appreciated. Maybe I can figure out a way to prepare W, but I sure as hell don't understand how to implement my own RNN as a superclass of RNNCell.</p>
",Related to library: Installation
Replace Validation Monitors with tf.train.SessionRunHook when using Estimators,"<p>I am running a DNNClassifier, for which I am monitoring accuracy while training. monitors.ValidationMonitor from contrib/learn has been working great, in my implementation I define it:</p>

<pre><code>validation_monitor = skflow.monitors.ValidationMonitor(input_fn=lambda: input_fn(A_test, Cl2_test), eval_steps=1, every_n_steps=50)
</code></pre>

<p>and then use call from:</p>

<pre><code>clf.fit(input_fn=lambda: input_fn(A, Cl2),
            steps=1000, monitors=[validation_monitor])
</code></pre>

<p>where: </p>

<pre><code>clf = tensorflow.contrib.learn.DNNClassifier(...
</code></pre>

<p>This works fine. That said, validation monitors appear to be deprecated and a similar functionality to be replaced with <code>tf.train.SessionRunHook</code>. </p>

<p>I am a newbie in TensorFlow, and it does not seem trivial to me how such a replacing implementation would look like. Any suggestion are highly appreciated. Again, I need to validate the training after a specific number of steps.
Thanks very much in advance.</p>
",Choosing a model: Lack of knowledge
tf.shape() get wrong shape in tensorflow,"<p>I define a tensor like this:</p>

<p><code>x = tf.get_variable(""x"", [100])</code></p>

<p>But when I try to print shape of tensor :</p>

<p><code>print( tf.shape(x) )</code></p>

<p>I get <strong>Tensor(""Shape:0"", shape=(1,), dtype=int32)</strong>, why the result of output should not be shape=(100)</p>
",Evaluation
Rename variable scope of saved model in TensorFlow,"<p>Is it possible to rename the variable scope of a given model in tensorflow?</p>

<p>For instance, I created a logistic regression model for MNIST digits, based on the tutorial:</p>

<pre><code>with tf.variable_scope('my-first-scope'):
    NUM_IMAGE_PIXELS = 784
    NUM_CLASS_BINS = 10
    x = tf.placeholder(tf.float32, shape=[None, NUM_IMAGE_PIXELS])
    y_ = tf.placeholder(tf.float32, shape=[None, NUM_CLASS_BINS])

    W = tf.Variable(tf.zeros([NUM_IMAGE_PIXELS,NUM_CLASS_BINS]))
    b = tf.Variable(tf.zeros([NUM_CLASS_BINS]))

    y = tf.nn.softmax(tf.matmul(x,W) + b)
    cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))
    saver = tf.train.Saver([W, b])

... # some training happens

saver.save(sess, 'my-model')
</code></pre>

<p>Now I want to reload the saved model in the <code>'my-first-scope'</code> variable scope and then save everything again to a new file and under a new variable scope of <code>'my-second-scope'</code>.</p>
",Data Preparation: Shape mismatch
TensorFlow: argmax (-min),"<p>I just noticed an unexpected (at least for me) behavior in TensorFlow. I thought <code>tf.argmax</code> (-<code>argmin</code>) operates on the ranks of a Tensor from outer to inner, but apparently it does not?!</p>

<p>Example:</p>

<pre><code>import numpy as np
import tensorflow as tf

sess = tf.InteractiveSession()

arr = np.array([[31, 23,  4, 24, 27, 34],
                [18,  3, 25,  0,  6, 35],
                [28, 14, 33, 22, 20,  8],
                [13, 30, 21, 19,  7,  9],
                [16,  1, 26, 32,  2, 29],
                [17, 12,  5, 11, 10, 15]])

# arr has rank 2 and shape (6, 6)
tf.rank(arr).eval()
&gt; 2
tf.shape(arr).eval()
&gt; array([6, 6], dtype=int32)
</code></pre>

<p><code>tf.argmax</code> takes two arguments: <code>input</code> and <code>dimension</code>. Since the indices of array <code>arr</code> are <code>arr[rows, columns]</code>, I would expect <code>tf.argmax(arr, 0)</code> to return the index of the maximum element per row, while I would have expected <code>tf.argmax(arr, 1)</code> to return the maximum element per column. Likewise for <code>tf.argmin</code>.</p>

<p>However, the opposite is true:</p>

<pre><code>tf.argmax(arr, 0).eval()
&gt; array([0, 3, 2, 4, 0, 1])

# 0 -&gt; 31 (arr[0, 0])
# 3 -&gt; 30 (arr[3, 1])
# 2 -&gt; 33 (arr[2, 2])
# ...
# thus, this is clearly searching for the maximum element
# for every column, and *not* for every row

tf.argmax(arr, 1).eval()
&gt; array([5, 5, 2, 1, 3, 0])

# 5 -&gt; 34 (arr[0, 5])
# 5 -&gt; 35 (arr[1, 5])
# 2 -&gt; 33 (arr[2, 2])
# ...
# this clearly returns the maximum element per row,
# albeit 'dimension' was set to 1
</code></pre>

<p>Can someone explain this behavior?</p>

<p>Generalized every n-dimensional Tensor <code>t</code> is indexed by <code>t[i, j, k, ...]</code>. Thus, <code>t</code> has rank n and shape <code>(i, j, k, ...)</code>. Since dimension 0 corresponds to <code>i</code>, dimension 1 to <code>j</code>, and so forth. Why does <code>tf.argmax</code> (&amp; -<code>argmin</code>) ignore this scheme?</p>
",Choosing a model: Lack of knowledge
Edit tensorflow inceptionV3 retraining-example.py for multiple classificiations,"<p>TLDR: Cannot figure out how to use retrained inceptionV3 for multiple image predictions. </p>

<p>Hello kind people :) I've spent a few days searching many stackoverflow posts and the documentation, but I could not find an answer to this question. Would greatly appreciate any help on this!</p>

<p>I have retrained a tensorflow inceptionV3 model on new pictures, and it is able to work on new images by following the instructions at <a href=""https://www.tensorflow.org/versions/r0.9/how_tos/image_retraining/index.html"" rel=""nofollow"">https://www.tensorflow.org/versions/r0.9/how_tos/image_retraining/index.html</a> and using the following commands:</p>

<pre><code>bazel build tensorflow/examples/label_image:label_image &amp;&amp; \
bazel-bin/tensorflow/examples/label_image/label_image \
--graph=/tmp/output_graph.pb --labels=/tmp/output_labels.txt \
--output_layer=final_result \
--image= IMAGE_DIRECTORY_TO_CLASSIFY
</code></pre>

<p>However, I need to classify multiple images (like a dataset), and am seriously stuck on how to do so. I've found the following example at</p>

<p><a href=""https://github.com/eldor4do/Tensorflow-Examples/blob/master/retraining-example.py"" rel=""nofollow"">https://github.com/eldor4do/Tensorflow-Examples/blob/master/retraining-example.py</a></p>

<p>on how to use the retrained model, but again, it is greatly sparse on details on how to modify it for multiple classifications. </p>

<p>From what I've gathered from the MNIST tutorial, I need to input feed_dict in the sess.run() object, but was stuck there as I couldn't understand how to implement it in this context.</p>

<p>Any assistance will be extremely appreciated! :) </p>

<p>EDIT:</p>

<p>Running Styrke's script with some modifications, i got this</p>

<pre><code>    waffle@waffleServer:~/git$ python tensorflowMassPred.py  I
       tensorflow/stream_executor/dso_loader.cc:108] successfully opened
       CUDA library libcublas.so locally I
       tensorflow/stream_executor/dso_loader.cc:108] successfully opened
       CUDA library libcudnn.so locally I
       tensorflow/stream_executor/dso_loader.cc:108] successfully opened
       CUDA library libcufft.so locally I
       tensorflow/stream_executor/dso_loader.cc:108] successfully opened
       CUDA library libcuda.so locally I
       tensorflow/stream_executor/dso_loader.cc:108] successfully opened
       CUDA library libcurand.so locally
       /home/waffle/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/array_ops.py:1197:
       VisibleDeprecationWarning: converting an array with ndim &gt; 0 to an
       index will result in an error in the future  
       result_shape.insert(dim, 1) I
       tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:924] successful
       NUMA node read from SysFS had negative value (-1), but there must be
       at least one NUMA node, so returning NUMA node zero I
       tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0
       with properties:  name: GeForce GTX 660 major: 3 minor: 0
       memoryClockRate (GHz) 1.0975 pciBusID 0000:01:00.0 Total memory:
       2.00GiB Free memory: 1.78GiB I tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0  I
       tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y  I
       tensorflow/core/common_runtime/gpu/gpu_device.cc:806] Creating
       TensorFlow device (/gpu:0) -&gt; (device: 0, name: GeForce GTX 660, pci
       bus id: 0000:01:00.0) W tensorflow/core/framework/op_def_util.cc:332]
       Op BatchNormWithGlobalNormalization is deprecated. It will cease to
       work in GraphDef version 9. Use tf.nn.batch_normalization(). E
       tensorflow/core/common_runtime/executor.cc:334] Executor failed to
       create kernel. Invalid argument: NodeDef mentions attr 'T' not in
       Op&lt;name=MaxPool; signature=input:float -&gt; output:float;
       attr=ksize:list(int),min=4; attr=strides:list(int),min=4;
       attr=padding:string,allowed=[""SAME"", ""VALID""];
       attr=data_format:string,default=""NHWC"",allowed=[""NHWC"", ""NCHW""]&gt;;
       NodeDef: pool = MaxPool[T=DT_FLOAT, data_format=""NHWC"", ksize=[1, 3,
       3, 1], padding=""VALID"", strides=[1, 2, 2, 1],
       _device=""/job:localhost/replica:0/task:0/gpu:0""](pool/control_dependency)
         [[Node: pool = MaxPool[T=DT_FLOAT, data_format=""NHWC"", ksize=[1, 3,
       3, 1], padding=""VALID"", strides=[1, 2, 2, 1],
       _device=""/job:localhost/replica:0/task:0/gpu:0""](pool/control_dependency)]]
       Traceback (most recent call last):   File
       ""/home/waffle/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py"",
       line 715, in _do_call
           return fn(*args)   File ""/home/waffle/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py"",
       line 697, in _run_fn
           status, run_metadata)   File ""/home/waffle/anaconda3/lib/python3.5/contextlib.py"", line 66, in
       __exit__
           next(self.gen)   File ""/home/waffle/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/errors.py"",
       line 450, in raise_exception_on_not_ok_status
           pywrap_tensorflow.TF_GetCode(status)) tensorflow.python.framework.errors.InvalidArgumentError: NodeDef
       mentions attr 'T' not in Op&lt;name=MaxPool; signature=input:float -&gt;
       output:float; attr=ksize:list(int),min=4;
       attr=strides:list(int),min=4; attr=padding:string,allowed=[""SAME"",
       ""VALID""]; attr=data_format:string,default=""NHWC"",allowed=[""NHWC"",
       ""NCHW""]&gt;; NodeDef: pool = MaxPool[T=DT_FLOAT, data_format=""NHWC"",
       ksize=[1, 3, 3, 1], padding=""VALID"", strides=[1, 2, 2, 1],
       _device=""/job:localhost/replica:0/task:0/gpu:0""](pool/control_dependency)
         [[Node: pool = MaxPool[T=DT_FLOAT, data_format=""NHWC"", ksize=[1, 3,
       3, 1], padding=""VALID"", strides=[1, 2, 2, 1],
       _device=""/job:localhost/replica:0/task:0/gpu:0""](pool/control_dependency)]]

       During handling of the above exception, another exception occurred:

       Traceback (most recent call last):   File ""tensorflowMassPred.py"",
       line 116, in &lt;module&gt;
           run_inference_on_image()   File ""tensorflowMassPred.py"", line 98, in run_inference_on_image
           {'DecodeJpeg/contents:0': image_data})   File ""/home/waffle/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py"",
       line 372, in run
           run_metadata_ptr)   File ""/home/waffle/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py"",
       line 636, in _run
           feed_dict_string, options, run_metadata)   File ""/home/waffle/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py"",
       line 708, in _do_run
           target_list, options, run_metadata)   File ""/home/waffle/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py"",
       line 728, in _do_call
           raise type(e)(node_def, op, message) tensorflow.python.framework.errors.InvalidArgumentError: NodeDef
       mentions attr 'T' not in Op&lt;name=MaxPool; signature=input:float -&gt;
       output:float; attr=ksize:list(int),min=4;
       attr=strides:list(int),min=4; attr=padding:string,allowed=[""SAME"",
       ""VALID""]; attr=data_format:string,default=""NHWC"",allowed=[""NHWC"",
       ""NCHW""]&gt;; NodeDef: pool = MaxPool[T=DT_FLOAT, data_format=""NHWC"",
       ksize=[1, 3, 3, 1], padding=""VALID"", strides=[1, 2, 2, 1],
       _device=""/job:localhost/replica:0/task:0/gpu:0""](pool/control_dependency)
         [[Node: pool = MaxPool[T=DT_FLOAT, data_format=""NHWC"", ksize=[1, 3,
       3, 1], padding=""VALID"", strides=[1, 2, 2, 1],
       _device=""/job:localhost/replica:0/task:0/gpu:0""](pool/control_dependency)]]
       Caused by op 'pool', defined at:   File ""tensorflowMassPred.py"", line
       116, in &lt;module&gt;
           run_inference_on_image()   File ""tensorflowMassPred.py"", line 87, in run_inference_on_image
           create_graph()   File ""tensorflowMassPred.py"", line 68, in create_graph
           _ = tf.import_graph_def(graph_def, name='')   File ""/home/waffle/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/importer.py"",
       line 274, in import_graph_def
           op_def=op_def)   File ""/home/waffle/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"",
       line 2260, in create_op
           original_op=self._default_original_op, op_def=op_def)   File ""/home/waffle/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"",
       line 1230, in __init__
           self._traceback = _extract_stack()
</code></pre>

<p>This is the script: some functions are removed.</p>

<pre><code>import os
import numpy as np
import tensorflow as tf
os.chdir('tensorflow/') #if need to run in the tensorflow directory
import csv,os
import pandas as pd
import glob

imagePath = '../_images_processed/test'
modelFullPath = '/tmp/output_graph.pb'
labelsFullPath = '/tmp/output_labels.txt'

# FILE NAME TO SAVE TO.
SAVE_TO_CSV = 'tensorflowPred.csv'


def makeCSV():
    global SAVE_TO_CSV
    with open(SAVE_TO_CSV,'w') as f:
        writer = csv.writer(f)
        writer.writerow(['id','label'])


def makeUniqueDic():
    global SAVE_TO_CSV
    df = pd.read_csv(SAVE_TO_CSV)
    doneID = df['id']
    unique = doneID.unique()
    uniqueDic = {str(key):'' for key in unique} #for faster lookup
    return uniqueDic


def create_graph():
    """"""Creates a graph from saved GraphDef file and returns a saver.""""""
    # Creates graph from saved graph_def.pb.
    with tf.gfile.FastGFile(modelFullPath, 'rb') as f:
        graph_def = tf.GraphDef()
        graph_def.ParseFromString(f.read())
        _ = tf.import_graph_def(graph_def, name='')


def run_inference_on_image():
    answer = []
    global imagePath
    if not tf.gfile.IsDirectory(imagePath):
        tf.logging.fatal('imagePath directory does not exist %s', imagePath)
        return answer

    if not os.path.exists(SAVE_TO_CSV):
        makeCSV()

    files = glob.glob(imagePath+'/*.jpg')
    uniqueDic = makeUniqueDic()        
    # Get a list of all files in imagePath directory
    #image_list = tf.gfile.ListDirectory(imagePath)

    # Creates graph from saved GraphDef.
    create_graph()

    with tf.Session() as sess:

        softmax_tensor = sess.graph.get_tensor_by_name('final_result:0')

        for pic in files:
            name = getNamePicture(pic)
            if name not in uniqueDic:
                image_data = tf.gfile.FastGFile(pic, 'rb').read()
                predictions = sess.run(softmax_tensor,
                                   {'DecodeJpeg/contents:0': image_data})
                predictions = np.squeeze(predictions)

                top_k = predictions.argsort()[-5:][::-1]  # Getting top 5 predictions
                f = open(labelsFullPath, 'rb')
                lines = f.readlines()
                labels = [str(w).replace(""\n"", """") for w in lines]
#            for node_id in top_k:
#                human_string = labels[node_id]
#                score = predictions[node_id]
#                print('%s (score = %.5f)' % (human_string, score))
                pred = labels[top_k[0]]
                with open(SAVE_TO_CSV,'a') as f:
                    writer = csv.writer(f)
                    writer.writerow([name,pred])
    return answer

if __name__ == '__main__':
    run_inference_on_image()
</code></pre>
",Choosing a model: Lack of knowledge
How to *actually* read CSV data in TensorFlow?,"<p>I'm relatively new to the world of TensorFlow, and pretty perplexed by how you'd <strong><em>actually</em></strong> read CSV data into a usable example/label tensors in TensorFlow. The example from the <a href=""https://www.tensorflow.org/versions/r0.7/how_tos/reading_data/index.html#reading-from-files"" rel=""nofollow noreferrer"">TensorFlow tutorial on reading CSV data</a> is pretty fragmented and only gets you part of the way to being able to train on CSV data.</p>

<p>Here's my code that I've pieced together, based off that CSV tutorial:</p>

<pre><code>from __future__ import print_function
import tensorflow as tf

def file_len(fname):
    with open(fname) as f:
        for i, l in enumerate(f):
            pass
    return i + 1

filename = ""csv_test_data.csv""

# setup text reader
file_length = file_len(filename)
filename_queue = tf.train.string_input_producer([filename])
reader = tf.TextLineReader(skip_header_lines=1)
_, csv_row = reader.read(filename_queue)

# setup CSV decoding
record_defaults = [[0],[0],[0],[0],[0]]
col1,col2,col3,col4,col5 = tf.decode_csv(csv_row, record_defaults=record_defaults)

# turn features back into a tensor
features = tf.stack([col1,col2,col3,col4])

print(""loading, "" + str(file_length) + "" line(s)\n"")
with tf.Session() as sess:
  tf.initialize_all_variables().run()

  # start populating filename queue
  coord = tf.train.Coordinator()
  threads = tf.train.start_queue_runners(coord=coord)

  for i in range(file_length):
    # retrieve a single instance
    example, label = sess.run([features, col5])
    print(example, label)

  coord.request_stop()
  coord.join(threads)
  print(""\ndone loading"")
</code></pre>

<p>And here is an brief example from the CSV file I'm loading - pretty basic data - 4 feature columns, and 1 label column:</p>

<pre><code>0,0,0,0,0
0,15,0,0,0
0,30,0,0,0
0,45,0,0,0
</code></pre>

<p>All the code above does is <strong>print each example from the CSV file, one by one</strong>, which, while nice, is pretty darn useless for training.</p>

<p>What I'm struggling with here is how you'd actually turn those individual examples, loaded one-by-one, into a training dataset. For example, <a href=""https://github.com/rringham/deep-learning-notebooks/blob/master/notMNIST_four_hidden_layers.ipynb"" rel=""nofollow noreferrer"">here's a notebook</a> I was working on in the Udacity Deep Learning course. I basically want to take the CSV data I'm loading, and plop it into something like <strong>train_dataset</strong> and <strong>train_labels</strong>:</p>

<pre><code>def reformat(dataset, labels):
  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)
  # Map 2 to [0.0, 1.0, 0.0 ...], 3 to [0.0, 0.0, 1.0 ...]
  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)
  return dataset, labels
train_dataset, train_labels = reformat(train_dataset, train_labels)
valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)
test_dataset, test_labels = reformat(test_dataset, test_labels)
print('Training set', train_dataset.shape, train_labels.shape)
print('Validation set', valid_dataset.shape, valid_labels.shape)
print('Test set', test_dataset.shape, test_labels.shape)
</code></pre>

<p>I've tried using <code>tf.train.shuffle_batch</code>, like this, but it just inexplicably hangs:</p>

<pre><code>  for i in range(file_length):
    # retrieve a single instance
    example, label = sess.run([features, colRelevant])
    example_batch, label_batch = tf.train.shuffle_batch([example, label], batch_size=file_length, capacity=file_length, min_after_dequeue=10000)
    print(example, label)
</code></pre>

<p>So to sum up, here are my questions:</p>

<ul>
<li><strong>What am I missing about this process?</strong>

<ul>
<li>It feels like there is some key intuition that I'm missing about how to properly build an input pipeline.</li>
</ul></li>
<li><strong>Is there a way to avoid having to know the length of the CSV file?</strong>

<ul>
<li>It feels pretty inelegant to have to know the number of lines you want to process (the <code>for i in range(file_length)</code> line of code above)</li>
</ul></li>
</ul>

<hr>

<p><strong>Edit:</strong>
As soon as Yaroslav pointed out that I was likely mixing up imperative and graph-construction parts here, it started to become clearer. I was able to pull together the following code, which I think is closer to what would typically done when training a model from CSV (excluding any model training code):</p>

<pre><code>from __future__ import print_function
import numpy as np
import tensorflow as tf
import math as math
import argparse

parser = argparse.ArgumentParser()
parser.add_argument('dataset')
args = parser.parse_args()

def file_len(fname):
    with open(fname) as f:
        for i, l in enumerate(f):
            pass
    return i + 1

def read_from_csv(filename_queue):
  reader = tf.TextLineReader(skip_header_lines=1)
  _, csv_row = reader.read(filename_queue)
  record_defaults = [[0],[0],[0],[0],[0]]
  colHour,colQuarter,colAction,colUser,colLabel = tf.decode_csv(csv_row, record_defaults=record_defaults)
  features = tf.stack([colHour,colQuarter,colAction,colUser])  
  label = tf.stack([colLabel])  
  return features, label

def input_pipeline(batch_size, num_epochs=None):
  filename_queue = tf.train.string_input_producer([args.dataset], num_epochs=num_epochs, shuffle=True)  
  example, label = read_from_csv(filename_queue)
  min_after_dequeue = 10000
  capacity = min_after_dequeue + 3 * batch_size
  example_batch, label_batch = tf.train.shuffle_batch(
      [example, label], batch_size=batch_size, capacity=capacity,
      min_after_dequeue=min_after_dequeue)
  return example_batch, label_batch

file_length = file_len(args.dataset) - 1
examples, labels = input_pipeline(file_length, 1)

with tf.Session() as sess:
  tf.initialize_all_variables().run()

  # start populating filename queue
  coord = tf.train.Coordinator()
  threads = tf.train.start_queue_runners(coord=coord)

  try:
    while not coord.should_stop():
      example_batch, label_batch = sess.run([examples, labels])
      print(example_batch)
  except tf.errors.OutOfRangeError:
    print('Done training, epoch reached')
  finally:
    coord.request_stop()

  coord.join(threads) 
</code></pre>
",Training: Performance/Evaluation: Accuracy
How to understand static shape and dynamic shape in TensorFlow?,"<p>In <a href=""https://www.tensorflow.org/versions/r0.8/resources/faq.html#tensor-shapes"">TensorFlow FAQ</a>, it says:</p>

<blockquote>
  <p>In TensorFlow, a tensor has both a static (inferred) shape and a
  dynamic (true) shape. The static shape can be read using the
  tf.Tensor.get_shape() method: this shape is inferred from the
  operations that were used to create the tensor, and may be partially
  complete. If the static shape is not fully defined, the dynamic shape
  of a Tensor t can be determined by evaluating tf.shape(t).</p>
</blockquote>

<p>But I still cannot fully understand the relationship between static shape and dynamic shape. Are there any examples showing their differences? Thanks.</p>
",Data Preparation
Difference between Variable and get_variable in TensorFlow,"<p>As far as I know, <code>Variable</code> is the default operation for making a variable, and <code>get_variable</code> is mainly used for weight sharing.</p>

<p>On the one hand, there are some people suggesting using <code>get_variable</code> instead of the primitive <code>Variable</code> operation whenever you need a variable. On the other hand, I merely see any use of <code>get_variable</code> in TensorFlow's official documents and demos.</p>

<p>Thus I want to know some rules of thumbs on how to correctly use these two mechanisms. Are there any ""standard"" principles?</p>
",
Tensorflow implementation of word2vec,"<p>The Tensorflow tutorial <a href=""https://www.tensorflow.org/versions/r0.9/tutorials/word2vec/index.html#vector-representations-of-words"" rel=""noreferrer"">here</a> refers to their basic implementation which you can find on github <a href=""https://github.com/tensorflow/tensorflow/blob/r0.9/tensorflow/examples/tutorials/word2vec/word2vec_basic.py"" rel=""noreferrer"">here</a>, where the Tensorflow authors implement word2vec vector embedding training/evaluation with the Skipgram model. </p>

<p>My question is about the actual generation of (target, context) pairs in the <code>generate_batch()</code> function. </p>

<p>On <a href=""https://github.com/tensorflow/tensorflow/blob/r0.9/tensorflow/examples/tutorials/word2vec/word2vec_basic.py#L107"" rel=""noreferrer"">this line</a> Tensorflow authors randomly sample nearby target indices from the ""center"" word index in the sliding window of words. </p>

<p>However, they <a href=""https://github.com/tensorflow/tensorflow/blob/r0.9/tensorflow/examples/tutorials/word2vec/word2vec_basic.py#L104"" rel=""noreferrer"">also keep a data structure <code>targets_to_avoid</code></a> to which they add first the ""center"" context word (which of course we don't want to sample) but ALSO other words after we add them. </p>

<p>My questions are as follows:</p>

<ol>
<li>Why sample from this sliding window around the word, why not just have a loop and use them all rather than sampling? It seems strange they would worry about performance/memory in <code>word2vec_basic.py</code> (their ""basic"" implementation). </li>
<li>Whatever the answer to 1) is, why are they <em>both</em> sampling and keeping track of what they've selected with <code>targets_to_avoid</code>? If they wanted truly random, they'd use selection with replacement, and if they wanted to ensure they got all the options, they should have just used a loop and gotten them all in the first place!</li>
<li>Does the built in <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/embedding/word2vec.py#L349"" rel=""noreferrer"">tf.models.embedding.gen_word2vec</a> work this way too? If so where can I find the source code? (couldn't find the .py file in the Github repo)</li>
</ol>

<p>Thanks!</p>
",Related to library: Lack of knowledge
How to add regularizations in TensorFlow?,"<p>I found in many available neural network code implemented using TensorFlow that regularization terms are often implemented by manually adding an additional term to loss value.</p>

<p>My questions are:</p>

<ol>
<li><p>Is there a more elegant or recommended way of regularization than doing it manually?</p></li>
<li><p>I also find that <code>get_variable</code> has an argument <code>regularizer</code>. How should it be used? According to my observation, if we pass a regularizer to it (such as <code>tf.contrib.layers.l2_regularizer</code>, a tensor representing regularized term will be computed and added to a graph collection named <code>tf.GraphKeys.REGULARIZATOIN_LOSSES</code>. Will that collection be automatically used by TensorFlow (e.g. used by optimizers when training)? Or is it expected that I should use that collection by myself?</p></li>
</ol>
",Related to library: Lack of knowledge
How to read data into TensorFlow batches from example queue?,"<p>How do I get TensorFlow example queues into proper batches for training?</p>

<p>I've got some images and labels:</p>

<pre><code>IMG_6642.JPG 1
IMG_6643.JPG 2
</code></pre>

<p>(feel free to suggest another label format; I think I may need another dense to sparse step...)</p>

<p>I've read through quite a few tutorials but don't quite have it all together yet.
Here's what I have, with comments indicating the steps required from TensorFlow's <a href=""https://www.tensorflow.org/versions/r0.8/how_tos/reading_data/index.html#reading-from-files"" rel=""noreferrer"">Reading Data</a> page.</p>

<ol>
<li>The list of filenames
(optional steps removed for the sake of simplicity)</li>
<li>Filename queue</li>
<li>A Reader for the file format</li>
<li>A decoder for a record read by the reader</li>
<li>Example queue</li>
</ol>

<p>And after the example queue I need to get this queue into batches for training; that's where I'm stuck...</p>

<p><strong>1. List of filenames</strong></p>

<p><code>files = tf.train.match_filenames_once('*.JPG')</code></p>

<p><strong>4. Filename queue</strong></p>

<p><code>filename_queue = tf.train.string_input_producer(files, num_epochs=None, shuffle=True, seed=None, shared_name=None, name=None)</code></p>

<p><strong>5. A reader</strong></p>

<p><code>reader = tf.TextLineReader()
key, value = reader.read(filename_queue)</code></p>

<p><strong>6. A decoder</strong></p>

<p><code>record_defaults = [[""""], [1]]
col1, col2 = tf.decode_csv(value, record_defaults=record_defaults)
</code>
(I don't think I need this step below because I already have my label in a tensor but I include it anyways)</p>

<p><code>features = tf.pack([col2])</code></p>

<p>The documentation page has an example to run one image, not get the images and labels into batches:</p>

<p><code>
for i in range(1200):
    # Retrieve a single instance:
    example, label = sess.run([features, col5])
</code></p>

<p>And then below it has a batching section:</p>

<pre><code>def read_my_file_format(filename_queue):
  reader = tf.SomeReader()
  key, record_string = reader.read(filename_queue)
  example, label = tf.some_decoder(record_string)
  processed_example = some_processing(example)
  return processed_example, label

def input_pipeline(filenames, batch_size, num_epochs=None):
  filename_queue = tf.train.string_input_producer(
  filenames, num_epochs=num_epochs, shuffle=True)
  example, label = read_my_file_format(filename_queue)
  # min_after_dequeue defines how big a buffer we will randomly sample
  #   from -- bigger means better shuffling but slower start up and more
  #   memory used.
  # capacity must be larger than min_after_dequeue and the amount larger
  #   determines the maximum we will prefetch.  Recommendation:
  #   min_after_dequeue + (num_threads + a small safety margin) *              batch_size
  min_after_dequeue = 10000
  capacity = min_after_dequeue + 3 * batch_size
  example_batch, label_batch = tf.train.shuffle_batch(
  [example, label], batch_size=batch_size, capacity=capacity,
  min_after_dequeue=min_after_dequeue)
  return example_batch, label_batch
</code></pre>

<p>My question is: <strong>how do I use the above example code with the code I have above?</strong>  I need <em>batches</em> to work with, and most of the tutorials come with mnist batches already.</p>

<pre><code>with tf.Session() as sess:
  sess.run(init)

  # Training cycle
for epoch in range(training_epochs):
    total_batch = int(mnist.train.num_examples/batch_size)
    # Loop over all batches
    for i in range(total_batch):
        batch_xs, batch_ys = mnist.train.next_batch(batch_size)
</code></pre>
",Hyper parameter tuning
Installing tensorflow with anaconda in windows,"<p>I have installed Anaconda on Windows 64 bit. I have downloaded PyCharm for creating a project and in the terminal of PyCharm I have installed <code>numpy</code>, <code>scipy</code>, <code>matplotlib</code> using the following commands:</p>

<pre><code>conda install numpy
conda install scipy
conda install matplotlib
</code></pre>

<p>I am not able to install Tensorflow in the same way I installed these other packages. How should I install it?</p>
",Data Preparation
Load saved checkpoint and predict not producing same results as in training,"<p>I'm training based on a sample code I found on the Internet. The accuracy in testing is at 92% and the checkpoints are saved in a directory. In parallel (the training is running for 3 days now) I want to create my prediction code so I can learn more instead of just waiting.</p>

<p>This is my third day of deep learning so I probably don't know what I'm doing. Here's how I'm trying to predict:</p>

<ul>
<li>Instantiate the model using the same code as in training</li>
<li>Load the last checkpoint</li>
<li>Try to predict</li>
</ul>

<p>The code works but the results are nowhere near 90%.</p>

<p>Here's how I create the model:</p>

<pre><code>INPUT_LAYERS = 2
OUTPUT_LAYERS = 2
AMOUNT_OF_DROPOUT = 0.3
HIDDEN_SIZE = 700
INITIALIZATION = ""he_normal""  # : Gaussian initialization scaled by fan_in (He et al., 2014)
CHARS = list(""abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ ."")

def generate_model(output_len, chars=None):
    """"""Generate the model""""""
    print('Build model...')
    chars = chars or CHARS
    model = Sequential()
    # ""Encode"" the input sequence using an RNN, producing an output of HIDDEN_SIZE
    # note: in a situation where your input sequences have a variable length,
    # use input_shape=(None, nb_feature).
    for layer_number in range(INPUT_LAYERS):
        model.add(recurrent.LSTM(HIDDEN_SIZE, input_shape=(None, len(chars)), init=INITIALIZATION,
                         return_sequences=layer_number + 1 &lt; INPUT_LAYERS))
        model.add(Dropout(AMOUNT_OF_DROPOUT))
    # For the decoder's input, we repeat the encoded input for each time step
    model.add(RepeatVector(output_len))
    # The decoder RNN could be multiple layers stacked or a single layer
    for _ in range(OUTPUT_LAYERS):
        model.add(recurrent.LSTM(HIDDEN_SIZE, return_sequences=True, init=INITIALIZATION))
        model.add(Dropout(AMOUNT_OF_DROPOUT))

    # For each of step of the output sequence, decide which character should be chosen
    model.add(TimeDistributed(Dense(len(chars), init=INITIALIZATION)))
    model.add(Activation('softmax'))

    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
    return model
</code></pre>

<p>In a separate file <code>predict.py</code> I import this method to create my model and try to predict:</p>

<pre><code>...import code
model = generate_model(len(question), dataset['chars'])
model.load_weights('models/weights.204-0.20.hdf5')

def decode(pred):
    return character_table.decode(pred, calc_argmax=False)


x = np.zeros((1, len(question), len(dataset['chars'])))
for t, char in enumerate(question):
    x[0, t, character_table.char_indices[char]] = 1.

preds = model.predict_classes([x], verbose=0)[0]

print(""======================================"")
print(decode(preds))
</code></pre>

<p>I don't know what the problem is. I have about 90 checkpoints in my directory and I'm loading the last one based on accuracy. All of them saved by a <code>ModelCheckpoint</code>:</p>

<pre><code>checkpoint = ModelCheckpoint(MODEL_CHECKPOINT_DIRECTORYNAME + '/' + MODEL_CHECKPOINT_FILENAME,
                         save_best_only=True)
</code></pre>

<p>I'm stuck. What am I doing wrong?</p>
",Related to library: Lack of knowledge
What is a good explanation of how to read the histogram feature of TensorBoard?,"<p>Question is simple, <strong>how do you read those graphs</strong>? I read their explanation and it doesn't make sense to me.</p>

<p>I was reading TensorFlow's <a href=""https://github.com/tensorflow/tensorflow/blob/r0.9/tensorflow/tensorboard/README.md"">newly updated readme file for TensorBoard</a> and in it it tries to explain what a ""histogram"" is. First it clarifies that its not really a histogram:</p>

<blockquote>
  <p>Right now, its name is a bit of a misnomer, as it doesn't show
  histograms; instead, it shows some high-level statistics on a
  distribution.</p>
</blockquote>

<p>I am trying to figure out what their description is actually trying to say.</p>

<p>Right now I am trying to parse the specific sentence:</p>

<blockquote>
  <p>Each line on the chart represents a percentile in the distribution
  over the data: for example, the bottom line shows how the minimum
  value has changed over time, and the line in the middle shows how the
  median has changed.</p>
</blockquote>

<p>The first question I have is, what do they mean by ""each line"". There are horizontal axis and there are lines that make a square grid on the graph or maybe the plotted lines, themselves. Consider a screen shot from <a href=""https://www.tensorflow.org/tensorboard/index.html#events"">the TensorBoard example</a>:</p>

<p><a href=""https://i.stack.imgur.com/u7zsf.png""><img src=""https://i.stack.imgur.com/u7zsf.png"" alt=""enter image description here""></a></p>

<p>What are they referring to with ""lines""? In the above example what are the lines and percentiles that they are talking about?</p>

<p>Then the readme file tries to provide more detail with an example:</p>

<blockquote>
  <p>Reading from top to bottom, the lines have the following meaning:
  [maximum, 93%, 84%, 69%, 50%, 31%, 16%, 7%, minimum]</p>
</blockquote>

<p>However, its unclear to me what they are talking about. What is lines and what percentiles? </p>

<p>It seems that they are trying to replace this in the future, but meanwhile, I am stuck with this. Can someone help me understand how to use this?</p>
",Training: Performance
TensorBoard - Plot training and validation losses on the same graph?,"<p>Is there a way to plot both the training losses and validation losses on the <em>same</em> graph?</p>

<p>It's easy to have two separate scalar summaries for each of them individually, but this puts them on separate graphs. If both are displayed in the same graph it's much easier to see the gap between them and whether or not they have begin to diverge due to overfitting.</p>

<p>Is there a built in way to do this? If not, a work around way? Thank you much!</p>
",Related to library: Lack of knowledge
How to count total number of trainable parameters in a tensorflow model?,"<p>Is there a function call or another way to count the total number of parameters in a tensorflow graph?</p>

<p>By parameters I mean: an N dim vector of trainable variables has N parameters, a <code>NxM</code> matrix has <code>N*M</code> parameters, etc. So essentially I'd like to sum the product of the shape dimensions of all the trainable variables in a tensorflow session.</p>
",Related to library: Lack of knowledge
TensorFlow - Read all examples from a TFRecords at once?,"<p>How do you read all examples from a TFRecords at once?</p>

<p>I've been using <code>tf.parse_single_example</code> to read out individual examples using code similar to that given in the method <code>read_and_decode</code> in the <a href=""https://github.com/tensorflow/tensorflow/blob/r0.8/tensorflow/examples/how_tos/reading_data/fully_connected_reader.py"" rel=""nofollow noreferrer"">example of the fully_connected_reader</a>. However, I want to run the network against my entire validation dataset at once, and so would like to load them in their entirety instead.</p>

<p>I'm not entirely sure, but <a href=""https://www.tensorflow.org/versions/r0.8/api_docs/python/io_ops.html#parse_example"" rel=""nofollow noreferrer"">the documentation</a> seems to suggest I can use <code>tf.parse_example</code> instead of <code>tf.parse_single_example</code> to load the entire TFRecords file at once. I can't seem to get this to work though. I'm guessing it has to do with how I specify the features, but I'm not sure how in the feature specification to state that there are multiple examples.</p>

<p>In other words, my attempt of using something similar to:</p>

<pre><code>reader = tf.TFRecordReader()
_, serialized_example = reader.read(filename_queue)
features = tf.parse_example(serialized_example, features={
    'image_raw': tf.FixedLenFeature([], tf.string),
    'label': tf.FixedLenFeature([], tf.int64),
})
</code></pre>

<p>isn't working, and I assume it's because the features aren't expecting multiple examples at once (but again, I'm not sure). [This results in an error of <code>ValueError: Shape () must have rank 1</code>]</p>

<p>Is this the proper way to read all the records at once? And if so, what do I need to change to actually read the records? Thank you much!</p>
",Training
TensorFlow: How to handle void labeled data in image segmentation?,"<p>I was wondering how to handle not labeled parts of an image in image segmentation using TensorFlow. For example, my input is an image of height * width * channels. The labels are too of the size height * width, with one label for every pixel.</p>

<p>Some parts of the image are annotated, other parts are not. I would wish that those parts have no influence on the gradient computation whatsoever. Furthermore, I am not interested in the network predicting this “void” label.</p>

<p>Is there a label or a function for this? At the moment I am using <code>tf.nn.sparse_softmax_cross_entropy_with_logits</code>.</p>
",Related to library: Lack of knowledge
Simple way to visualize a TensorFlow graph in Jupyter?,"<p>The official way to visualize a TensorFlow graph is with TensorBoard, but sometimes I just want a quick look at the graph when I'm working in Jupyter.</p>

<p>Is there a quick solution, ideally based on TensorFlow tools, or standard SciPy packages (like matplotlib), but if necessary based on 3rd party libraries?</p>
",Data Preparation: Shape mismatch
How to find the variable names that are saved in a tensorflow checkpoint?,"<p>I want to see the variables that are saved in a tensorflow checkpoint along with their values.
How can I find the variable names that are saved in a tensorflow checkpoint?</p>

<p><strong><em>EDIT :</em></strong> </p>

<p>I used <code>tf.train.NewCheckpointReader</code> which is explained <a href=""https://github.com/tensorflow/tensorflow/blob/861644c0bcae5d56f7b3f439696eefa6df8580ec/tensorflow/python/training/saver_test.py#L1203"" rel=""noreferrer"">here</a>. But, it is not given in the documentation of tensorflow. Is there any other way?</p>

<p>` </p>

<pre><code>    import tensorflow as tf
    v0 = tf.Variable([[1, 2, 3], [4, 5, 6]], dtype=tf.float32, name=""v0"")
    v1 = tf.Variable([[[1], [2]], [[3], [4]], [[5], [6]]], dtype=tf.float32,
                     name=""v1"")
    init_all_op = tf.initialize_all_variables()
    save = tf.train.Saver({""v0"": v0, ""v1"": v1})
    checkpoint_path = os.path.join(model_dir, ""model.ckpt"")    

    with tf.Session() as sess:
      sess.run(init_all_op)
      # Saves a checkpoint.      
      save.save(sess, checkpoint_path)

      # Creates a reader.
      reader = tf.train.NewCheckpointReader(checkpoint_path)
      print('reder:\n', reader)

      # Verifies that the tensors exist.
      print('is exist v0?', reader.has_tensor(""v0""))
      print('is exist v1?', reader.has_tensor(""v1""))

      # Verifies that debug string contains the right strings.
      debug_string = reader.debug_string()
      print('\n All Variables: \n', debug_string)

      # Verifies get_variable_to_shape_map() returns the correct information.
      var_map = reader.get_variable_to_shape_map()
      print('\n All Variables information :\n', var_map)

      # Verifies get_tensor() returns the tensor value.
      v0_tensor = reader.get_tensor(""v0"")
      v1_tensor = reader.get_tensor(""v1"")
      print('\n   returns the v0 tensor value:\n', v0_tensor)
      print('\n   returns the v1 tensor value:\n', v1_tensor)
</code></pre>

<p>`</p>
",Related to library: Lack of knowledge
How does one initialize a variable with tf.get_variable and a numpy value in TensorFlow?,"<p>I wanted to initialize some of the variable on my network with numpy values. For the sake of the example consider:</p>

<pre><code>init=np.random.rand(1,2)
tf.get_variable('var_name',initializer=init)
</code></pre>

<p>when I do that I get an error:</p>

<pre><code>ValueError: Shape of a new variable (var_name) must be fully defined, but instead was &lt;unknown&gt;.
</code></pre>

<p>why is it that I am getting that error? </p>

<p>To try to fix it I tried doing:</p>

<pre><code>tf.get_variable('var_name',initializer=init, shape=[1,2])
</code></pre>

<p>which yielded a even weirder error:</p>

<pre><code>TypeError: 'numpy.ndarray' object is not callable
</code></pre>

<p>I tried reading <a href=""https://www.tensorflow.org/versions/r0.9/api_docs/python/state_ops.html#variable_scope"" rel=""noreferrer"">the docs and examples</a> but it didn't really help.</p>

<p>Is it not possible to initialize variables with numpy arrays with the get_variable method in TensorFlow?</p>
",Related to library: Lack of knowledge
TensorFlow - Low GPU usage on Titan X,"<p>For a while, I have been noticing that TensorFlow (v0.8) does not seem to fully use the computation power of my Titan X. For several CNNs that I have been running the GPU usage does not seem to exceed ~30%. Typically the GPU utilization is even lower, more like 15%. One particular example of a CNN that shows this behavior is the CNN from DeepMind's Atari paper with Q-learning (see link below for code). </p>

<p>When I see other people of our lab running CNNs written in Theano or Torch the GPU usage is typically 80%+. This makes me wondering, why are the CNNs that I write in TensorFlow so 'slow' and what can I do to make more efficient use of the GPU processing power? Generally, I am interested in ways to profile the GPU operations and discover where the bottlenecks are. Any recommendations how to do this are very welcome since this seems not really possible with TensorFlow at the moment.</p>

<p>Things I did to find out more about the cause of this problem:</p>

<ol>
<li><p>Analyzing TensorFlow's <strong>device placement</strong>, everything seems to be on gpu:/0 so looks OK.</p></li>
<li><p>Using <strong>cProfile</strong>, I have optimized the batch generation and other preprocessing steps. The preprocessing is performed on a single thread, but the actual optimization performed by TensorFlow steps take much longer (see average runtimes below). One obvious idea to increase the speed is by using TFs queue runners, but since the batch preparation is already 20x faster than optimization I wonder whether this is going to make a big difference.</p>

<pre><code>Avg. Time Batch Preparation: 0.001 seconds
Avg. Time Train Operation:   0.021 seconds
Avg. Time Total per Batch:   0.022 seconds (45.18 batches/second)
</code></pre></li>
<li><p>Run on multiple machines to rule out hardware issues.</p></li>
<li><p>Upgraded to the latest versions of CuDNN v5 (RC), CUDA Toolkit 7.5 and reinstalled TensorFlow from sources about a week ago.</p></li>
</ol>

<p>An example of the Q-learning CNN for which this 'problem' occurs can be found here: <a href=""https://github.com/tomrunia/DeepReinforcementLearning-Atari/blob/master/qnetwork.py"" rel=""noreferrer"">https://github.com/tomrunia/DeepReinforcementLearning-Atari/blob/master/qnetwork.py</a></p>

<p>Example of NVIDIA SMI displaying the low GPU utilization: <a href=""https://i.stack.imgur.com/shmpF.png"" rel=""noreferrer"">NVIDIA-SMI</a></p>
",Related to library: Lack of knowledge
Use LSTM tutorial code to predict next word in a sentence?,"<p>I've been trying to understand the sample code with <a href=""https://www.tensorflow.org/tutorials/recurrent"" rel=""noreferrer"">https://www.tensorflow.org/tutorials/recurrent</a>
which you can find at <a href=""https://github.com/tensorflow/models/blob/master/tutorials/rnn/ptb/ptb_word_lm.py"" rel=""noreferrer"">https://github.com/tensorflow/models/blob/master/tutorials/rnn/ptb/ptb_word_lm.py</a></p>

<p>(Using tensorflow 1.3.0.)</p>

<p>I've summarized (what I think are) the key parts, for my question, below:</p>

<pre><code> size = 200
 vocab_size = 10000
 layers = 2
 # input_.input_data is a 2D tensor [batch_size, num_steps] of
 #    word ids, from 1 to 10000

 cell = tf.contrib.rnn.MultiRNNCell(
    [tf.contrib.rnn.BasicLSTMCell(size) for _ in range(2)]
    )

 embedding = tf.get_variable(
      ""embedding"", [vocab_size, size], dtype=tf.float32)
 inputs = tf.nn.embedding_lookup(embedding, input_.input_data)

inputs = tf.unstack(inputs, num=num_steps, axis=1)
outputs, state = tf.contrib.rnn.static_rnn(
    cell, inputs, initial_state=self._initial_state)

output = tf.reshape(tf.stack(axis=1, values=outputs), [-1, size])
softmax_w = tf.get_variable(
    ""softmax_w"", [size, vocab_size], dtype=data_type())
softmax_b = tf.get_variable(""softmax_b"", [vocab_size], dtype=data_type())
logits = tf.matmul(output, softmax_w) + softmax_b

# Then calculate loss, do gradient descent, etc.
</code></pre>

<p>My biggest question is <em>how do I use the produced model to actually generate a next word suggestion, given the first few words of a sentence</em>? Concretely, I imagine the flow is like this, but I cannot get my head around what the code for the commented lines would be:</p>

<pre><code>prefix = [""What"", ""is"", ""your""]
state = #Zeroes
# Call static_rnn(cell) once for each word in prefix to initialize state
# Use final output to set a string, next_word
print(next_word)
</code></pre>

<p>My sub-questions are:</p>

<ul>
<li>Why use a random (uninitialized, untrained) word-embedding?</li>
<li>Why use softmax?</li>
<li>Does the hidden layer have to match the dimension of the input (i.e. the dimension of the word2vec embeddings)</li>
<li>How/Can I bring in a pre-trained word2vec model, instead of that uninitialized one?</li>
</ul>

<p>(I'm asking them all as one question, as I suspect they are all connected, and connected to some gap in my understanding.)</p>

<p>What I was expecting to see here was loading an existing word2vec set of word embeddings (e.g. using gensim's <code>KeyedVectors.load_word2vec_format()</code>), convert each word in the input corpus to that representation when loading in each sentence, and then afterwards the LSTM would spit out a vector of the same dimension, and we would try and find the most similar word (e.g. using gensim's <code>similar_by_vector(y, topn=1)</code>).</p>

<p>Is using softmax saving us from the relatively slow <code>similar_by_vector(y, topn=1)</code> call?</p>

<hr>

<p>BTW, for the pre-existing word2vec part of my question <a href=""https://stackoverflow.com/q/42064690/841830"">Using pre-trained word2vec with LSTM for word generation</a> is similar. However the answers there, currently, are not what I'm looking for. What I'm hoping for is a plain English explanation that switches the light on for me, and plugs whatever the gap in my understanding is.　　<a href=""https://stackoverflow.com/questions/44614097/use-pre-trained-word2vec-in-lstm-language-model"">Use pre-trained word2vec in lstm language model?</a> is another similar question.</p>

<p><strong>UPDATE:</strong> <a href=""https://stackoverflow.com/q/33773661/841830"">Predicting next word using the language model tensorflow example</a> and <a href=""https://stackoverflow.com/q/36286594/841830"">Predicting the next word using the LSTM ptb model tensorflow example</a> are similar questions. However, neither shows the code to actually take the first few words of a sentence, and print out its prediction of the next word. I tried pasting in code from the 2nd question, and from <a href=""https://stackoverflow.com/a/39282697/841830"">https://stackoverflow.com/a/39282697/841830</a> (which comes with a github branch), but cannot get either to run without errors. I think they may be for an earlier version of TensorFlow?</p>

<p><strong>ANOTHER UPDATE:</strong> Yet another question asking basically the same thing: <a href=""https://stackoverflow.com/q/42333101/841830"">Predicting Next Word of LSTM Model from Tensorflow Example</a>
It links to 
<a href=""https://stackoverflow.com/q/33773661/841830"">Predicting next word using the language model tensorflow example</a> (and, again, the answers there are not quite what I am looking for).</p>

<p>In case it still isn't clear, what I am trying to write a high-level function called <code>getNextWord(model, sentencePrefix)</code>, where <code>model</code> is a previously built LSTM that I've loaded from disk, and <code>sentencePrefix</code> is a string, such as ""Open the"", and it might return ""pod"".  I then might call it with ""Open the pod"" and it will return ""bay"", and so on.</p>

<p>An example (with a character RNN, and using mxnet) is the <code>sample()</code> function shown near the end of <a href=""https://github.com/zackchase/mxnet-the-straight-dope/blob/master/chapter05_recurrent-neural-networks/simple-rnn.ipynb"" rel=""noreferrer"">https://github.com/zackchase/mxnet-the-straight-dope/blob/master/chapter05_recurrent-neural-networks/simple-rnn.ipynb</a>
You can call <code>sample()</code> during training, but you can also call it after training, and with any sentence you want.</p>
",Related to library: Lack of knowledge
Exception CallbackOnCollectedDelegate when creating tensorflow graph,"<p>I try to build a little tensorflow application with TensorFlowSharp and sometimes I recieve this exception:</p>

<blockquote>
  <p>Managed Debugging Assistant 'CallbackOnCollectedDelegate' </p>
</blockquote>

<p>For the function <em>TensorFlowSharp!TensorFlow.TFBuffer+BufferReleaseFunc::Invoke</em></p>

<p>I tried to find out what it means but I didn't fully understand the explanations. This is the part of the code where the exception is thrown:</p>

<pre><code>var graph = new TFGraph();
var model = File.ReadAllBytes(ModelsFile);
graph.Import(model, """");
</code></pre>

<p>Does somebody know what I should do to prevent this exception?</p>

<p>Bruno</p>
",
How to create an optimizer in Tensorflow,"<p>I want to write a new optimization algorithm for my network on Tensorflow. I hope to implement the <a href=""https://en.wikipedia.org/wiki/Levenberg%E2%80%93Marquardt_algorithm"">Levenberg Marquardt optimization algorithm</a>, which now is excluded from TF API. I found poor documentation on how to write a custom optimizer, so i ask if someone can give my any advice. Thanks.</p>
",
Is RNN initial state reset for subsequent mini-batches?,"<p>Could someone please clarify whether the initial state of the RNN in TF is reset for subsequent mini-batches, or the last state of the previous mini-batch is used as mentioned in <a href=""https://arxiv.org/abs/1409.2329"" rel=""nofollow noreferrer"">Ilya Sutskever et al., ICLR 2015 </a> ?</p>
",
How to build a shared library for TensorFlow on Travis-CI,"<p>I'm assisting in building a <a href=""https://github.com/Arafatk/tensorflow.rb"">Ruby-wrapper</a> for <a href=""https://www.tensorflow.org/"">TensorFlow</a>. Obviously. we'd want to setup automatic test of the project and so I'm <a href=""https://github.com/chrhansen/tensorflow.rb/pull/1/files"">currently</a> <strike>trying</strike> struggling to configure Travis-CI to build the project and run tests like I can on my own machine (OSX El Capitan). </p>

<p>My question is: What is the magic that needs to go in the <code>.travis.yml</code>-file to properly <code>bazel build</code> a shared lib (<code>tensorflow.so</code>) for TensorFlow?</p>

<p>As far as I can tell, I've successfully installed <a href=""https://www.tensorflow.org/versions/r0.9/get_started/os_setup.html#installing-from-sources"">TensorFlow's dependencies</a>, including Google's build tool <a href=""http://www.bazel.io/"">Bazel</a>, but Travis-CI still can't build. As of this writing the <code>bazel build</code> command fails with the message:</p>

<pre><code>...
...
INFO: Building...
[1 / 13] Writing file external/gif_archive/libgif.so-2.params
[3 / 13] Compiling external/gif_archive/giflib-5.1.4/lib/egif_lib.c
ERROR: /home/travis/.cache/bazel/_bazel_travis/1a58902034d650eeef2a9da5b1248179/external/gif_archive/BUILD:14:1: C++ compilation of rule '@gif_archive//:gif' failed: namespace-sandbox failed: error executing command 
  (cd /home/travis/.cache/bazel/_bazel_travis/1a58902034d650eeef2a9da5b1248179/execroot/tensorflow &amp;&amp; \
  exec env - \
...
...
The command ""bazel build --verbose_failures=1 @gif_archive//:gif"" failed and exited with 1 during .
</code></pre>

<p><a href=""https://travis-ci.org/chrhansen/tensorflow.rb/builds/145716589"">https://travis-ci.org/chrhansen/tensorflow.rb/builds/145716589</a></p>

<p>Note: I've noticed that the guys over at github.com/node-tensorflow/node-tensorflow successfully made this happen: <a href=""https://travis-ci.org/node-tensorflow/node-tensorflow/builds"">https://travis-ci.org/node-tensorflow/node-tensorflow/builds</a></p>

<p>UPDATE: <a href=""https://github.com/tensorflow/tensorflow/issues/3374"">https://github.com/tensorflow/tensorflow/issues/3374</a></p>
",
What is the equivalent of np.std() in TensorFlow?,"<p>Just looking for the equivalent of np.std() in TensorFlow to calculate the standard deviation of a tensor.</p>
",
TensorFlow operator overloading,"<p>What is the difference between </p>

<pre><code>   tf.add(x, y)
</code></pre>

<p>and </p>

<pre><code>   x + y
</code></pre>

<p>in TensorFlow? What would be different in your computation graph when you construct your graph with <code>+</code> instead of <code>tf.add()</code>? </p>

<p>More generally, are  <code>+</code> or other operations overloaded for tensors?</p>
",
For what reason Convolution 1x1 is used in deep neural networks?,"<p>I'm looking at InceptionV3 (GoogLeNet) architecture and cannot understand why do we need conv1x1 layers?</p>

<p>I know how convolution works, but I see a profit with patch size > 1. </p>
",
What does TensorFlow's `conv2d_transpose()` operation do?,"<p>The documentation for the <code>conv2d_transpose()</code> operation does not clearly explain what it does:</p>

<blockquote>
  <p>The transpose of conv2d.</p>
  
  <p>This operation is sometimes called ""deconvolution"" after
  <a href=""http://www.matthewzeiler.com/pubs/cvpr2010/cvpr2010.pdf"" rel=""noreferrer"">Deconvolutional Networks</a>, but is actually the transpose (gradient) of
  conv2d rather than an actual deconvolution.</p>
</blockquote>

<p>I went through the paper that the doc points to, but it did not help.</p>

<p>What does this operation do and what are examples of why you would want to use it?</p>
",
Using tf.unpack() when first dimension of Variable is None,"<p>I'm feeding in a dynamic shaped Tensor using: </p>

<p><code>x = tf.placeholder(tf.int32, shape=[None, vector_size])</code></p>

<p>I need to turn this into a list of Tensors that have <code>shape=[1, vector_size]</code> using <code>x_list = tf.unpack(x, 0)</code></p>

<p>But it raises a <code>ValueError</code> because the length of the first dimension is not known i.e. it's <code>None</code>. </p>

<p>I've been trying to get around this by using another <code>tf.placeholder</code> to dynamically supply the shape of <code>x</code> but the parameter <code>shape</code> cannot be a Tensor.</p>

<p>How can I use <code>tf.unpack()</code> in this situation? </p>

<p>Or is there another function that can also turn the variable that I feed in into a list of Tensors?</p>

<p>Thanks in advance. </p>
",
TensorFlow freeze_graph.py: The name 'save/Const:0' refers to a Tensor which does not exist,"<p>I am currently trying to export a trained TensorFlow model as a ProtoBuf file to use it with the TensorFlow C++ API on Android. Therefore, I'm using the <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/freeze_graph.py"" rel=""noreferrer""><code>freeze_graph.py</code></a> script.</p>

<p>I exported my model using <code>tf.train.write_graph</code>:</p>

<p><code>tf.train.write_graph(graph_def, FLAGS.save_path, out_name, as_text=True)
</code></p>

<p>and I'm using a checkpoint saved with <code>tf.train.Saver</code>.</p>

<p>I invoke <code>freeze_graph.py</code> as described at the top of the script. After compiling, I run</p>

<pre><code>bazel-bin/tensorflow/python/tools/freeze_graph \
--input_graph=&lt;path_to_protobuf_file&gt; \
--input_checkpoint=&lt;model_name&gt;.ckpt-10000 \
--output_graph=&lt;output_protobuf_file_path&gt; \
--output_node_names=dropout/mul_1
</code></pre>

<p>This gives me the following error message:</p>

<pre><code>TypeError: Cannot interpret feed_dict key as Tensor: The name 'save/Const:0' refers to a Tensor which does not exist. The operation, 'save/Const', does not exist in the graph.
</code></pre>

<p>As the error states I do not have a tensor <code>save/Const:0</code> in my exported model. However, the code of <code>freeze_graph.py</code> says that one can specify this tensor name by the flag <code>filename_tensor_name</code>. Unfortunately I cannot find any information on what this tensor should be and how to set it correctly for my model.</p>

<p>Can somebody tell my either how to produce a <code>save/Const:0</code> tensor in my exported ProtoBuf model or how to set the flag <code>filename_tensor_name</code> correctly?</p>
",
Should we do learningrate decay for adam optimizer,"<p>I'm training a network for image localization with Adam optimizer, and someone suggest me to use exponential decay. I don't want to try that because Adam optimizer itself decays learning rate. But that guy insists and he said he did that before. So should I do that and is there any theory behind your suggestion?</p>
",
Tensorflow Java Multi-GPU inference,"<p>I have a server with multiple GPUs and want to make full use of them during model inference inside a java app. 
By default tensorflow seizes all available GPUs, but uses only the first one.</p>

<p>I can think of three options to overcome this issue:</p>

<ol>
<li><p>Restrict device visibility on process level, namely using <code>CUDA_VISIBLE_DEVICES</code> environment variable.</p>

<p>That would require me to run several instances of the java app and distribute traffic among them. Not that tempting idea.</p></li>
<li><p>Launch several sessions inside a single application and try to assign one device to each of them via <code>ConfigProto</code>: </p>

<pre><code>public class DistributedPredictor {

    private Predictor[] nested;
    private int[] counters;

    // ...

    public DistributedPredictor(String modelPath, int numDevices, int numThreadsPerDevice) {
        nested = new Predictor[numDevices];
        counters = new int[numDevices];

        for (int i = 0; i &lt; nested.length; i++) {
            nested[i] = new Predictor(modelPath, i, numDevices, numThreadsPerDevice);
        }
    }

    public Prediction predict(Data data) {
        int i = acquirePredictorIndex();
        Prediction result = nested[i].predict(data);
        releasePredictorIndex(i);
        return result;
    }

    private synchronized int acquirePredictorIndex() {
        int i = argmin(counters);
        counters[i] += 1;
        return i;
    }

    private synchronized void releasePredictorIndex(int i) {
        counters[i] -= 1;
    }
}


public class Predictor {

    private Session session;

    public Predictor(String modelPath, int deviceIdx, int numDevices, int numThreadsPerDevice) {

        GPUOptions gpuOptions = GPUOptions.newBuilder()
                .setVisibleDeviceList("""" + deviceIdx)
                .setAllowGrowth(true)
                .build();

        ConfigProto config = ConfigProto.newBuilder()
                .setGpuOptions(gpuOptions)
                .setInterOpParallelismThreads(numDevices * numThreadsPerDevice)
                .build();

        byte[] graphDef = Files.readAllBytes(Paths.get(modelPath));
        Graph graph = new Graph();
        graph.importGraphDef(graphDef);

        this.session = new Session(graph, config.toByteArray());
    }

    public Prediction predict(Data data) {
        // ...
    }
}
</code></pre>

<p>This approach seems to work fine at a glance. However, sessions occasionally ignore <code>setVisibleDeviceList</code> option and all go for the first device causing Out-Of-Memory crash.</p></li>
<li><p>Build the model in a multi-tower fashion in python using <code>tf.device()</code> specification. On java side, give different <code>Predictor</code>s different towers inside a shared session.</p>

<p>Feels cumbersome and idiomatically wrong to me. </p></li>
</ol>

<p><strong>UPDATE:</strong> As @ash proposed, there's yet another option:</p>

<ol start=""4"">
<li><p>Assign an appropriate device to each operation of the existing graph by modifying its definition (<code>graphDef</code>). </p>

<p>To get it done, one could adapt the code from Method 2:</p>

<pre><code>public class Predictor {

    private Session session;

    public Predictor(String modelPath, int deviceIdx, int numDevices, int numThreadsPerDevice) {

        byte[] graphDef = Files.readAllBytes(Paths.get(modelPath));
        graphDef = setGraphDefDevice(graphDef, deviceIdx)

        Graph graph = new Graph();
        graph.importGraphDef(graphDef);

        ConfigProto config = ConfigProto.newBuilder()
                .setAllowSoftPlacement(true)
                .build();

        this.session = new Session(graph, config.toByteArray());
    }

    private static byte[] setGraphDefDevice(byte[] graphDef, int deviceIdx) throws InvalidProtocolBufferException {
        String deviceString = String.format(""/gpu:%d"", deviceIdx);

        GraphDef.Builder builder = GraphDef.parseFrom(graphDef).toBuilder();
        for (int i = 0; i &lt; builder.getNodeCount(); i++) {
            builder.getNodeBuilder(i).setDevice(deviceString);
        }
        return builder.build().toByteArray();
    }

    public Prediction predict(Data data) {
        // ...
    }
}
</code></pre>

<p>Just like other mentioned approaches, this one doesn't set me free from manually distributing data among devices. But at least it works stably and is comparably easy to implement. Overall, this looks like an (almost) normal technique.</p></li>
</ol>

<p>Is there an elegant way to do such a basic thing with tensorflow java API? Any ideas would be appreciated. </p>
",
tf.SequenceExample with multidimensional arrays,"<p>In Tensorflow, I want to save a multidimensional array to a TFRecord. For example:</p>

<pre><code>[[1, 2, 3], [1, 2], [3, 2, 1]]
</code></pre>

<p>As the task I am trying to solve is sequential, I am trying to use Tensorflow's <code>tf.train.SequenceExample()</code> and when writing the data I am successful in writing the data to a TFRecord file. However, when I try to load the data from the TFRecord file using <code>tf.parse_single_sequence_example</code>, I am greeted with a large number of cryptic errors:</p>

<pre><code>W tensorflow/core/framework/op_kernel.cc:936] Invalid argument: Name: , Key: input_characters, Index: 1.  Number of int64 values != expected.  values size: 6 but output shape: []
E tensorflow/core/client/tensor_c_api.cc:485] Name: , Key: input_characters, Index: 1.  Number of int64 values != expected.  values size: 6 but output shape: []
</code></pre>

<p>The function I am using to try to load my data is below:</p>

<pre><code>def read_and_decode_single_example(filename):

    filename_queue = tf.train.string_input_producer([filename],
                                                num_epochs=None)

    reader = tf.TFRecordReader()
    _, serialized_example = reader.read(filename_queue)

    context_features = {
         ""length"": tf.FixedLenFeature([], dtype=tf.int64)
    }

    sequence_features = {
         ""input_characters"": tf.FixedLenSequenceFeature([],           dtype=tf.int64),
         ""output_characters"": tf.FixedLenSequenceFeature([], dtype=tf.int64)
    }

    context_parsed, sequence_parsed = tf.parse_single_sequence_example(
    serialized=serialized_example,
    context_features=context_features,
    sequence_features=sequence_features
)

context = tf.contrib.learn.run_n(context_parsed, n=1, feed_dict=None)
print context
</code></pre>

<p>The function that I am using to save the data is here:</p>

<pre><code># http://www.wildml.com/2016/08/rnns-in-tensorflow-a-practical-guide-and-undocumented-features/
def make_example(input_sequence, output_sequence):
    """"""
    Makes a single example from Python lists that follows the
    format of tf.train.SequenceExample.
    """"""

    example_sequence = tf.train.SequenceExample()

    # 3D length
    sequence_length = sum([len(word) for word in input_sequence])
    example_sequence.context.feature[""length""].int64_list.value.append(sequence_length)

    input_characters = example_sequence.feature_lists.feature_list[""input_characters""]
    output_characters = example_sequence.feature_lists.feature_list[""output_characters""]

    for input_character, output_character in izip_longest(input_sequence,
                                                          output_sequence):

        # Extend seems to work, therefore it replaces append.
        if input_sequence is not None:
            input_characters.feature.add().int64_list.value.extend(input_character)

        if output_characters is not None:
            output_characters.feature.add().int64_list.value.extend(output_character)

    return example_sequence
</code></pre>

<p>Any help would be welcomed.</p>
",
How can I copy a variable in tensorflow,"<p>In numpy I can create a copy of the variable with <a href=""http://docs.scipy.org/doc/numpy/reference/generated/numpy.copy.html"">numpy.copy</a>. Is there a similar method, that I can use to create a copy of a Tensor in TensorFlow?</p>
",
Why is this TensorFlow implementation vastly less successful than Matlab's NN?,"<p>As a toy example I'm trying to fit a function <code>f(x) = 1/x</code> from 100 no-noise data points. The matlab default implementation is phenomenally successful with mean square difference ~10^-10, and interpolates perfectly.</p>

<p>I implement a neural network with one hidden layer of 10 sigmoid neurons. I'm a beginner at neural networks so be on your guard against dumb code.</p>

<pre><code>import tensorflow as tf
import numpy as np

def weight_variable(shape):
  initial = tf.truncated_normal(shape, stddev=0.1)
  return tf.Variable(initial)

def bias_variable(shape):
  initial = tf.constant(0.1, shape=shape)
  return tf.Variable(initial)

#Can't make tensorflow consume ordinary lists unless they're parsed to ndarray
def toNd(lst):
    lgt = len(lst)
    x = np.zeros((1, lgt), dtype='float32')
    for i in range(0, lgt):
        x[0,i] = lst[i]
    return x

xBasic = np.linspace(0.2, 0.8, 101)
xTrain = toNd(xBasic)
yTrain = toNd(map(lambda x: 1/x, xBasic))

x = tf.placeholder(""float"", [1,None])
hiddenDim = 10

b = bias_variable([hiddenDim,1])
W = weight_variable([hiddenDim, 1])

b2 = bias_variable([1])
W2 = weight_variable([1, hiddenDim])

hidden = tf.nn.sigmoid(tf.matmul(W, x) + b)
y = tf.matmul(W2, hidden) + b2

# Minimize the squared errors.
loss = tf.reduce_mean(tf.square(y - yTrain))
optimizer = tf.train.GradientDescentOptimizer(0.5)
train = optimizer.minimize(loss)

# For initializing the variables.
init = tf.initialize_all_variables()

# Launch the graph
sess = tf.Session()
sess.run(init)

for step in xrange(0, 4001):
    train.run({x: xTrain}, sess)
    if step % 500 == 0:
        print loss.eval({x: xTrain}, sess)
</code></pre>

<p>Mean square difference ends at ~2*10^-3, so about 7 orders of magnitude worse than matlab. Visualising with</p>

<pre><code>xTest = np.linspace(0.2, 0.8, 1001)
yTest = y.eval({x:toNd(xTest)}, sess)  
import matplotlib.pyplot as plt
plt.plot(xTest,yTest.transpose().tolist())
plt.plot(xTest,map(lambda x: 1/x, xTest))
plt.show()
</code></pre>

<p>we can see the fit is systematically imperfect:
<a href=""https://i.stack.imgur.com/Blxq9.png""><img src=""https://i.stack.imgur.com/Blxq9.png"" alt=""enter image description here""></a>
while the matlab one looks perfect to the naked eye with the differences uniformly &lt; 10^-5:
<a href=""https://i.stack.imgur.com/kC8aJ.jpg""><img src=""https://i.stack.imgur.com/kC8aJ.jpg"" alt=""enter image description here""></a>
I have tried to replicate with TensorFlow the diagram of the Matlab network:</p>

<p><a href=""https://i.stack.imgur.com/ORLXL.png""><img src=""https://i.stack.imgur.com/ORLXL.png"" alt=""enter image description here""></a></p>

<p>Incidentally, the diagram seems to imply a tanh rather than sigmoid activation function. I cannot find it anywhere in documentation to be sure. However, when I try to use a tanh neuron in TensorFlow the fitting quickly fails with <code>nan</code> for variables. I do not know why.</p>

<p>Matlab uses Levenberg–Marquardt training algorithm. Bayesian regularization is even more successful with mean squares at 10^-12 (we are probably in the area of vapours of float arithmetic).</p>

<p>Why is TensorFlow implementation so much worse, and what can I do to make it better?</p>
",
Tensorflow model zoo?,"<p>One of the main advantages of caffe for me was the possibility of doing transfer learning on freely distributed pretrained models.</p>

<p>Is there a place to get trained models from papers/competitions in tensorflow format? </p>

<p>If not, is there a possibility to convert existing caffe(or any other) models into tensorflow models?</p>
",
How to use stop_gradient in Tensorflow,"<p>I'm wondering how to use <code>stop_gradient</code> in tensorflow, and the documentation is not clear to me. </p>

<p>I'm currently using <code>stop_gradient</code> to produce the gradient of the loss function w.r.t. the word embeddings in a CBOW word2vec model. I want to just get the value, and not do backpropagation (as I'm generating adversarial examples). </p>

<p>Currently, I'm using the code:</p>

<pre><code>lossGrad = gradients.gradients(loss, embed)[0]
real_grad = lossGrad.eval(feed_dict)
</code></pre>

<p><s>But when I run this, it does the backpropogation anyway!</s> What am I doing wrong, and just as importantly, how can I fix this? </p>

<p>CLARIFICATION: To clarify by ""backpropagation"" I mean ""calculating values and updating model parameters"".</p>

<h3>UPDATE</h3>

<p>If I run the two lines above after the first training step, the I get a different loss after 100 training steps than when I don't run those two lines. I might be fundamentally misunderstanding something about Tensorflow.</p>

<p>I've tried setting using <code>set_random_seed</code> both in the beginning of the graph declaration and before each training step. The total loss is consistent between multiple runs, but not between including/excluding those two lines. So if it's not the RNG causing the disparity, and it's not unanticipated updating of the model parameters between training steps, do you have any idea what would cause this behavior? </p>

<h3>SOLUTION</h3>

<p>Welp, it's a bit late but here's how I solved it. I only wanted to optimize over some, but not all, variables. I thought that the way to prevent optimizing some variables would be to use <code>stop_grad</code> - but I never found a way to make that work. Maybe there is a way, but what worked for me was to adjust my <code>optimizer</code> to only optimize over a list of variables. So instead of:</p>

<pre><code>opt = tf.train.GradientDescentOptimizer(learning_rate=eta)
train_op = opt.minimize(loss)
</code></pre>

<p>I used:</p>

<pre><code>opt = tf.train.GradientDescentOptimizer(learning_rate=eta)
train_op = opt.minimize(loss, var_list=[variables to optimize over])
</code></pre>

<p>This prevented <code>opt</code> from updating the variables not in <code>var_list</code>. Hopefully it works for you, too!</p>
",
How to create a tensorflow serving client for the 'wide and deep' model?,"<p>I've created a model based on the 'wide and deep' example (<a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/learn/wide_n_deep_tutorial.py"">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/learn/wide_n_deep_tutorial.py</a>).</p>

<p>I've exported the model as follows:</p>

<pre><code>  m = build_estimator(model_dir)
  m.fit(input_fn=lambda: input_fn(df_train, True), steps=FLAGS.train_steps)
  results = m.evaluate(input_fn=lambda: input_fn(df_test, True), steps=1)

  print('Model statistics:')

  for key in sorted(results):
    print(""%s: %s"" % (key, results[key]))

  print('Done training!!!')

  # Export model
  export_path = sys.argv[-1]
  print('Exporting trained model to %s' % export_path)

  m.export(
   export_path,
   input_fn=serving_input_fn,
   use_deprecated_input_fn=False,
   input_feature_key=INPUT_FEATURE_KEY
</code></pre>

<p>My question is, how do I create a client to make predictions from this exported model?  Also, have I exported the model correctly?</p>

<p>Ultimately I need to be able do this in Java too.  I suspect I can do this by creating Java classes from proto files using gRPC.</p>

<p>Documentation is very sketchy, hence why I am asking on here.</p>

<p>Many thanks!</p>
",
TensorFlow - numpy-like tensor indexing,"<p>In numpy, we can do this:</p>

<pre class=""lang-py prettyprint-override""><code>x = np.random.random((10,10))
a = np.random.randint(0,10,5)
b = np.random.randint(0,10,5)
x[a,b] # gives 5 entries from x, indexed according to the corresponding entries in a and b
</code></pre>

<p>When I try something equivalent in TensorFlow:</p>

<pre><code>xt = tf.constant(x)
at = tf.constant(a)
bt = tf.constant(b)
xt[at,bt]
</code></pre>

<p>The last line gives a ""Bad slice index tensor"" exception. It seems TensorFlow doesn't support indexing like numpy or Theano.</p>

<p>Does anybody know if there is a TensorFlow way of doing this (indexing a tensor by arbitrary values). I've seen the tf.nn.embedding part, but I'm not sure they can be used for this and even if they can, it's a huge workaround for something this straightforward.</p>

<p>(Right now, I'm feeding the data from <code>x</code> as an input and doing the indexing in numpy but I hoped to put <code>x</code> inside TensorFlow to get higher efficiency)</p>
",
What is difference between tf.truncated_normal and tf.random_normal?,"<p><code>tf.random_normal(shape, mean=0.0, stddev=1.0, dtype=tf.float32, seed=None, name=None)</code> outputs random values from a normal distribution.</p>

<p><code>tf.truncated_normal(shape, mean=0.0, stddev=1.0, dtype=tf.float32, seed=None, name=None)</code> outputs random values from a truncated normal distribution.</p>

<p>I tried googling 'truncated normal distribution'. But didn't understand much.</p>
",
How to average summaries over multiple batches?,"<p>Assuming I have a bunch of summaries defined like:</p>

<pre class=""lang-py prettyprint-override""><code>loss = ...
tf.scalar_summary(""loss"", loss)
# ...
summaries = tf.merge_all_summaries()
</code></pre>

<p>I can evaluate the <code>summaries</code> tensor every few steps on the training data and pass the result to a <code>SummaryWriter</code>.
The result will be noisy summaries, because they're only computed on one batch.</p>

<p>However, I would like to compute the summaries on the entire validation dataset.
Of course, I can't pass the validation dataset as a single batch, because it would be too big.
So, I'll get summary outputs for each validation batch.</p>

<p>Is there a way to average those summaries so that it appears as if the summaries have been computed on the entire validation set?</p>
",
Visualizing output of convolutional layer in tensorflow,"

<p>I'm trying to visualize the output of a convolutional layer in tensorflow using the function <code>tf.image_summary</code>. I'm already using it successfully in other instances (e. g. visualizing the input image), but have some difficulties reshaping the output here correctly. I have the following conv layer:</p>

<pre class=""lang-py prettyprint-override""><code>img_size = 256
x_image = tf.reshape(x, [-1,img_size, img_size,1], ""sketch_image"")

W_conv1 = weight_variable([5, 5, 1, 32])
b_conv1 = bias_variable([32])

h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)
</code></pre>

<p>So the output of <code>h_conv1</code> would have the shape <code>[-1, img_size, img_size, 32]</code>. Just using <code>tf.image_summary(""first_conv"", tf.reshape(h_conv1, [-1, img_size, img_size, 1]))</code> Doesn't account for the 32 different kernels, so I'm basically slicing through different feature maps here.</p>

<p>How can I reshape them correctly? Or is there another helper function I could use for including this output in the summary?</p>
",
Is sparse tensor multiplication implemented in TensorFlow?,"

<p>Multiplication of sparse tensors with themselves or with dense tensors does not seem to work in TensorFlow.  The following example</p>

<pre class=""lang-py prettyprint-override""><code>from __future__ import print_function
import tensorflow as tf

x = tf.constant([[1.0,2.0],
                 [3.0,4.0]])
y = tf.SparseTensor(indices=[[0,0],[1,1]], values=[1.0,1.0], shape=[2,2])
z = tf.matmul(x,y)

sess = tf.Session()
sess.run(tf.initialize_all_variables())
print(sess.run([x, y, z]))
</code></pre>

<p>fails with the error message</p>

<pre class=""lang-py prettyprint-override""><code>TypeError: Input 'b' of 'MatMul' Op has type string that does not match type 
float32 of argument 'a'
</code></pre>

<p>Both tensors have values of type float32 as seen by evaluating them  without the multiplication op.  Multiplication of y with itself returns a similar error message.  Multipication of x with itself works fine.</p>
",
Extract features using pre-trained (Tensorflow) CNN,"<p>Deep Learning has been applied successfully on several large data sets for the classification of a handful of classes (cats, dogs, cars, planes, etc), with performances beating simpler descriptors like Bags of Features over SIFT, color histograms, etc.</p>

<p>Nevertheless, training such a network requires a lot of data per class and a lot of training time. However, very often one doesn't have enough data or just wants to get an idea of how well a convolutional neural network might do, before spending time one designing and training such a device and gathering the training data.</p>

<p>In this particular case, it might be ideal to have a network configured and trained using some benchmark data set used by the state of the art publications, and to simply apply it to some data set that you might have as a feature extractor.</p>

<p>This results in a set of features for each image, which one could feed to a classical classification method like SVM's, logistic regression, neural networks, etc.</p>

<p>In particular when one does not have enough data to train the CNN, I may expect this to outperform a pipeline where the CNN was trained on few samples.</p>

<p>I was looking at the tensorflow tutorials, but they always seem to have a clear training / testing phase. I couldn't find a pickle file (or similar) with a pre-configured CNN feature extractor.</p>

<p>My questions are: do such pre-trained networks exist and where can I find them. Alternatively: does this approach make sense? Where could I find a CNN+weights ?</p>

<p><strong>EDIT</strong>
W.r.t. @john's comment I tried using <code>'DecodeJpeg:0'</code> and <code>'DecodeJpeg/contents:0'</code> and checked the outputs, which are different (:S)</p>



<pre class=""lang-py prettyprint-override""><code>import cv2, requests, numpy
import tensorflow.python.platform
import tensorflow as tf


response = requests.get('https://i.stack.imgur.com/LIW6C.jpg?s=328&amp;g=1')
data = numpy.asarray(bytearray(response.content), dtype=np.uint8)
image = cv2.imdecode(data,-1)

compression_worked, jpeg_data = cv2.imencode('.jpeg', image)
if not compression_worked:
    raise Exception(""Failure when compressing image to jpeg format in opencv library"")
jpeg_data = jpeg_data.tostring()

with open('./deep_learning_models/inception-v3/classify_image_graph_def.pb', 'rb') as graph_file:
    graph_def = tf.GraphDef()
    graph_def.ParseFromString(graph_file.read())
    tf.import_graph_def(graph_def, name='')

with tf.Session() as sess:
    softmax_tensor = sess.graph.get_tensor_by_name('pool_3:0')

    arr0 = numpy.squeeze(sess.run(
        softmax_tensor,
        {'DecodeJpeg:0': image}
    ))

    arr1 = numpy.squeeze(sess.run(
        softmax_tensor,
        {'DecodeJpeg/contents:0': jpeg_data}
    ))

    print(numpy.abs(arr0 - arr1).max())
</code></pre>

<p>So the max absolute difference is <code>1.27649</code>, and in general all the elements differ (especially since the average value of the <code>arr0</code> and <code>arr1</code> themselves lies between 0 - 0.5).</p>

<p>I also would expect that <code>'DecodeJpeg:0'</code> needs a jpeg-string, not a numpy array, why else does the name contain 'Jpeg'. @john: Could you state how
 sure you are about your comment?</p>

<p>So I guess I'm not sure what is what, as I would expect a trained neural network to be deterministic (but chaotic at most).</p>
",
"In TensorFlow, what is the difference between Session.run() and Tensor.eval()?","<p>TensorFlow has two ways to evaluate part of graph: <code>Session.run</code> on a list of variables and <code>Tensor.eval</code>.  Is there a difference between these two?</p>
",
Fail to run word embedding example in tensorflow tutorial with GPUs,"<p>I am trying to run the word embedding example code at <a href=""https://github.com/tensorflow/tensorflow/tree/master/tensorflow/g3doc/tutorials/word2vec"" rel=""noreferrer"">https://github.com/tensorflow/tensorflow/tree/master/tensorflow/g3doc/tutorials/word2vec</a> (installed with GPU version of tensorflow under Ubuntu 14.04), but it returns the following error message:</p>

<pre><code>Found and verified text8.zip
Data size 17005207
Most common words (+UNK) [['UNK', 418391], ('the', 1061396), ('of', 593677), ('and', 416629), ('one', 411764)]
Sample data [5239, 3084, 12, 6, 195, 2, 3137, 46, 59, 156]
3084 -&gt; 12
originated -&gt; as
3084 -&gt; 5239
originated -&gt; anarchism
12 -&gt; 3084
as -&gt; originated
12 -&gt; 6
as -&gt; a
6 -&gt; 12
a -&gt; as
6 -&gt; 195
a -&gt; term
195 -&gt; 6
term -&gt; a
195 -&gt; 2
term -&gt; of
I tensorflow/core/common_runtime/local_device.cc:25] Local device intra op parallelism threads: 12
I tensorflow/core/common_runtime/gpu/gpu_init.cc:88] Found device 0 with properties: 
name: GeForce GTX TITAN X
major: 5 minor: 2 memoryClockRate (GHz) 1.076
pciBusID 0000:03:00.0
Total memory: 12.00GiB
Free memory: 443.32MiB
I tensorflow/core/common_runtime/gpu/gpu_init.cc:88] Found device 1 with properties: 
name: GeForce GTX TITAN X
major: 5 minor: 2 memoryClockRate (GHz) 1.076
pciBusID 0000:05:00.0
Total memory: 12.00GiB
Free memory: 451.61MiB
I tensorflow/core/common_runtime/gpu/gpu_init.cc:112] DMA: 0 1 
I tensorflow/core/common_runtime/gpu/gpu_init.cc:122] 0:   Y Y 
I tensorflow/core/common_runtime/gpu/gpu_init.cc:122] 1:   Y Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:643] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:03:00.0)
I tensorflow/core/common_runtime/gpu/gpu_device.cc:643] Creating TensorFlow device (/gpu:1) -&gt; (device: 1, name: GeForce GTX TITAN X, pci bus id: 0000:05:00.0)
I tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:47] Setting region size to 254881792
I tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:47] Setting region size to 263835648
I tensorflow/core/common_runtime/local_session.cc:45] Local session inter op parallelism threads: 12
Initialized
Traceback (most recent call last):
  File ""word2vec_basic.py"", line 171, in &lt;module&gt;
    _, loss_val = session.run([optimizer, loss], feed_dict=feed_dict)
  File ""/home/chentingpc/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 345, in run
    results = self._do_run(target_list, unique_fetch_targets, feed_dict_string)
  File ""/home/chentingpc/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 419, in _do_run
    e.code)
tensorflow.python.framework.errors.InvalidArgumentError: Cannot assign a device to node 'GradientDescent/update_Variable_2/ScatterSub': Could not satisfy explicit device specification '' because the node was colocated with a group of nodes that required incompatible device '/job:localhost/replica:0/task:0/GPU:0'
     [[Node: GradientDescent/update_Variable_2/ScatterSub = ScatterSub[T=DT_FLOAT, Tindices=DT_INT64, use_locking=false](Variable_2, gradients/concat_1, GradientDescent/update_Variable_2/mul)]]
Caused by op u'GradientDescent/update_Variable_2/ScatterSub', defined at:
  File ""word2vec_basic.py"", line 145, in &lt;module&gt;
    optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss)
  File ""/home/chentingpc/anaconda/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py"", line 167, in minimize
    name=name)
  File ""/home/chentingpc/anaconda/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py"", line 256, in apply_gradients
    update_ops.append(self._apply_sparse(grad, var))
  File ""/home/chentingpc/anaconda/lib/python2.7/site-packages/tensorflow/python/training/gradient_descent.py"", line 40, in _apply_sparse
    return var.scatter_sub(delta, use_locking=self._use_locking)
  File ""/home/chentingpc/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/variables.py"", line 324, in scatter_sub
    use_locking=use_locking)
  File ""/home/chentingpc/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/gen_state_ops.py"", line 227, in scatter_sub
    name=name)
  File ""/home/chentingpc/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/op_def_library.py"", line 633, in apply_op
    op_def=op_def)
  File ""/home/chentingpc/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1710, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/home/chentingpc/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 988, in __init__
    self._traceback = _extract_stack()
</code></pre>

<p>When I run the code in CPU version tensorflow, it works just fine. But not for GPU version. I also tried to use tf.device('/cpu:0') to force it using CUP instead of GPU, but it produces the same output.</p>

<p>Is there any function in this example cannot be run in GPUs? And how do I switch to CPU without reinstalling CPU version of tensorflow since tf.device('/cpu:0') not working?</p>
",
TensorFlow - get current value of a Variable,"<p>I'm starting out with TensorFlow (aren't we all) and have been reading the docs for the last couple of hours but could not figure out how to do this.</p>

<p>Concretely, suppose we have a variable:</p>

<pre><code>x = tf.Variable(...)
</code></pre>

<p>This variable can be updated during the training, which is done by .assign().</p>

<p>The question is, what is the best way to get the current value of a variable? I know we could use this:</p>

<pre><code>session.run(x)
</code></pre>

<p>But I'm afraid this would trigger a whole chain of operations and I couldn't find docs explaining the behavior here.</p>

<p>In Theano, you could just do</p>

<pre><code>y = theano.shared(...)
y_vals = y.get_value()
</code></pre>

<p>I'm looking for the equivalent thing in TensorFlow.</p>
",
Tensorflow One Hot Encoder?,"<p>Does tensorflow have something similar to scikit learn's <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html"">one hot encoder</a> for processing categorical data?  Would using a placeholder of tf.string behave as categorical data?</p>

<p>I realize I can manually pre-process the data before sending it to tensorflow, but having it built in is very convenient.</p>
",
"Keras, How to get the output of each layer?","<p>I have trained a binary classification model with CNN, and here is my code</p>

<pre><code>model = Sequential()
model.add(Convolution2D(nb_filters, kernel_size[0], kernel_size[1],
                        border_mode='valid',
                        input_shape=input_shape))
model.add(Activation('relu'))
model.add(Convolution2D(nb_filters, kernel_size[0], kernel_size[1]))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=pool_size))
# (16, 16, 32)
model.add(Convolution2D(nb_filters*2, kernel_size[0], kernel_size[1]))
model.add(Activation('relu'))
model.add(Convolution2D(nb_filters*2, kernel_size[0], kernel_size[1]))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=pool_size))
# (8, 8, 64) = (2048)
model.add(Flatten())
model.add(Dense(1024))
model.add(Activation('relu'))
model.add(Dropout(0.5))
model.add(Dense(2))  # define a binary classification problem
model.add(Activation('softmax'))

model.compile(loss='categorical_crossentropy',
              optimizer='adadelta',
              metrics=['accuracy'])
model.fit(x_train, y_train,
          batch_size=batch_size,
          nb_epoch=nb_epoch,
          verbose=1,
          validation_data=(x_test, y_test))
</code></pre>

<p>And here, I wanna get the output of each layer just like TensorFlow, how can I do that?</p>
",
Problems implementing an XOR gate with Neural Nets in Tensorflow,"<p>I want to make a trivial neural network, it should just implement the XOR gate. I am using the TensorFlow library, in python. 
For an XOR gate, the only data I train with, is the complete truth table, that should be enough right? Over optimization is what I will expect to happen very quickly. Problem with the code is that the <em>weights</em> and <em>biases</em> do not update. Somehow it still gives me 100% accuracy with zero for the biases and weights.</p>

<pre><code>x = tf.placeholder(""float"", [None, 2])
W = tf.Variable(tf.zeros([2,2]))
b = tf.Variable(tf.zeros([2]))

y = tf.nn.softmax(tf.matmul(x,W) + b)

y_ = tf.placeholder(""float"", [None,1])


print ""Done init""

cross_entropy = -tf.reduce_sum(y_*tf.log(y))
train_step = tf.train.GradientDescentOptimizer(0.75).minimize(cross_entropy)

print ""Done loading vars""

init = tf.initialize_all_variables()
print ""Done: Initializing variables""

sess = tf.Session()
sess.run(init)
print ""Done: Session started""

xTrain = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
yTrain = np.array([[1], [0], [0], [0]])


acc=0.0
while acc&lt;0.85:
  for i in range(500):
      sess.run(train_step, feed_dict={x: xTrain, y_: yTrain})


  print b.eval(sess)
  print W.eval(sess)


  print ""Done training""


  correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))

  accuracy = tf.reduce_mean(tf.cast(correct_prediction, ""float""))

  print ""Result:""
  acc= sess.run(accuracy, feed_dict={x: xTrain, y_: yTrain})
  print acc

B0 = b.eval(sess)[0]
B1 = b.eval(sess)[1]
W00 = W.eval(sess)[0][0]
W01 = W.eval(sess)[0][1]
W10 = W.eval(sess)[1][0]
W11 = W.eval(sess)[1][1]

for A,B in product([0,1],[0,1]):
  top = W00*A + W01*A + B0
  bottom = W10*B + W11*B + B1
  print ""A:"",A,"" B:"",B
  # print ""Top"",top,"" Bottom: "", bottom
  print ""Sum:"",top+bottom
</code></pre>

<p>I am following the tutorial from <a href=""http://tensorflow.org/tutorials/mnist/beginners/index.md#softmax_regressions"" rel=""noreferrer"">http://tensorflow.org/tutorials/mnist/beginners/index.md#softmax_regressions</a>
and in the final for-loop I am printing the results form the matrix(as described in the link).</p>

<p>Can anybody point out my error and what I should do to fix it?</p>
",
Tensorflow: How to replace a node in a calculation graph?,"<p>If you have two disjoint graphs, and want to link them, turning this:</p>

<pre><code>x = tf.placeholder('float')
y = f(x)

y = tf.placeholder('float')
z = f(y)
</code></pre>

<p>into this:</p>

<pre><code>x = tf.placeholder('float')
y = f(x)
z = g(y)
</code></pre>

<p>Is there a way to do that? It seems like it could make construction easier in some cases. </p>

<p>For example if you have a graph that has the input image as a <code>tf.placeholder</code>, and want to optimize the input image, deep-dream style, is there a way to just replace the placeholder with a <code>tf.variable</code> node? Or do you have to think of that before building the graph?</p>
",
Efficiently grab gradients from TensorFlow?,"<p>I'm trying to implement an asynchronous parameter server, DistBelief style using TensorFlow. I found that minimize() is split into two functions, compute_gradients and apply_gradients, so my plan is to insert a network boundary between them. I have a question about how to evaluate all the gradients simultaneously and pull them out all at once. I understand that eval only evaluates the subgraph necessary, but it also only returns one tensor, not the chain of tensors required to compute that tensor.</p>

<p>How can I do this more efficiently? I took the Deep MNIST example as a starting point:</p>

<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf
import download_mnist

def weight_variable(shape, name):
   initial = tf.truncated_normal(shape, stddev=0.1)
   return tf.Variable(initial, name=name)

def bias_variable(shape, name):
   initial = tf.constant(0.1, shape=shape)
   return tf.Variable(initial, name=name)

def conv2d(x, W):
   return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')

def max_pool_2x2(x):
   return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],
                         strides=[1, 2, 2, 1], padding='SAME')

mnist = download_mnist.read_data_sets('MNIST_data', one_hot=True)
session = tf.InteractiveSession()
x = tf.placeholder(""float"", shape=[None, 784], name='x')
x_image = tf.reshape(x, [-1,28,28,1], name='reshape')
y_ = tf.placeholder(""float"", shape=[None, 10], name='y_')
W_conv1 = weight_variable([5, 5, 1, 32], 'W_conv1')
b_conv1 = bias_variable([32], 'b_conv1')
h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)
h_pool1 = max_pool_2x2(h_conv1)
W_conv2 = weight_variable([5, 5, 32, 64], 'W_conv2')
b_conv2 = bias_variable([64], 'b_conv2')
h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)
h_pool2 = max_pool_2x2(h_conv2)
W_fc1 = weight_variable([7 * 7 * 64, 1024], 'W_fc1')
b_fc1 = bias_variable([1024], 'b_fc1')
h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])
h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)
keep_prob = tf.placeholder(""float"", name='keep_prob')
h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)
W_fc2 = weight_variable([1024, 10], 'W_fc2')
b_fc2 = bias_variable([10], 'b_fc2')
y_conv=tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)

loss = -tf.reduce_sum(y_ * tf.log(y_conv))
optimizer = tf.train.AdamOptimizer(1e-4)
correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))
accuracy = tf.reduce_mean(tf.cast(correct_prediction, ""float""))
compute_gradients = optimizer.compute_gradients(loss)
session.run(tf.initialize_all_variables())

batch = mnist.train.next_batch(50)
feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5}


gradients = []
for grad_var in compute_gradients:
    grad = grad_var[0].eval(feed_dict=feed_dict)
    var = grad_var[1]
    gradients.append((grad, var))
</code></pre>

<p>I think this last for loop is actually recalculating the last gradient several times, whereas the first gradient is computed only once? How can I grab all the gradients without recomputing them?</p>
",
Tensorflow: how to save/restore a model?,"<p>After you train a model in Tensorflow: </p>

<ol>
<li>How do you save the trained model?</li>
<li>How do you later restore this saved model?</li>
</ol>
",
Running TensorFlow on a Slurm Cluster?,"<p>I could get access to a computing cluster, specifically one node with two 12-Core CPUs, which is running with <a href=""https://en.wikipedia.org/wiki/Slurm_Workload_Manager"" rel=""noreferrer"">Slurm Workload Manager</a>.</p>

<p>I would like to run <a href=""https://en.wikipedia.org/wiki/TensorFlow"" rel=""noreferrer"">TensorFlow</a> on that system but unfortunately I were not able to find any information about how to do this or if this is even possible. I am new to this but as far as I understand it, I would have to run TensorFlow by creating a Slurm job and can not directly execute python/tensorflow via ssh. </p>

<p>Has anyone an idea, tutorial or any kind of source on this topic?</p>
",
Does TensorFlow by default use all available GPUs in the machine?,"<p>I have 3 GTX Titan GPUs in my machine. I run the example provided in Cifar10 with cifar10_train.py and got the following output:</p>

<pre><code>I tensorflow/core/common_runtime/gpu/gpu_init.cc:60] cannot enable peer access from device ordinal 0 to device ordinal 1
I tensorflow/core/common_runtime/gpu/gpu_init.cc:60] cannot enable peer access from device ordinal 1 to device ordinal 0
I tensorflow/core/common_runtime/gpu/gpu_init.cc:127] DMA: 0 1 
I tensorflow/core/common_runtime/gpu/gpu_init.cc:137] 0:   Y N 
I tensorflow/core/common_runtime/gpu/gpu_init.cc:137] 1:   N Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:694] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: GeForce GTX TITAN, pci bus id: 0000:03:00.0)
I tensorflow/core/common_runtime/gpu/gpu_device.cc:694] Creating TensorFlow device (/gpu:1) -&gt; (device: 1, name: GeForce GTX TITAN, pci bus id: 0000:84:00.0)
</code></pre>

<p>It looks to me that TensorFlow is trying to initialize itself on two devices (gpu0 and gpu1). </p>

<p>My question is why it only does that on two devices and is there any way to prevent that? (I only want it to run as if there is a single GPU)</p>
",
Tensorflow: Where is tf.nn.conv2d Actually Executed?,"<p>I am curious about the Tensorflow implementation of <code>tf.nn.conv2d(...)</code>. To call it, one simply runs <code>tf.nn.conv2d(...)</code>. However, I'm going down the rabbit hole trying to see where it is executed. The code is as follows (where the arrow indicates the function it ultimately calls):</p>

<pre><code>tf.nn.conv2d(...) -&gt; tf.nn_ops.conv2d(...) -&gt; tf.gen_nn_ops.conv2d(...) -&gt; _op_def_lib.apply_op(""Conv2D"", ...) -&gt; ?
</code></pre>

<p>I am familiar with Tensorflow's implementation of LSTMs and the ability to easily manipulate them as one deems fit. Is the function that performs the <code>conv2d()</code> calculation written in Python, and if so, where is it? Can I see where and how the strides are executed?</p>
",
What does `tf.strided_slice()` do?,"<p>I am wondering what <code>tf.strided_slice()</code> operator actually does.<br>
The <a href=""https://www.tensorflow.org/api_docs/python/array_ops/slicing_and_joining#strided_slice"" rel=""noreferrer"">doc</a> says,</p>

<blockquote>
  <p>To a first order, this operation extracts a slice of size end - begin from a tensor input starting at the location specified by begin. The slice continues by adding stride to the begin index until all dimensions are not less than end. Note that components of stride can be negative, which causes a reverse slice.</p>
</blockquote>

<p>And in the sample, </p>

<pre><code># 'input' is [[[1, 1, 1], [2, 2, 2]],
#             [[3, 3, 3], [4, 4, 4]],
#             [[5, 5, 5], [6, 6, 6]]]
tf.slice(input, [1, 0, 0], [2, 1, 3], [1, 1, 1]) ==&gt; [[[3, 3, 3]]]
tf.slice(input, [1, 0, 0], [2, 2, 3], [1, 1, 1]) ==&gt; [[[3, 3, 3],
                                                       [4, 4, 4]]]
tf.slice(input, [1, 1, 0], [2, -1, 3], [1, -1, 1]) ==&gt;[[[4, 4, 4],
                                                        [3, 3, 3]]]
</code></pre>

<p>So in my understanding of the doc, the first sample (<code>tf.slice(input, begin=[1, 0, 0], end=[2, 1, 3], strides=[1, 1, 1])</code>),</p>

<ul>
<li>resulting size is <code>end - begin = [1, 1, 3]</code>. The sample result shows <code>[[[3, 3, 3,]]]</code>, that shape is <code>[1, 1, 3]</code>, it seems OK.</li>
<li>the first element of the result is at <code>begin = [1, 0, 0]</code>. The first element of the sample result is <code>3</code>, which is <code>input[1,0,0]</code>, it seems OK.</li>
<li>the slice continues by adding stride to the begin index. So the second element of the result should be <code>input[begin + strides] = input[2, 1, 1] = 6</code>, but the sample shows the second element is <code>3</code>.</li>
</ul>

<p>What <code>strided_slice()</code> does?</p>

<p>(Note: <a href=""https://github.com/tensorflow/tensorflow/issues/5761"" rel=""noreferrer"">method names in the samples and the last example is incorrect</a>.)</p>
",
How to feed a placeholder?,"<p>I am trying to implement a simple feed forward network. However, I can't figure out how to feed a <code>Placeholder</code>. This example:</p>

<pre><code>import tensorflow as tf

num_input  = 2
num_hidden = 3
num_output = 2

x  = tf.placeholder(""float"", [num_input, 1])
W_hidden = tf.Variable(tf.zeros([num_hidden, num_input]))
W_out    = tf.Variable(tf.zeros([num_output, num_hidden]))
b_hidden = tf.Variable(tf.zeros([num_hidden]))
b_out    = tf.Variable(tf.zeros([num_output]))

h = tf.nn.softmax(tf.matmul(W_hidden,x) + b_hidden)

sess = tf.Session()

with sess.as_default():
    print h.eval()
</code></pre>

<p>Gives me the following error:</p>

<pre><code>  ...
    results = self._do_run(target_list, unique_fetch_targets, feed_dict_string)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 419, in _do_run
    e.code)
tensorflow.python.framework.errors.InvalidArgumentError: You must feed a value for placeholder tensor 'Placeholder' with dtype float and shape dim { size: 2 } dim { size: 1 }
     [[Node: Placeholder = Placeholder[dtype=DT_FLOAT, shape=[2,1], _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]
Caused by op u'Placeholder', defined at:
  File ""/home/sfalk/workspace/SemEval2016/java/semeval2016-python/slot1_tf.py"", line 8, in &lt;module&gt;
    x  = tf.placeholder(""float"", [num_input, 1])
  ...
</code></pre>

<p>I have tried</p>

<pre><code>tf.assign([tf.Variable(1.0), tf.Variable(1.0)], x)
tf.assign([1.0, 1.0], x)
</code></pre>

<p>but that does not work apparently.</p>
",
What are c_state and m_state in Tensorflow LSTM?,"<p>Tensorflow r0.12's documentation for tf.nn.rnn_cell.LSTMCell describes this as the init:</p>

<pre><code>tf.nn.rnn_cell.LSTMCell.__call__(inputs, state, scope=None)
</code></pre>

<p>where <code>state</code> is as follows:</p>

<blockquote>
  <p>state: if state_is_tuple is False, this must be a state Tensor, 2-D, batch x state_size. If state_is_tuple is True, this must be a tuple of state Tensors, both 2-D, with column sizes c_state and m_state.</p>
</blockquote>

<p>What aare <code>c_state</code> and <code>m_state</code> and how do they fit into LSTMs? I cannot find reference to them anywhere in the documentation.</p>

<p><a href=""https://web.archive.org/web/20170223030652/https://www.tensorflow.org/versions/r0.11/api_docs/python/rnn_cell/rnn_cells_for_use_with_tensorflow_s_core_rnn_methods"" rel=""nofollow noreferrer"">Here is a link to that page in the documentation.</a></p>
",
What's the difference between tf.Session() and tf.InteractiveSession()?,"<p>In which cases should <code>tf.Session()</code> and <code>tf.InteractiveSession()</code> be considered for what purpose?</p>

<p>When I tried to use the former one, some functions (for example, <code>.eval()</code>) didn't work, and when I changed to the later one, it worked.</p>
",
Restoring TensorFlow model,"<p>I'm trying to restore TensorFlow model. I followed this example:
<a href=""http://nasdag.github.io/blog/2016/01/19/classifying-bees-with-google-tensorflow/"" rel=""noreferrer"">http://nasdag.github.io/blog/2016/01/19/classifying-bees-with-google-tensorflow/</a></p>

<p>At the end of the code in the example I added these lines:</p>

<pre><code>saver = tf.train.Saver()
save_path = saver.save(sess, ""model.ckpt"")
print(""Model saved in file: %s"" % save_path)
</code></pre>

<p>Two files were created: checkpoint and model.ckpt.</p>

<p>In a new python file (tomas_bees_predict.py), I have this code:</p>

<pre><code>import tensorflow as tf

saver = tf.train.Saver()

with tf.Session() as sess:
  # Restore variables from disk.
  saver.restore(sess, ""model.ckpt"")
  print(""Model restored."")
</code></pre>

<p>However when I execute the code, I get this error:</p>

<pre><code>Traceback (most recent call last):
  File ""tomas_bees_predict.py"", line 3, in &lt;module&gt;
    saver = tf.train.Saver()
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 705, in __init__
raise ValueError(""No variables to save"")
</code></pre>

<p>ValueError: No variables to save</p>

<p>Is there a way to read mode.ckpt file and see what variables are saved?
Or maybe someone can help with saving the model and restoring it based on the example described above? </p>

<p><strong>EDIT 1:</strong></p>

<p>I think I tried running the same code in order to recreate model structure and I was getting the error. I think it could be related to the fact that code described here isn't using named variables:
<a href=""http://nasdag.github.io/blog/2016/01/19/classifying-bees-with-google-tensorflow/"" rel=""noreferrer"">http://nasdag.github.io/blog/2016/01/19/classifying-bees-with-google-tensorflow/</a></p>

<pre><code>def weight_variable(shape):
  initial = tf.truncated_normal(shape, stddev=0.1)
  return tf.Variable(initial)

def bias_variable(shape):
  initial = tf.constant(0.1, shape=shape)
  return tf.Variable(initial)
</code></pre>

<p>So I did this experiment. I wrote two versions of the code (with and without named variables) to save the model and the code to restore the model.</p>

<p><strong>tensor_save_named_vars.py</strong>:</p>

<pre><code>import tensorflow as tf

# Create some variables.
v1 = tf.Variable(1, name=""v1"")
v2 = tf.Variable(2, name=""v2"")

# Add an op to initialize the variables.
init_op = tf.initialize_all_variables()

# Add ops to save and restore all the variables.
saver = tf.train.Saver()

# Later, launch the model, initialize the variables, do some work, save the
# variables to disk.
with tf.Session() as sess:
  sess.run(init_op)
  print ""v1 = "", v1.eval()
  print ""v2 = "", v2.eval()
  # Save the variables to disk.
  save_path = saver.save(sess, ""/tmp/model.ckpt"")
  print ""Model saved in file: "", save_path
</code></pre>

<p><strong>tensor_save_not_named_vars.py:</strong></p>

<pre><code>import tensorflow as tf

# Create some variables.
v1 = tf.Variable(1)
v2 = tf.Variable(2)

# Add an op to initialize the variables.
init_op = tf.initialize_all_variables()

# Add ops to save and restore all the variables.
saver = tf.train.Saver()

# Later, launch the model, initialize the variables, do some work, save the
# variables to disk.
with tf.Session() as sess:
  sess.run(init_op)
  print ""v1 = "", v1.eval()
  print ""v2 = "", v2.eval()
  # Save the variables to disk.
  save_path = saver.save(sess, ""/tmp/model.ckpt"")
  print ""Model saved in file: "", save_path
</code></pre>

<p><strong>tensor_restore.py:</strong></p>

<pre><code>import tensorflow as tf

# Create some variables.
v1 = tf.Variable(0, name=""v1"")
v2 = tf.Variable(0, name=""v2"")

# Add ops to save and restore all the variables.
saver = tf.train.Saver()

# Later, launch the model, use the saver to restore variables from disk, and
# do some work with the model.
with tf.Session() as sess:
  # Restore variables from disk.
  saver.restore(sess, ""/tmp/model.ckpt"")
  print ""Model restored.""
  print ""v1 = "", v1.eval()
  print ""v2 = "", v2.eval()
</code></pre>

<p>Here is what I get when I execute this code:</p>

<pre><code>$ python tensor_save_named_vars.py 

I tensorflow/core/common_runtime/local_device.cc:40] Local device intra op parallelism threads: 4
I tensorflow/core/common_runtime/direct_session.cc:58] Direct session inter op parallelism threads: 4
v1 =  1
v2 =  2
Model saved in file:  /tmp/model.ckpt

$ python tensor_restore.py 

I tensorflow/core/common_runtime/local_device.cc:40] Local device intra op parallelism threads: 4
I tensorflow/core/common_runtime/direct_session.cc:58] Direct session inter op parallelism threads: 4
Model restored.
v1 =  1
v2 =  2

$ python tensor_save_not_named_vars.py 

I tensorflow/core/common_runtime/local_device.cc:40] Local device intra op parallelism threads: 4
I tensorflow/core/common_runtime/direct_session.cc:58] Direct session inter op parallelism threads: 4
v1 =  1
v2 =  2
Model saved in file:  /tmp/model.ckpt

$ python tensor_restore.py 
I tensorflow/core/common_runtime/local_device.cc:40] Local device intra op parallelism threads: 4
I tensorflow/core/common_runtime/direct_session.cc:58] Direct session inter op parallelism threads: 4
W tensorflow/core/common_runtime/executor.cc:1076] 0x7ff953881e40 Compute status: Not found: Tensor name ""v2"" not found in checkpoint files /tmp/model.ckpt
     [[Node: save/restore_slice_1 = RestoreSlice[dt=DT_INT32, preferred_shard=-1, _device=""/job:localhost/replica:0/task:0/cpu:0""](_recv_save/Const_0, save/restore_slice_1/tensor_name, save/restore_slice_1/shape_and_slice)]]
W tensorflow/core/common_runtime/executor.cc:1076] 0x7ff953881e40 Compute status: Not found: Tensor name ""v1"" not found in checkpoint files /tmp/model.ckpt
     [[Node: save/restore_slice = RestoreSlice[dt=DT_INT32, preferred_shard=-1, _device=""/job:localhost/replica:0/task:0/cpu:0""](_recv_save/Const_0, save/restore_slice/tensor_name, save/restore_slice/shape_and_slice)]]
Traceback (most recent call last):
  File ""tensor_restore.py"", line 14, in &lt;module&gt;
    saver.restore(sess, ""/tmp/model.ckpt"")
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 891, in restore
    sess.run([self._restore_op_name], {self._filename_tensor_name: save_path})
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 368, in run
    results = self._do_run(target_list, unique_fetch_targets, feed_dict_string)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 444, in _do_run
    e.code)
tensorflow.python.framework.errors.NotFoundError: Tensor name ""v2"" not found in checkpoint files /tmp/model.ckpt
     [[Node: save/restore_slice_1 = RestoreSlice[dt=DT_INT32, preferred_shard=-1, _device=""/job:localhost/replica:0/task:0/cpu:0""](_recv_save/Const_0, save/restore_slice_1/tensor_name, save/restore_slice_1/shape_and_slice)]]
Caused by op u'save/restore_slice_1', defined at:
  File ""tensor_restore.py"", line 8, in &lt;module&gt;
    saver = tf.train.Saver()
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 713, in __init__
    restore_sequentially=restore_sequentially)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 432, in build
    filename_tensor, vars_to_save, restore_sequentially, reshape)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 191, in _AddRestoreOps
    values = self.restore_op(filename_tensor, vs, preferred_shard)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 106, in restore_op
    preferred_shard=preferred_shard)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/io_ops.py"", line 189, in _restore_slice
    preferred_shard, name=name)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_io_ops.py"", line 271, in _restore_slice
    preferred_shard=preferred_shard, name=name)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/op_def_library.py"", line 664, in apply_op
    op_def=op_def)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1834, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1043, in __init__
    self._traceback = _extract_stack()
</code></pre>

<p>So perhaps the original code (see the external link above) could be modified to something like this:</p>

<pre><code>def weight_variable(shape):
  initial = tf.truncated_normal(shape, stddev=0.1)
  weight_var = tf.Variable(initial, name=""weight_var"")
  return weight_var

def bias_variable(shape):
  initial = tf.constant(0.1, shape=shape)
  bias_var = tf.Variable(initial, name=""bias_var"")
  return bias_var
</code></pre>

<p>But then the question I have: is restoring weight_var and bias_var variables sufficient to implement the prediction? I did the training on the powerful machine with GPU and I would like to copy the model to the less powerful computer without GPU to run predictions.</p>
",
How to set adaptive learning rate for GradientDescentOptimizer?,"<p>I am using TensorFlow to train a neural network. This is how I am initializing the <code>GradientDescentOptimizer</code>:</p>

<pre><code>init = tf.initialize_all_variables()
sess = tf.Session()
sess.run(init)

mse        = tf.reduce_mean(tf.square(out - out_))
train_step = tf.train.GradientDescentOptimizer(0.3).minimize(mse)
</code></pre>

<p>The thing here is that I don't know how to set an update rule for the learning rate or a decay value for that. </p>

<p>How can I use an adaptive learning rate here?</p>
",
TensorFlow: Max of a tensor along an axis,"<p>My question is in two connected parts:</p>

<ol>
<li><p>How do I calculate the max along a certain axis of a tensor? For example, if I have</p>

<pre><code>x = tf.constant([[1,220,55],[4,3,-1]])
</code></pre>

<p>I want something like </p>

<pre><code>x_max = tf.max(x, axis=1)
print sess.run(x_max)

output: [220,4]
</code></pre>

<p>I know there is a <code>tf.argmax</code> and a <code>tf.maximum</code>, but neither give the maximum value along an axis of a single tensor. For now I have a workaround:</p>

<pre><code>x_max = tf.slice(x, begin=[0,0], size=[-1,1])
for a in range(1,2):
    x_max = tf.maximum(x_max , tf.slice(x, begin=[0,a], size=[-1,1]))
</code></pre>

<p>But it looks less than optimal. Is there a better way to do this?</p></li>
<li><p>Given the indices of an <code>argmax</code> of a tensor, how do I index into another tensor using those indices? Using the example of <code>x</code> above, how do I do something like the following:</p>

<pre><code>ind_max = tf.argmax(x, dimension=1)    #output is [1,0]
y = tf.constant([[1,2,3], [6,5,4])
y_ = y[:, ind_max]                     #y_ should be [2,6]
</code></pre>

<p>I know slicing, like the last line, does not exist in TensorFlow yet (<a href=""https://github.com/tensorflow/tensorflow/issues/206"" rel=""nofollow noreferrer"">#206</a>). </p>

<p>My question is: <em>what is the best workaround for my specific case (maybe using other methods like gather, select, etc.)?</em></p>

<p>Additional information: I know <code>x</code> and <code>y</code> are going to be two dimensional tensors only!</p></li>
</ol>
",
Wide & Deep learning for large data error: GraphDef cannot be larger than 2GB,"<p>Inserting 1MM+ rows into the <a href=""https://www.tensorflow.org/tutorials/wide_and_deep/"" rel=""noreferrer"">wide and deep learning model</a> throws <code>ValueError: GraphDef cannot be larger than 2GB</code>:</p>

<pre><code>Traceback (most recent call last):
  File ""search_click.py"", line 207, in &lt;module&gt;
    tf.app.run()
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 30, in run
    sys.exit(main(sys.argv))
  File ""search_click.py"", line 204, in main
    train_and_eval()
  File ""search_click.py"", line 181, in train_and_eval
    m.fit(input_fn=lambda: input_fn(df_train), steps=FLAGS.train_steps)
  File ""/usr/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py"", line 182, in fit
    monitors=monitors)
  File ""/usr/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py"", line 458, in _train_model
    summary_writer=graph_actions.get_summary_writer(self._model_dir))
  File ""/usr/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/graph_actions.py"", line 76, in get_summary_writer
    graph=ops.get_default_graph())
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/training/summary_io.py"", line 113, in __init__
    self.add_graph(graph=graph, graph_def=graph_def)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/training/summary_io.py"", line 204, in add_graph
    true_graph_def = graph.as_graph_def(add_shapes=True)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 2117, in as_graph_def
    raise ValueError(""GraphDef cannot be larger than 2GB."")
ValueError: GraphDef cannot be larger than 2GB.
</code></pre>

<p>I defined the same <code>input_fn</code> as in the example:</p>

<pre><code>def input_fn(df):
  """"""Input builder function.""""""
  # Creates a dictionary mapping from each continuous feature column name (k) to
  # the values of that column stored in a constant Tensor.
  continuous_cols = {k: tf.constant(df[k].values) for k in CONTINUOUS_COLUMNS}
  # Creates a dictionary mapping from each categorical feature column name (k)
  # to the values of that column stored in a tf.SparseTensor.
  categorical_cols = {k: tf.SparseTensor(
      indices=[[i, 0] for i in range(df[k].size)],
      values=df[k].values,
      shape=[df[k].size, 1])
                      for k in CATEGORICAL_COLUMNS}
  # Merges the two dictionaries into one.
  feature_cols = dict(continuous_cols)
  feature_cols.update(categorical_cols)
  # Converts the label column into a constant Tensor.
  label = tf.constant(df[LABEL_COLUMN].values)
  # Returns the feature columns and the label.
  return feature_cols, label
</code></pre>

<p>Is there a replacement to <code>tf.constant</code> and <code>tf.SparseTensor</code> that allows for inserting in batches and avoiding the memory error?</p>
",
What are the differences between tf.initialize_all_variables() and tf.global_variables_initializer(),"<p>On Tensorflow official website, it gives explantions of the <code>tf.initialize_all_variables()</code> and <code>tf.global_variables_initializer()</code> functions as follow  </p>

<blockquote>
  <h3>tf.initialize_all_variables():</h3>
  
  <p>Returns an op that initializes all variables.</p>
  
  <h3>tf.global_variables_initializer():</h3>
  
  <p>Adds an op to initialize all variables in the model</p>
</blockquote>

<p>It seems like both can be used to initialize all variables in graphs. Can we use these two functions exchangbly? If not, what would be the differences? </p>
",
How can I use Tensorflow with react-native?,"<p>I've looked around the web and cant seem to find a way to use <a href=""https://facebook.github.io/react-native/"" rel=""nofollow noreferrer"">react native</a> with tensorflow.</p>

<p>I dont think TF supports react-native (at least not officially) integration but i hope someone in the community has found a way.</p>

<p>How can one use tensorflow in a react-native project?</p>

<p>Thanks.</p>
",
"How to train images, when they have different size ?","<p>I am trying train my model which classifies images. 
The problem i have is, they have different sizes. Is there any possibilty to train those images without resizing them. </p>
",
What's the purpose of tf.app.flags in TensorFlow?,"<p>I am reading some example codes in Tensorflow, I found following code </p>

<pre><code>flags = tf.app.flags
FLAGS = flags.FLAGS
flags.DEFINE_float('learning_rate', 0.01, 'Initial learning rate.')
flags.DEFINE_integer('max_steps', 2000, 'Number of steps to run trainer.')
flags.DEFINE_integer('hidden1', 128, 'Number of units in hidden layer 1.')
flags.DEFINE_integer('hidden2', 32, 'Number of units in hidden layer 2.')
flags.DEFINE_integer('batch_size', 100, 'Batch size.  '
                 'Must divide evenly into the dataset sizes.')
flags.DEFINE_string('train_dir', 'data', 'Directory to put the training data.')
flags.DEFINE_boolean('fake_data', False, 'If true, uses fake data '
                 'for unit testing.')
</code></pre>

<p>in <code>tensorflow/tensorflow/g3doc/tutorials/mnist/fully_connected_feed.py</code></p>

<p>But I can't find any docs about this usage of <code>tf.app.flags</code>. </p>

<p>And I found the implementation of this flags is in the 
<a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/platform/default/_flags.py""><code>tensorflow/tensorflow/python/platform/default/_flags.py</code></a></p>

<p>Obviously, this <code>tf.app.flags</code> is somehow used to configure a network, so why  is it not in the API docs? Can anyone explain what is going on here? </p>
",
Example for Deploying a Tensorflow Model via a RESTful API,"<p>Is there any example code for deploying a Tensorflow Model via a RESTful API? I see examples for a command line program and for a mobile app. Is there a framework for this or people just load the model and expose the predict method via a web framework (like Flask)to take input (say via JSON) and return the response? By framework I mean scaling for large number of predict requests. Of course since the models are immutable we can launch multiple instances of our prediction server and put it behind a load balancer (like HAProxy). My question is, are people using some framework for this or doing this from scratch, or, maybe this is already available in Tensorflow and I have not noticed it.</p>
",
Is Tensorflow compatible with a Windows workflow?,"<p>I haven't seen anything about Windows compatibility -- is this on the way or currently available somewhere if I put forth some effort? (I have a Mac and an Ubuntu box but the Windows machine is the one with the discrete graphics card that I currently use with theano).</p>
",
How do I use distributed DNN training in TensorFlow?,"<p>Google released TensorFlow today.</p>

<p>I have been poking around in the code, and I don't see anything in the code or API about training across a cluster of GPU servers.</p>

<p>Does it have distributed training functionality yet?</p>
",
"Where is the folder for Installing tensorflow with pip, Mac OSX?","<p>just installed tensorflow using pip with the command:</p>

<p><code>$ pip install tensorflow</code></p>

<p>On the <a href=""http://tensorflow.org/get_started/os_setup.md"">""Getting Started"" for Tensorflow</a> they have an example for convolutional neural networks</p>

<p><code>$ python tensorflow/models/image/mnist/convolutional.py</code></p>

<p>Where is that directory located when installing with pip?</p>
",
Training custom dataset with translate model,"<p>Running the model out of the box generates these files in the data dir : </p>

<pre><code>ls
dev-v2.tgz                            newstest2013.en
giga-fren.release2.fixed.en           newstest2013.en.ids40000
giga-fren.release2.fixed.en.gz        newstest2013.fr
giga-fren.release2.fixed.en.ids40000  newstest2013.fr.ids40000
giga-fren.release2.fixed.fr           training-giga-fren.tar
giga-fren.release2.fixed.fr.gz        vocab40000.from
giga-fren.release2.fixed.fr.ids40000  vocab40000.to
</code></pre>

<p>Reading the src of translate.py : </p>

<p><a href=""https://github.com/tensorflow/models/blob/master/tutorials/rnn/translate/translate.py"" rel=""noreferrer"">https://github.com/tensorflow/models/blob/master/tutorials/rnn/translate/translate.py</a></p>

<pre><code>tf.app.flags.DEFINE_string(""from_train_data"", None, ""Training data."")
tf.app.flags.DEFINE_string(""to_train_data"", None, ""Training data."")
</code></pre>

<p>To utilize my own training data I created dirs my-from-train-data &amp; to-from-train-data and add my own training data to each of these dirs, training data is contained in the files mydata.from &amp; mydata.to</p>

<pre><code>my-to-train-data contains mydata.from
my-from-train-data contains mydata.to
</code></pre>

<p>I could not find documentation as to using own training data or what format it should take so I inferred this from the translate.py src and contents of data dir created when executing translate model out of the box.</p>

<p>Contents of mydata.from : </p>

<pre><code> Is this a question
</code></pre>

<p>Contents of mydata.to : </p>

<pre><code> Yes!
</code></pre>

<p>I then attempt to train the model using : </p>

<pre><code>python translate.py --from_train_data my-from-train-data --to_train_data my-to-train-data
</code></pre>

<p>This returns with an error : </p>

<pre><code>tensorflow.python.framework.errors_impl.NotFoundError: my-from-train-data.ids40000
</code></pre>

<p>Appears I need to create file my-from-train-data.ids40000 , what should it's contents be ? Is there an example of how to train this model using custom data ?</p>
",
"On Windows, running ""import tensorflow"" generates No module named ""_pywrap_tensorflow"" error","<p>On Windows, TensorFlow reports either or both of the following errors after executing an <code>import tensorflow</code> statement:</p>

<ul>
<li><code>No module named ""_pywrap_tensorflow""</code></li>
<li><code>DLL load failed.</code></li>
</ul>
",
TensorFlow (Mac OS X): can't determine number of CPU cores:,"<p>There must be a simple setting for Mac OS X, to get rid of the following warning...something in .bash_profile?</p>

<pre><code>&gt;&gt;&gt; import tensorflow as tf
&gt;&gt;&gt; sess = tf.Session()
can't determine number of CPU cores: assuming 4
I tensorflow/core/common_runtime/local_device.cc:25] Local device intra op parallelism threads: 4
</code></pre>
",
"After building TensorFlow from source, seeing libcudart.so and libcudnn errors","<p>I'm building TensorFlow from source code.  The build appears to succeed; however, when my TensorFlow program invokes <code>import tensorflow</code>, one or both of the following errors appear:</p>

<ul>
<li><code>ImportError: libcudart.so.8.0: cannot open shared object file: No such file or directory</code></li>
<li><code>ImportError: libcudnn.5: cannot open shared object file: No such file or directory</code></li>
</ul>
",
Debugging nans in the backward pass,"<p>I'm trying to debug a somewhat complicated and non-canonical NN architecture. Computing the forward pass is fine and is giving me the expected results, but when I try to optimize using Adam or any of the standard optimizers, even after one iteration with a very small learning rate I get nans everywhere. I'm trying to localize them and was wondering if there's a way to catch the first occurrence of a nan and detect in which op it arose? I tried <code>tf.add_check_numerics_ops()</code> but it doesn't appear to be doing anything, or perhaps I'm using it incorrectly.</p>
",
How to build and use Google TensorFlow C++ api,"<p>I'm really eager to start using Google's new Tensorflow library in C++. The website and docs are just really unclear in terms of how to build the project's C++ API and I don't know where to start. </p>

<p>Can someone with more experience help by discovering and sharing a guide to using tensorflow's C++ API?  </p>
",
Tensorflow successfully installs on mac but gets ImportError on copyreg when used,"<p>After successfully <code>pip install</code>, importing the tensorflow library fails.</p>

<pre><code>&gt;&gt;&gt; import tensorflow
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""/Library/Python/2.7/site-packages/tensorflow/__init__.py"", line 4, in &lt;module&gt;
    from tensorflow.python import *
  File ""/Library/Python/2.7/site-packages/tensorflow/python/__init__.py"", line 13, in &lt;module&gt;
    from tensorflow.core.framework.graph_pb2 import *
  File ""/Library/Python/2.7/site-packages/tensorflow/core/framework/graph_pb2.py"", line 8, in &lt;module&gt;
    from google.protobuf import reflection as _reflection
  File ""/Library/Python/2.7/site-packages/google/protobuf/reflection.py"", line 58, in &lt;module&gt;
    from google.protobuf.internal import python_message as message_impl
  File ""/Library/Python/2.7/site-packages/google/protobuf/internal/python_message.py"", line 59, in &lt;module&gt;
    import six.moves.copyreg as copyreg
ImportError: No module named copyreg
</code></pre>
",
How would I implement k-means with TensorFlow?,"<p>The intro tutorial, which uses the built-in gradient descent optimizer, makes a lot of sense. However, k-means isn't just something I can plug into gradient descent. It seems like I'd have to write my own sort of optimizer, but I'm not quite sure how to do that given the TensorFlow primitives.</p>

<p>What approach should I take?</p>
",
TensorFlow random_shuffle_queue is closed and has insufficient elements,"<p>I'm reading batch of images by getting idea <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/how_tos/reading_data/fully_connected_reader.py"">here</a> from tfrecords(converted by <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/how_tos/reading_data/convert_to_records.py"">this</a>) </p>

<p>My images are cifar images, [32, 32, 3] and as you can see while reading and passing images the shapes are normal (<code>batch_size=100</code>)</p>

<p>the 2 most notable problems stated in the log, as far as I know is</p>

<ol>
<li>Shape of 12228, which I don't know from where I get this. All my tensors are either in shape [32, 32, 3] or [None, 3072]</li>
<li>Running out of sample</li>
</ol>

<p><code>Compute status: Out of range: RandomSuffleQueue '_2_input/shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 100, current size 0)</code></p>

<p>How can I solve this?</p>

<p>Logs:</p>

<pre><code>1- image shape is  TensorShape([Dimension(3072)])
1.1- images batch shape is  TensorShape([Dimension(100), Dimension(3072)])
2- images shape is  TensorShape([Dimension(100), Dimension(3072)])

W tensorflow/core/kernels/queue_ops.cc:79] Invalid argument: Shape mismatch in tuple component 0. Expected [3072], got [12288]
W tensorflow/core/common_runtime/executor.cc:1027] 0x7fa72abc89a0 Compute status: Invalid argument: Shape mismatch in tuple component 0. Expected [3072], got [12288]
     [[Node: input/shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueue[Tcomponents=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device=""/job:localhost/replica:0/task:0/cpu:0""](input/shuffle_batch/random_shuffle_queue, input/sub, input/Cast_1)]]
W tensorflow/core/kernels/queue_ops.cc:79] Invalid argument: Shape mismatch in tuple component 0. Expected [3072], got [12288]
W tensorflow/core/common_runtime/executor.cc:1027] 0x7fa72ab9d080 Compute status: Invalid argument: Shape mismatch in tuple component 0. Expected [3072], got [12288]
     [[Node: input/shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueue[Tcomponents=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device=""/job:localhost/replica:0/task:0/cpu:0""](input/shuffle_batch/random_shuffle_queue, input/sub, input/Cast_1)]]
W tensorflow/core/kernels/queue_ops.cc:79] Invalid argument: Shape mismatch in tuple component 0. Expected [3072], got [12288]
W tensorflow/core/common_runtime/executor.cc:1027] 0x7fa7285e55a0 Compute status: Invalid argument: Shape mismatch in tuple component 0. Expected [3072], got [12288]
     [[Node: input/shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueue[Tcomponents=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device=""/job:localhost/replica:0/task:0/cpu:0""](input/shuffle_batch/random_shuffle_queue, input/sub, input/Cast_1)]]
W tensorflow/core/kernels/queue_ops.cc:79] Invalid argument: Shape mismatch in tuple component 0. Expected [3072], got [12288]
W tensorflow/core/common_runtime/executor.cc:1027] 0x7fa72aadb080 Compute status: Invalid argument: Shape mismatch in tuple component 0. Expected [3072], got [12288]
     [[Node: input/shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueue[Tcomponents=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device=""/job:localhost/replica:0/task:0/cpu:0""](input/shuffle_batch/random_shuffle_queue, input/sub, input/Cast_1)]]
W tensorflow/core/common_runtime/executor.cc:1027] 0x7fa72ad499a0 Compute status: Out of range: RandomSuffleQueue '_2_input/shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 100, current size 0)
     [[Node: input/shuffle_batch = QueueDequeueMany[component_types=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device=""/job:localhost/replica:0/task:0/cpu:0""](input/shuffle_batch/random_shuffle_queue, input/shuffle_batch/n)]]
Traceback (most recent call last):
  File ""/Users/HANEL/Documents/my_cifar_train.py"", line 110, in &lt;module&gt;
    tf.app.run()
  File ""/Users/HANEL/tensorflow/lib/python2.7/site-packages/tensorflow/python/platform/default/_app.py"", line 11, in run
    sys.exit(main(sys.argv))
  File ""/Users/HANEL/my_cifar_train.py"", line 107, in main
    train()
  File ""/Users/HANEL/my_cifar_train.py"", line 76, in train
    _, loss_value = sess.run([train_op, loss])
  File ""/Users/HANEL/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 345, in run
    results = self._do_run(target_list, unique_fetch_targets, feed_dict_string)
  File ""/Users/HANEL/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 419, in _do_run
    e.code)
tensorflow.python.framework.errors.OutOfRangeError: RandomSuffleQueue '_2_input/shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 100, current size 0)
     [[Node: input/shuffle_batch = QueueDequeueMany[component_types=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device=""/job:localhost/replica:0/task:0/cpu:0""](input/shuffle_batch/random_shuffle_queue, input/shuffle_batch/n)]]
Caused by op u'input/shuffle_batch', defined at:
  File ""/Users/HANEL/my_cifar_train.py"", line 110, in &lt;module&gt;
    tf.app.run()
  File ""/Users/HANEL/tensorflow/lib/python2.7/site-packages/tensorflow/python/platform/default/_app.py"", line 11, in run
    sys.exit(main(sys.argv))
  File ""/Users/HANEL/my_cifar_train.py"", line 107, in main
    train()
  File ""/Users/HANEL/my_cifar_train.py"", line 39, in train
    images, labels = my_input.inputs()
  File ""/Users/HANEL/my_input.py"", line 157, in inputs
    min_after_dequeue=200)
  File ""/Users/HANEL/tensorflow/lib/python2.7/site-packages/tensorflow/python/training/input.py"", line 453, in shuffle_batch
    return queue.dequeue_many(batch_size, name=name)
  File ""/Users/HANEL/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py"", line 245, in dequeue_many
    self._queue_ref, n, self._dtypes, name=name)
  File ""/Users/HANEL/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py"", line 319, in _queue_dequeue_many
    timeout_ms=timeout_ms, name=name)
  File ""/Users/HANEL/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/op_def_library.py"", line 633, in apply_op
    op_def=op_def)
  File ""/Users
/HANEL/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1710, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/Users/HANEL/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 988, in __init__
    self._traceback =

_extract_stack()
</code></pre>
",
Tensorflow installation error: not a supported wheel on this platform,"<p>when I try to install tensorflow by cloning from git, I run into the error ""no module named copyreg,"" so I tried installing using a virtualenv. However, I then run into this error:</p>

<pre><code>pip install https://storage.googleapis.com/tensorflow/mac/tensorflow-0.5.0-py2-none-any.whl

tensorflow-0.5.0-py2-none-any.whl is not a supported wheel on this platform.
</code></pre>

<p>I don't see this under the common problems section, so any help would be appreciated! Thank you. </p>
",
Error in python after 'import tensorflow': TypeError: __init__() got an unexpected keyword argument 'syntax',"

<p>I installed TensorFlow on my Ubuntu 15.10 machine as instructed for CPU only:</p>

<pre class=""lang-sh prettyprint-override""><code>$ pip install https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.5.0-cp27-none-linux_x86_64.whl
</code></pre>

<p>Then when I run the Python REPL and import tensorflow, I get:</p>

<pre class=""lang-sh prettyprint-override""><code>$ python
Python 2.7.10 (default, Oct 14 2015, 16:09:02) 
[GCC 5.2.1 20151010] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
&gt;&gt;&gt; import tensorflow as tf
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""/home/phil/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 4, in &lt;module&gt;
   from tensorflow.python import *
  File ""/home/phil/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 13, in &lt;module&gt;
    from tensorflow.core.framework.graph_pb2 import *
  File ""/home/phil/.local/lib/python2.7/site-packages/tensorflow/core/framework/graph_pb2.py"", line 16, in &lt;module&gt;
    from tensorflow.core.framework import attr_value_pb2 as     tensorflow_dot_core_dot_framework_dot_attr__value__pb2
  File ""/home/phil/.local/lib/python2.7/site-packages/tensorflow/core/framework/attr_value_pb2.py"", line 16, in &lt;module&gt;
    from tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2
  File ""/home/phil/.local/lib/python2.7/site-packages/tensorflow/core/framework/tensor_pb2.py"", line 16, in &lt;module&gt;
    from tensorflow.core.framework import tensor_shape_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2
  File ""/home/phil/.local/lib/python2.7/site-packages/tensorflow/core/framework/tensor_shape_pb2.py"", line 22, in &lt;module&gt;
    serialized_pb=_b('\n,tensorflow/core/framework/tensor_shape.proto \x12\ntensorflow\""d\n\x10TensorShapeProto\x12-\n\x03\x64im\x18\x02 \x03(\x0b\x32 .tensorflow.TensorShapeProto.Dim\x1a!\n\x03\x44im\x12\x0c\n\x04size\x18\x01 \x01(\x03\x12\x0c\n\x04name\x18\x02 \x01(\tb\x06proto3')
TypeError: __init__() got an unexpected keyword argument 'syntax'
</code></pre>

<p>I have the Ubuntu protobuf-compiler package installed and it's version 2.6.1-1.2</p>
",
"pip installation error ""No such file or directory: setup.py""","

<p>I'm getting installation error because pip couldn't find setup.py.</p>

<pre class=""lang-sh prettyprint-override""><code>sudo pip install https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.5.0-cp27-none-linux_x86_64.whl
</code></pre>

<p>I get this error message.</p>

<pre class=""lang-sh prettyprint-override""><code>Downloading/unpacking https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.5.0-cp27-none-linux_x86_64.whl
Downloading tensorflow-0.5.0-cp27-none-linux_x86_64.whl (10.9Mb): 10.9Mb downloaded
Running setup.py egg_info for package from https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.5.0-cp27-none-linux_x86_64.whl
Traceback (most recent call last):
File ""&lt;string&gt;"", line 14, in &lt;module&gt;
IOError: [Errno 2] No such file or directory: '/tmp/pip-GdGgbz-build/setup.py'
Complete output from command python setup.py egg_info:
Traceback (most recent call last):

File ""&lt;string&gt;"", line 14, in &lt;module&gt;

IOError: [Errno 2] No such file or directory: '/tmp/pip-GdGgbz-build/setup.py'
</code></pre>

<p>Can anyone tell me how to solve this problem?</p>

<p>Thanks.</p>
",
Issue feeding a list into feed_dict in TensorFlow,"<p>I'm trying to pass a list into <code>feed_dict</code>, however I'm having trouble doing so. Say I have:</p>

<pre><code>inputs = 10 * [tf.placeholder(tf.float32, shape=(batch_size, input_size))]
</code></pre>

<p>where inputs is fed into some function <code>outputs</code> that I want to compute. So to run this in tensorflow, I created a session and ran the following:</p>

<pre><code>sess.run(outputs, feed_dict = {inputs: data}) 
#data is my list of inputs, which is also of length 10
</code></pre>

<p>but I get an error, <code>TypeError: unhashable type: 'list'.</code>
However, I'm able to pass the data element-wise like so:</p>

<pre><code>sess.run(outputs, feed_dict = {inputs[0]: data[0], ..., inputs[9]: data[9]}) 
</code></pre>

<p>So I'm wondering if there's a way I can solve this issue. I've also tried to construct a dictionary(using a <code>for</code> loop), however this results in a dictionary with a single element, where they key is: 
<code>tensorflow.python.framework.ops.Tensor at 0x107594a10</code></p>
",
What am I missing from this csv reader for TensorFlow?,"<p>It is mostly a copy paste from the tutorial, on the website. I am getting an error:</p>

<blockquote>
  <p>Invalid argument: ConcatOp : Expected concatenating dimensions in the
  range [0, 0), but got 0    [[Node: concat = Concat[N=4, T=DT_INT32,
  _device=""/job:localhost/replica:0/task:0/cpu:0""](concat/concat_dim, DecodeCSV, DecodeCSV:1, DecodeCSV:2, DecodeCSV:3)]]</p>
</blockquote>

<p>the contents of my csv file is:</p>

<blockquote>
  <p>3,4,1,8,4</p>
</blockquote>

<pre><code> import tensorflow as tf


filename_queue = tf.train.string_input_producer([""test2.csv""])

reader = tf.TextLineReader()
key, value = reader.read(filename_queue)

# Default values, in case of empty columns. Also specifies the type of the
# decoded result.
record_defaults = [[1], [1], [1], [1], [1]]
col1, col2, col3, col4, col5 = tf.decode_csv(
    value, record_defaults=record_defaults)
# print tf.shape(col1)

features = tf.concat(0, [col1, col2, col3, col4])
with tf.Session() as sess:
  # Start populating the filename queue.
  coord = tf.train.Coordinator()
  threads = tf.train.start_queue_runners(coord=coord)

  for i in range(1200):
    # Retrieve a single instance:
    example, label = sess.run([features, col5])

  coord.request_stop()
  coord.join(threads)
</code></pre>
",
Negative dimension size caused by subtracting 3 from 1 for 'Conv2D',"<p>I'm using <a href=""https://keras.io/"" rel=""noreferrer"">Keras</a> with <a href=""https://www.tensorflow.org/"" rel=""noreferrer"">Tensorflow</a> as backend , here is my code:</p>

<pre><code>import numpy as np
np.random.seed(1373) 
import tensorflow as tf
tf.python.control_flow_ops = tf

import os
from keras.datasets import mnist
from keras.models import Sequential
from keras.layers.core import Dense, Dropout, Activation, Flatten
from keras.layers.convolutional import Convolution2D, MaxPooling2D
from keras.utils import np_utils

batch_size = 128
nb_classes = 10
nb_epoch = 12


img_rows, img_cols = 28, 28

nb_filters = 32

nb_pool = 2

nb_conv = 3


(X_train, y_train), (X_test, y_test) = mnist.load_data()

print(X_train.shape[0])

X_train = X_train.reshape(X_train.shape[0], 1, img_rows, img_cols)
X_test = X_test.reshape(X_test.shape[0], 1, img_rows, img_cols)


X_train = X_train.astype('float32')
X_test = X_test.astype('float32')
X_train /= 255
X_test /= 255


print('X_train shape:', X_train.shape)
print(X_train.shape[0], 'train samples')
print(X_test.shape[0], 'test samples')


Y_train = np_utils.to_categorical(y_train, nb_classes)
Y_test = np_utils.to_categorical(y_test, nb_classes)

model = Sequential()

model.add(Convolution2D(nb_filters, nb_conv, nb_conv,
border_mode='valid',
input_shape=(1, img_rows, img_cols)))
model.add(Activation('relu'))
model.add(Convolution2D(nb_filters, nb_conv, nb_conv))
model.add(Activation('relu'))

model.add(MaxPooling2D(pool_size=(nb_pool, nb_pool)))
model.add(Dropout(0.25))

model.add(Flatten())
model.add(Dense(128))
model.add(Activation('relu'))
model.add(Dropout(0.5))
model.add(Dense(nb_classes)) 
model.add(Activation('softmax')) 

model.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics=[""accuracy""])


model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch,
verbose=1, validation_data=(X_test, Y_test))

score = model.evaluate(X_test, Y_test, verbose=0)

print('Test score:', score[0])
print('Test accuracy:', score[1])
</code></pre>

<p>and Trackback error:</p>

<pre><code>Using TensorFlow backend.
60000
('X_train shape:', (60000, 1, 28, 28))
(60000, 'train samples')
(10000, 'test samples')
Traceback (most recent call last):
  File ""mnist.py"", line 154, in &lt;module&gt;
    input_shape=(1, img_rows, img_cols)))
  File ""/usr/local/lib/python2.7/dist-packages/keras/models.py"", line 276, in add
    layer.create_input_layer(batch_input_shape, input_dtype)
  File ""/usr/local/lib/python2.7/dist-packages/keras/engine/topology.py"", line 370, in create_input_layer
    self(x)
  File ""/usr/local/lib/python2.7/dist-packages/keras/engine/topology.py"", line 514, in __call__
    self.add_inbound_node(inbound_layers, node_indices, tensor_indices)
  File ""/usr/local/lib/python2.7/dist-packages/keras/engine/topology.py"", line 572, in add_inbound_node
    Node.create_node(self, inbound_layers, node_indices, tensor_indices)
  File ""/usr/local/lib/python2.7/dist-packages/keras/engine/topology.py"", line 149, in create_node
    output_tensors = to_list(outbound_layer.call(input_tensors[0], mask=input_masks[0]))
  File ""/usr/local/lib/python2.7/dist-packages/keras/layers/convolutional.py"", line 466, in call
    filter_shape=self.W_shape)
  File ""/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py"", line 1579, in conv2d
    x = tf.nn.conv2d(x, kernel, strides, padding=padding)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_nn_ops.py"", line 396, in conv2d
    data_format=data_format, name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py"", line 759, in apply_op
    op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 2242, in create_op
    set_shapes_for_outputs(ret)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1617, in set_shapes_for_outputs
    shapes = shape_func(op)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1568, in call_with_requiring
    return call_cpp_shape_fn(op, require_shape_fn=True)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/common_shapes.py"", line 610, in call_cpp_shape_fn
    debug_python_shape_fn, require_shape_fn)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/common_shapes.py"", line 675, in _call_cpp_shape_fn_impl
    raise ValueError(err.message)
ValueError: Negative dimension size caused by subtracting 3 from 1 for 'Conv2D' (op: 'Conv2D') with input shapes: [?,1,28,28], [3,3,28,32].
</code></pre>

<p>First I saw some answers that problem is with <code>Tensorflow</code> version so I upgrade <code>Tensorflow</code> to <code>0.12.0</code>, but still exist , is that problem with network or I missing something, what should <code>input_shape</code> looks like?</p>

<p><strong>Update</strong>
Here is <code>./keras/keras.json</code>:</p>

<pre><code>{
    ""image_dim_ordering"": ""tf"", 
    ""epsilon"": 1e-07, 
    ""floatx"": ""float32"", 
    ""backend"": ""tensorflow""
}
</code></pre>
",
What's the best way to refresh TensorBoard after new events/logs were added?,"<p>What is the best way to quickly see the updated graph in the most recent event file in an open TensorBoard session? Re-running my Python app results in a new log file being created with potentially new events/graph. However, TensorBoard does not seem to notice those differences, unless restarted.</p>
",
Efficient element-wise multiplication of a matrix and a vector in TensorFlow,"<p>What would be the most efficient way to multiply (element-wise) a 2D tensor (matrix):</p>

<pre><code>x11 x12 .. x1N
...
xM1 xM2 .. xMN
</code></pre>

<p>by a vertical vector:</p>

<pre><code>w1
...
wN
</code></pre>

<p>to obtain a new matrix:</p>

<pre><code>x11*w1 x12*w2 ... x1N*wN
...
xM1*w1 xM2*w2 ... xMN*wN
</code></pre>

<p>To give some context, we have <code>M</code> data samples in a batch that can be processed in parallel, and each <code>N</code>-element sample must be multiplied by weights <code>w</code> stored in a variable to eventually pick the largest <code>Xij*wj</code> for each row <code>i</code>.</p>
",
Tensorflow on Raspberry Pi,"<p>I'm trying to install <code>Tensorflow</code> on <code>Raspberry Pi</code>. The OS is <code>Ubuntu Mate</code>, <code>python2.7</code> and <code>PIP</code> version is <code>7.1.2</code></p>

<p>When I run this</p>

<pre><code>pip install https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.5.0-cp27-none-linux_x86_64.whl
</code></pre>

<p>it tells that </p>

<blockquote>
  <p>""tensorflow-0.5.0... is not a supported wheel on this platform.""</p>
</blockquote>

<p>Is it possible to install <code>Tensorflow</code> on <code>Raspberry</code>?
And someone knows what I'm missing?</p>
",
Sentiment Analysis using tensorflow,"<p>I am exploring tensorflow and would like to do sentiment analysis using the options available. I had a look at the following tutorial <a href=""http://www.tensorflow.org/tutorials/recurrent/index.html#language_modeling"">http://www.tensorflow.org/tutorials/recurrent/index.html#language_modeling</a> </p>

<p>I have worked woth Naive Bayes Classifier, Maximum Entropy Algorithm and Scikit Learn Classifier and would like to know if there are any better algorithms offered by tensorflow. Is this the right place to start or are there any other options?</p>

<p>Any help pointing in the right direction would be greatly appreciated.</p>

<p>Thanks in advance.</p>
",
Best way to flatten a 2D tensor containing a vector in TensorFlow?,"<p>What is the most efficient way to flatten a 2D tensor which is actually a horizontal or vertical vector into a 1D tensor? </p>

<p>Is there a difference in terms of performance between:</p>

<pre><code>tf.reshape(w, [-1])
</code></pre>

<p>and</p>

<pre><code>tf.squeeze(w)
</code></pre>

<p>?</p>
",
How to prevent tensorflow from allocating the totality of a GPU memory?,"<p>I work in an environment in which computational resources are shared, i.e., we have a few server machines equipped with a few Nvidia Titan X GPUs each.</p>

<p>For small to moderate size models, the 12GB of the Titan X are usually enough for 2-3 people to run training concurrently on the same GPU. If the models are small enough that a single model does not take full advantage of all the computational units of the Titan X, this can actually result in a speedup compared with running one training process after the other. Even in cases where the concurrent access to the GPU does slow down the individual training time, it is still nice to have the flexibility of having several users running things on the GPUs at once.</p>

<p>The problem with TensorFlow is that, by default, it allocates the full amount of available memory on the GPU when it is launched. Even for a small 2-layer Neural Network, I see that the 12 GB of the Titan X are used up.</p>

<p>Is there a way to make TensorFlow only allocate, say, 4GB of GPU memory, if one knows that that amount is enough for a given model?</p>
",
Run Tensorflow unit tests,"<p>Is there any way to run Tensorflow unit tests manually? I want to perform sanity checks while modifying TF source code.</p>

<p>I see there are many _test.py files with classes that perform many test operations and I can't figure out how to run them. There should be an easy way?</p>
",
Tensorflow read images with labels,"<p>I am building a standard image classification model with Tensorflow. For this I have input images, each assigned with a label (number in {0,1}). The Data can hence be stored in a list using the following format:</p>

<pre><code>/path/to/image_0 label_0
/path/to/image_1 label_1
/path/to/image_2 label_2
...
</code></pre>

<p>I want to use TensorFlow's queuing system to read my data and feed it to my model. Ignoring the labels, one can easily achieve this by using <code>string_input_producer</code> and <code>wholeFileReader</code>. Here the code:</p>

<pre><code>def read_my_file_format(filename_queue):
  reader = tf.WholeFileReader()
  key, value = reader.read(filename_queue)
  example = tf.image.decode_png(value)
  return example

#removing label, obtaining list containing /path/to/image_x
image_list = [line[:-2] for line in image_label_list]

input_queue = tf.train.string_input_producer(image_list)                                                     
input_images = read_my_file_format(input_queue)
</code></pre>

<p>However, the labels are lost in that process as the image data is purposely shuffled as part of the input pipeline. What is the easiest way of pushing the labels together with the image data through the input queues?</p>
",
Is there an example on how to generate protobuf files holding trained Tensorflow graphs,"<p>I am looking at Google's example on how to deploy and use a pre-trained Tensorflow graph (model) on Android, at:</p>

<p><a href=""https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android"">https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android</a></p>

<p>This example uses a .pb file at: <strong>[this is a link to a file that downloads automatically</strong>]
<a href=""https://storage.googleapis.com/download.tensorflow.org/models/inception5h.zip"">https://storage.googleapis.com/download.tensorflow.org/models/inception5h.zip</a></p>

<p>The example shows how to load the .pb file to a Tensorflow session and use it to perform classification, but it doesn't (?) mention how to generate such a .pb file after a graph is trained (e.g., in Python).</p>

<p>Are there any examples on how to do that?</p>
",
ValueError when executing softmax_cross_entropy_with_logits,"<p>I am following the <a href=""https://pythonprogramming.net/tensorflow-neural-network-session-machine-learning-tutorial/"" rel=""noreferrer"">tensorflow tutorial</a>. There has been recent tensor flow update in which the cost function <code>softmax_cross_entropy_with_logits()</code> has been modified. Hence the code in the tutorial is giving the following error:</p>

<p><code>ValueError: Only call softmax_cross_entropy_with_logits with named arguments (labels=..., logits=..., ...)</code> </p>

<p>What does it mean and how to rectify it?</p>

<p>Here's the entire code till that point:</p>

<pre><code>import tensorflow as tf
from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets(""/tmp/data/"", one_hot = True)

n_nodes_hl1 = 500
n_nodes_hl2 = 500
n_nodes_hl3 = 500

n_classes = 10
batch_size = 100

x = tf.placeholder('float', [None, 784])
y = tf.placeholder('float')

def neural_network_model(data):
hidden_1_layer = {'weights':tf.Variable(tf.random_normal([784, n_nodes_hl1])),
                  'biases':tf.Variable(tf.random_normal([n_nodes_hl1]))}

hidden_2_layer = {'weights':tf.Variable(tf.random_normal([n_nodes_hl1, n_nodes_hl2])),
                  'biases':tf.Variable(tf.random_normal([n_nodes_hl2]))}

hidden_3_layer = {'weights':tf.Variable(tf.random_normal([n_nodes_hl2, n_nodes_hl3])),
                  'biases':tf.Variable(tf.random_normal([n_nodes_hl3]))}

output_layer = {'weights':tf.Variable(tf.random_normal([n_nodes_hl3, n_classes])),
                'biases':tf.Variable(tf.random_normal([n_classes])),}


l1 = tf.add(tf.matmul(data,hidden_1_layer['weights']), hidden_1_layer['biases'])
l1 = tf.nn.relu(l1)

l2 = tf.add(tf.matmul(l1,hidden_2_layer['weights']), hidden_2_layer['biases'])
l2 = tf.nn.relu(l2)

l3 = tf.add(tf.matmul(l2,hidden_3_layer['weights']), hidden_3_layer['biases'])
l3 = tf.nn.relu(l3)

output = tf.matmul(l3,output_layer['weights']) + output_layer['biases']

return output

def train_neural_network(x):
prediction = neural_network_model(x)
cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(prediction,y) )
optimizer = tf.train.AdamOptimizer().minimize(cost)
</code></pre>
",
Image recognition using TensorFlow,"<p>I'm new to TensorFlow and I am looking for help on image recognition. Is there an example that showcases how to use TensorFlow to train your own digital images for image recognition like the image-net model used in the <a href=""https://www.tensorflow.org/versions/master/tutorials/image_recognition/index.html#image-recognition"">TensorFlow image recognition tutorial</a></p>

<p>I looked at the CIFAR-10 model training but it doesn't seem to provide examples for training your own images.</p>
",
Configure input_map when importing a tensorflow model from metagraph file,"

<p>I've trained a DCGAN model and would now like to load it into a library that visualizes the drivers of neuron activation through image space optimization.</p>

<p>The following code works, but forces me to work with (1, width, height, channels) images when doing subsequent image analysis, which is a pain (the library assumptions about the shape of network input).</p>

<pre class=""lang-py prettyprint-override""><code># creating TensorFlow session and loading the model
graph = tf.Graph()
sess = tf.InteractiveSession(graph=graph)

new_saver = tf.train.import_meta_graph(model_fn)
new_saver.restore(sess, './')
</code></pre>

<p>I'd like to change the input_map, After reading the source, I expected this code to work:</p>

<pre class=""lang-py prettyprint-override""><code>graph = tf.Graph()
sess = tf.InteractiveSession(graph=graph)

t_input = tf.placeholder(np.float32, name='images') # define the input tensor
t_preprocessed = tf.expand_dims(t_input, 0)

new_saver = tf.train.import_meta_graph(model_fn, input_map={'images': t_input})
new_saver.restore(sess, './')
</code></pre>

<p>But got an error:</p>

<blockquote>
  <p>ValueError: tf.import_graph_def() requires a non-empty <code>name</code> if <code>input_map</code> is used.</p>
</blockquote>

<p>When the stack gets down to <code>tf.import_graph_def()</code> the name field is set to import_scope, so I tried the following:</p>

<pre class=""lang-py prettyprint-override""><code>graph = tf.Graph()
sess = tf.InteractiveSession(graph=graph)

t_input = tf.placeholder(np.float32, name='images') # define the input tensor
t_preprocessed = tf.expand_dims(t_input, 0)

new_saver = tf.train.import_meta_graph(model_fn, input_map={'images': t_input}, import_scope='import')
new_saver.restore(sess, './')
</code></pre>

<p>Which netted me the following <code>KeyError</code>:</p>

<blockquote>
  <p>KeyError: ""The name 'gradients/discriminator/minibatch/map/while/TensorArrayWrite/TensorArrayWriteV3_grad/TensorArrayReadV3/RefEnter:0' refers to a Tensor which does not exist. The operation, 'gradients/discriminator/minibatch/map/while/TensorArrayWrite/TensorArrayWriteV3_grad/TensorArrayReadV3/RefEnter', does not exist in the graph.""</p>
</blockquote>

<p>If I set 'import_scope', I get the same error whether or not I set 'input_map'. </p>

<p>I'm not sure where to go from here.</p>
",
"Adam optimizer goes haywire after 200k batches, training loss grows","<p>I've been seeing a very strange behavior when training a network, where after a couple of 100k iterations (8 to 10 hours) of learning fine, everything breaks and the training loss <em>grows</em>:</p>

<p><a href=""https://i.stack.imgur.com/47T4x.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/47T4x.jpg"" alt=""Loss explodes""></a></p>

<p>The training data itself is randomized and spread across many <code>.tfrecord</code> files containing <code>1000</code> examples each, then shuffled again in the input stage and batched to <code>200</code> examples.</p>

<h3>The background</h3>

<p>I am designing a network that performs four different regression tasks at the same time, e.g. determining the likelihood of an object appearing in the image and simultanously determining its orientation. The network starts with a couple of convolutional layers, some with residual connections, and then branches into the four fully-connected segments.</p>

<p>Since the first regression results in a probability, I'm using cross entropy for the loss, whereas the others use classical L2 distance. However, due to their nature, the probability loss is around the order of <code>0..1</code>, while the orientation losses can be much larger, say <code>0..10</code>. I already normalized both input and output values and use clipping</p>

<pre><code>normalized = tf.clip_by_average_norm(inferred.sin_cos, clip_norm=2.)
</code></pre>

<p>in cases where things can get really bad.</p>

<p>I've been (successfully) using the Adam optimizer to optimize on the tensor containing all distinct losses (rather than <code>reduce_sum</code>ing them), like so:</p>

<pre><code>reg_loss = tf.reduce_sum(tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES))
loss = tf.pack([loss_probability, sin_cos_mse, magnitude_mse, pos_mse, reg_loss])

optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate,
                                   epsilon=self.params.adam_epsilon)
op_minimize = optimizer.minimize(loss, global_step=global_step)
</code></pre>

<p>In order to display the results in TensorBoard, I then actually do</p>

<pre><code>loss_sum = tf.reduce_sum(loss)
</code></pre>

<p>for a scalar summary.</p>

<p>Adam is set to learning rate <code>1e-4</code> and epsilon <code>1e-4</code> (I see the same behavior with the default value for epislon and it breaks even faster when I keep the learning rate on <code>1e-3</code>). Regularization also has no influence on this one, it does this sort-of consistently at some point.</p>

<p>I should also add that stopping the training and restarting from the last checkpoint - implying that the training input files are shuffled again as well - results in the same behavior. The training always seems to behave similarly at that point.</p>
",
Printing the loss during TensorFlow training,"<p>I am looking at the TensorFlow ""MNIST For ML Beginners"" tutorial, and I want to print out the training loss after every training step.</p>

<p>My training loop currently looks like this:</p>

<pre><code>for i in range(100):
    batch_xs, batch_ys = mnist.train.next_batch(100)
    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})
</code></pre>

<p>Now, <code>train_step</code> is defined as:</p>

<pre><code>train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)
</code></pre>

<p>Where <code>cross_entropy</code> is the loss which I want to print out:</p>

<pre><code>cross_entropy = -tf.reduce_sum(y_ * tf.log(y))
</code></pre>

<p>One way to print this would be to explicitly compute <code>cross_entropy</code> in the training loop:</p>

<pre><code>for i in range(100):
    batch_xs, batch_ys = mnist.train.next_batch(100)
    cross_entropy = -tf.reduce_sum(y_ * tf.log(y))
    print 'loss = ' + str(cross_entropy)
    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})
</code></pre>

<p>I now have two questions regarding this:</p>

<ol>
<li><p>Given that <code>cross_entropy</code> is already computed during <code>sess.run(train_step, ...)</code>, it seems inefficient to compute it twice, requiring twice the number of forward passes of all the training data. Is there a way to access the value of <code>cross_entropy</code> when it was computed during <code>sess.run(train_step, ...)</code>?</p></li>
<li><p>How do I even print a <code>tf.Variable</code>? Using <code>str(cross_entropy)</code> gives me an error...</p></li>
</ol>

<p>Thank you!</p>
",
"Tensorflow Slim: TypeError: Expected int32, got list containing Tensors of type '_Message' instead","<p>I am following <a href=""https://github.com/tensorflow/models/blob/master/slim/slim_walkthough.ipynb"" rel=""noreferrer"">this</a> tutorial for learning TensorFlow Slim but upon running the following code for Inception:</p>

<pre><code>import numpy as np
import os
import tensorflow as tf
import urllib2

from datasets import imagenet
from nets import inception
from preprocessing import inception_preprocessing

slim = tf.contrib.slim

batch_size = 3
image_size = inception.inception_v1.default_image_size
checkpoints_dir = '/tmp/checkpoints/'
with tf.Graph().as_default():
    url = 'https://upload.wikimedia.org/wikipedia/commons/7/70/EnglishCockerSpaniel_simon.jpg'
    image_string = urllib2.urlopen(url).read()
    image = tf.image.decode_jpeg(image_string, channels=3)
    processed_image = inception_preprocessing.preprocess_image(image, image_size, image_size, is_training=False)
    processed_images  = tf.expand_dims(processed_image, 0)

    # Create the model, use the default arg scope to configure the batch norm parameters.
    with slim.arg_scope(inception.inception_v1_arg_scope()):
        logits, _ = inception.inception_v1(processed_images, num_classes=1001, is_training=False)
    probabilities = tf.nn.softmax(logits)

    init_fn = slim.assign_from_checkpoint_fn(
        os.path.join(checkpoints_dir, 'inception_v1.ckpt'),
        slim.get_model_variables('InceptionV1'))

    with tf.Session() as sess:
        init_fn(sess)
        np_image, probabilities = sess.run([image, probabilities])
        probabilities = probabilities[0, 0:]
        sorted_inds = [i[0] for i in sorted(enumerate(-probabilities), key=lambda x:x[1])]

    plt.figure()
    plt.imshow(np_image.astype(np.uint8))
    plt.axis('off')
    plt.show()

    names = imagenet.create_readable_names_for_imagenet_labels()
    for i in range(5):
        index = sorted_inds[i]
        print('Probability %0.2f%% =&gt; [%s]' % (probabilities[index], names[index]))
</code></pre>

<p>I seem to be getting this set of errors:</p>

<pre><code>Traceback (most recent call last):
  File ""DA_test_pred.py"", line 24, in &lt;module&gt;
    logits, _ = inception.inception_v1(processed_images, num_classes=1001, is_training=False)
  File ""/home/deepankar1994/Desktop/MTP/TensorFlowEx/TFSlim/models/slim/nets/inception_v1.py"", line 290, in inception_v1
    net, end_points = inception_v1_base(inputs, scope=scope)
  File ""/home/deepankar1994/Desktop/MTP/TensorFlowEx/TFSlim/models/slim/nets/inception_v1.py"", line 96, in inception_v1_base
    net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3])
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/array_ops.py"", line 1053, in concat
    dtype=dtypes.int32).get_shape(
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 651, in convert_to_tensor
    as_ref=False)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 716, in internal_convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/constant_op.py"", line 176, in _constant_tensor_conversion_function
    return constant(v, dtype=dtype, name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/constant_op.py"", line 165, in constant
    tensor_util.make_tensor_proto(value, dtype=dtype, shape=shape, verify_shape=verify_shape))
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_util.py"", line 367, in make_tensor_proto
    _AssertCompatible(values, dtype)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_util.py"", line 302, in _AssertCompatible
    (dtype.name, repr(mismatch), type(mismatch).__name__))
TypeError: Expected int32, got list containing Tensors of type '_Message' instead.
</code></pre>

<p>This is strange because all of this code is from their official guide. I am new to TF and any help would be appreciated.</p>
",
sparse autoencoder cost function in tensorflow,"<p>I've been going through a variety of TensorFlow tutorials to try to familiarize myself with how it works; and I've become interested in utilizing autoencoders.</p>

<p>I started by using the model autoencoder in Tensorflow's models repository:</p>

<p><a href=""https://github.com/tensorflow/models/tree/master/autoencoder"" rel=""noreferrer"">https://github.com/tensorflow/models/tree/master/autoencoder</a></p>

<p>I got it working, and while visualizing the weights, expected to see something like this:</p>

<p><a href=""https://i.stack.imgur.com/F33rU.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/F33rU.png"" alt=""enter image description here""></a></p>

<p>however, my autoencoder gives me garbage-looking weights (despite accurately recreating the input image).</p>

<p><a href=""https://i.stack.imgur.com/tkOvj.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/tkOvj.png"" alt=""enter image description here""></a></p>

<p>Further reading suggests that what I'm missing is that my autoencoder is not sparse, so I need to enforce a sparsity cost to the weights.</p>

<p>I've tried to add a sparsity cost to the original code (based off of this example<a href=""https://github.com/georgeiswang/Sparse-Autoencoder-Tensorflow/blob/master/sparse_autoencoder.py/06_autoencoder.py"" rel=""noreferrer"">3</a>), but it doesn't seem to change the weights to looking like the model ones.</p>

<p>How can I properly change the cost to get features that look like the ones that a typically found in the autoencoded MNIST dataset?  My modified model is here:</p>

<pre><code>import numpy as np
import random
import math
import tensorflow as tf
from tensorflow.examples.tutorials.mnist import input_data
import matplotlib.pyplot as plt

def xavier_init(fan_in, fan_out, constant = 1):
    low = -constant * np.sqrt(6.0 / (fan_in + fan_out))
    high = constant * np.sqrt(6.0 / (fan_in + fan_out))
    return tf.random_uniform((fan_in, fan_out), minval = low, maxval = high, dtype = tf.float32)

class AdditiveGaussianNoiseAutoencoder(object):
    def __init__(self, n_input, n_hidden, transfer_function = tf.nn.sigmoid, optimizer = tf.train.AdamOptimizer(),
                 scale = 0.1):
        self.n_input = n_input
        self.n_hidden = n_hidden
        self.transfer = transfer_function
        self.scale = tf.placeholder(tf.float32)
        self.training_scale = scale
        network_weights = self._initialize_weights()
        self.weights = network_weights
        self.sparsity_level= 0.1#np.repeat([0.05], self.n_hidden).astype(np.float32)
        self.sparse_reg = 10

        # model
        self.x = tf.placeholder(tf.float32, [None, self.n_input])
        self.hidden = self.transfer(tf.add(tf.matmul(self.x + scale * tf.random_normal((n_input,)),
                self.weights['w1']),
                self.weights['b1']))
        self.reconstruction = tf.add(tf.matmul(self.hidden, self.weights['w2']), self.weights['b2'])

        # cost
        self.cost = 0.5 * tf.reduce_sum(tf.pow(tf.subtract(self.reconstruction, self.x), 2.0)) + self.sparse_reg \
                        * self.kl_divergence(self.sparsity_level, self.hidden)

        self.optimizer = optimizer.minimize(self.cost)

        init = tf.global_variables_initializer()
        self.sess = tf.Session()
        self.sess.run(init)

    def _initialize_weights(self):
        all_weights = dict()
        all_weights['w1'] = tf.Variable(xavier_init(self.n_input, self.n_hidden))
        all_weights['b1'] = tf.Variable(tf.zeros([self.n_hidden], dtype = tf.float32))
        all_weights['w2'] = tf.Variable(tf.zeros([self.n_hidden, self.n_input], dtype = tf.float32))
        all_weights['b2'] = tf.Variable(tf.zeros([self.n_input], dtype = tf.float32))
        return all_weights

    def partial_fit(self, X):
        cost, opt = self.sess.run((self.cost, self.optimizer), feed_dict = {self.x: X,
                                                                            self.scale: self.training_scale
                                                                            })
        return cost

    def kl_divergence(self, p, p_hat):
        return tf.reduce_mean(p * tf.log(p) - p * tf.log(p_hat) + (1 - p) * tf.log(1 - p) - (1 - p) * tf.log(1 - p_hat))

    def calc_total_cost(self, X):
        return self.sess.run(self.cost, feed_dict = {self.x: X,
                                                     self.scale: self.training_scale
                                                     })

    def transform(self, X):
        return self.sess.run(self.hidden, feed_dict = {self.x: X,
                                                       self.scale: self.training_scale
                                                       })

    def generate(self, hidden = None):
        if hidden is None:
            hidden = np.random.normal(size = self.weights[""b1""])
        return self.sess.run(self.reconstruction, feed_dict = {self.hidden: hidden})

    def reconstruct(self, X):
        return self.sess.run(self.reconstruction, feed_dict = {self.x: X,
                                                               self.scale: self.training_scale
                                                               })

    def getWeights(self):
        return self.sess.run(self.weights['w1'])

    def getBiases(self):
        return self.sess.run(self.weights['b1'])


mnist = input_data.read_data_sets('MNIST_data', one_hot = True)

def get_random_block_from_data(data, batch_size):
    start_index = np.random.randint(0, len(data) - batch_size)
    return data[start_index:(start_index + batch_size)]

X_train = mnist.train.images
X_test = mnist.test.images

n_samples = int(mnist.train.num_examples)
training_epochs = 50
batch_size = 128
display_step = 1

autoencoder = AdditiveGaussianNoiseAutoencoder(n_input = 784,
                                               n_hidden = 200,
                                               transfer_function = tf.nn.sigmoid,
                                               optimizer = tf.train.GradientDescentOptimizer(learning_rate = 0.01),
                                               scale = 0.01)

for epoch in range(training_epochs):
    avg_cost = 0.
    total_batch = int(n_samples / batch_size)
    # Loop over all batches
    for i in range(total_batch):
        batch_xs = get_random_block_from_data(X_train, batch_size)

        # Fit training using batch data
        cost = autoencoder.partial_fit(batch_xs)
        # Compute average loss
        avg_cost += cost / n_samples * batch_size

    # Display logs per epoch step
    if epoch % display_step == 0:
        print(""Epoch:"", '%04d' % (epoch + 1), ""cost="", avg_cost)

print(""Total cost: "" + str(autoencoder.calc_total_cost(X_test)))

imageToUse = random.choice(mnist.test.images)

plt.imshow(np.reshape(imageToUse,[28,28]), interpolation=""nearest"", cmap=""gray"", clim=(0, 1.0))
plt.show()

# input weights
wts = autoencoder.getWeights()
dim = math.ceil(math.sqrt(autoencoder.n_hidden))
plt.figure(1, figsize=(dim, dim))
for i in range(0,autoencoder.n_hidden):
    im = wts.flatten()[i::autoencoder.n_hidden].reshape((28,28))
    plt.subplot(dim, dim, i+1)
    #plt.title('Feature Weights ' + str(i))
    plt.imshow(im, cmap=""gray"", clim=(-1.0, 1.0))
    plt.colorbar()
plt.show()

predicted_imgs = autoencoder.reconstruct(X_test[:100])

# plot the reconstructed images
plt.figure(1, figsize=(10, 10))
plt.title('Autoencoded Images')
for i in range(0,100):
    im = predicted_imgs[i].reshape((28,28))
    plt.subplot(10, 10, i+1)
    plt.imshow(im, cmap=""gray"", clim=(0.0, 1.0))
plt.show()
</code></pre>
",
Keras + Tensorflow: Prediction on multiple gpus,"<p>I'm using Keras with tensorflow as backend.
I have one compiled/trained model.</p>

<p>My prediction loop is slow so I would like to find a way to parallelize the <code>predict_proba</code> calls to speed things up.
I would like to take a list of batches (of data) and then per available gpu, run <code>model.predict_proba()</code> over a subset of those batches.<br>
Essentially:</p>

<pre><code>data = [ batch_0, batch_1, ... , batch_N ]
on gpu_0 =&gt; return predict_proba(batch_0)
on gpu_1 =&gt; return predict_proba(batch_1)
...
on gpu_N =&gt; return predict_proba(batch_N) 
</code></pre>

<p>I know that it's possible in pure Tensorflow to assign ops to a given gpu (<a href=""https://www.tensorflow.org/tutorials/using_gpu"" rel=""noreferrer"">https://www.tensorflow.org/tutorials/using_gpu</a>).  However, I don't know how this translates to my situation since I've built/compiled/trained my model using Keras' api.</p>

<p>I had thought that maybe I just needed to use python's multiprocessing module and start a process per gpu that would run <code>predict_proba(batch_n)</code>.  I know this is theoretically possible given another SO post of mine: <a href=""https://stackoverflow.com/questions/42504669/keras-tensorflow-and-multiprocessing-in-python"">Keras + Tensorflow and Multiprocessing in Python</a>.  However, this still leaves me with the dilemma of not knowing how to actually ""choose"" a gpu to operate the process on.</p>

<p>My question boils down to: how does one parallelize prediction for one model in Keras across multiple gpus when using Tensorflow as Keras' backend?</p>

<p>Additionally I am curious if similar parallelization for prediction is possible with only one gpu.   </p>

<p>A high level description or code example would be greatly appreciated!</p>

<p>Thanks!</p>
",
How do I convert a directory of jpeg images to TFRecords file in tensorflow?,"<p>I have training data that is a directory of jpeg images and a corresponding text file containing the file name and the associated category label.  I am trying to convert this training data into a tfrecords file as described in the tensorflow documentation. I have spent quite some time trying to get this to work but there are no examples in tensorflow that demonstrate how to use any of the readers to read in jpeg files and add them to a tfrecord using tfrecordwriter</p>
",
Batch normalization with 3D convolutions in TensorFlow,"

<p>I'm implementing a model relying on 3D convolutions (for a task that is similar to action recognition) and I want to use batch normalization (see <a href=""https://arxiv.org/abs/1502.03167"" rel=""noreferrer"">[Ioffe &amp; Szegedy 2015]</a>). I could not find any tutorial focusing on 3D convs, hence I'm making a short one here which I'd like to review with you.</p>

<p>The code below refers to TensorFlow r0.12 and it explicitly instances variables - I mean I'm not using tf.contrib.learn except for the tf.contrib.layers.batch_norm() function. I'm doing this both to better understand how things work under the hood and to have more implementation freedom (e.g., variable summaries).</p>

<p>I will get to the 3D convolution case smoothly by first writing the example for a fully-connected layer, then for a 2D convolution and finally for the 3D case. While going through the code, it would be great if you could check if everything is done correctly - the code runs, but I'm not 100% sure about the way I apply batch normalization. I end this post with a more detailed question.</p>

<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf

# This flag is used to allow/prevent batch normalization params updates
# depending on whether the model is being trained or used for prediction.
training = tf.placeholder_with_default(True, shape=())
</code></pre>

<h2>Fully-connected (FC) case</h2>

<pre class=""lang-py prettyprint-override""><code># Input.
INPUT_SIZE = 512
u = tf.placeholder(tf.float32, shape=(None, INPUT_SIZE))

# FC params: weights only, no bias as per [Ioffe &amp; Szegedy 2015].
FC_OUTPUT_LAYER_SIZE = 1024
w = tf.Variable(tf.truncated_normal(
    [INPUT_SIZE, FC_OUTPUT_LAYER_SIZE], dtype=tf.float32, stddev=1e-1))

# Layer output with no activation function (yet).
fc = tf.matmul(u, w)

# Batch normalization.
fc_bn = tf.contrib.layers.batch_norm(
    fc,
    center=True,
    scale=True,
    is_training=training,
    scope='fc-batch_norm')

# Activation function.
fc_bn_relu = tf.nn.relu(fc_bn)
print(fc_bn_relu)  # Tensor(""Relu:0"", shape=(?, 1024), dtype=float32)
</code></pre>

<h2>2D convolutional (CNN) layer case</h2>

<pre class=""lang-py prettyprint-override""><code># Input: 640x480 RGB images (whitened input, hence tf.float32).
INPUT_HEIGHT = 480
INPUT_WIDTH = 640
INPUT_CHANNELS = 3
u = tf.placeholder(tf.float32, shape=(None, INPUT_HEIGHT, INPUT_WIDTH, INPUT_CHANNELS))

# CNN params: wights only, no bias as per [Ioffe &amp; Szegedy 2015].
CNN_FILTER_HEIGHT = 3  # Space dimension.
CNN_FILTER_WIDTH = 3  # Space dimension.
CNN_FILTERS = 128
w = tf.Variable(tf.truncated_normal(
    [CNN_FILTER_HEIGHT, CNN_FILTER_WIDTH, INPUT_CHANNELS, CNN_FILTERS],
    dtype=tf.float32, stddev=1e-1))

# Layer output with no activation function (yet).
CNN_LAYER_STRIDE_VERTICAL = 1
CNN_LAYER_STRIDE_HORIZONTAL = 1
CNN_LAYER_PADDING = 'SAME'
cnn = tf.nn.conv2d(
    input=u, filter=w,
    strides=[1, CNN_LAYER_STRIDE_VERTICAL, CNN_LAYER_STRIDE_HORIZONTAL, 1],
    padding=CNN_LAYER_PADDING)

# Batch normalization.
cnn_bn = tf.contrib.layers.batch_norm(
    cnn,
    data_format='NHWC',  # Matching the ""cnn"" tensor which has shape (?, 480, 640, 128).
    center=True,
    scale=True,
    is_training=training,
    scope='cnn-batch_norm')

# Activation function.
cnn_bn_relu = tf.nn.relu(cnn_bn)
print(cnn_bn_relu)  # Tensor(""Relu_1:0"", shape=(?, 480, 640, 128), dtype=float32)
</code></pre>

<h2>3D convolutional (CNN3D) layer case</h2>

<pre class=""lang-py prettyprint-override""><code># Input: sequence of 9 160x120 RGB images (whitened input, hence tf.float32).
INPUT_SEQ_LENGTH = 9
INPUT_HEIGHT = 120
INPUT_WIDTH = 160
INPUT_CHANNELS = 3
u = tf.placeholder(tf.float32, shape=(None, INPUT_SEQ_LENGTH, INPUT_HEIGHT, INPUT_WIDTH, INPUT_CHANNELS))

# CNN params: wights only, no bias as per [Ioffe &amp; Szegedy 2015].
CNN3D_FILTER_LENGHT = 3  # Time dimension.
CNN3D_FILTER_HEIGHT = 3  # Space dimension.
CNN3D_FILTER_WIDTH = 3  # Space dimension.
CNN3D_FILTERS = 96
w = tf.Variable(tf.truncated_normal(
    [CNN3D_FILTER_LENGHT, CNN3D_FILTER_HEIGHT, CNN3D_FILTER_WIDTH, INPUT_CHANNELS, CNN3D_FILTERS],
    dtype=tf.float32, stddev=1e-1))

# Layer output with no activation function (yet).
CNN3D_LAYER_STRIDE_TEMPORAL = 1
CNN3D_LAYER_STRIDE_VERTICAL = 1
CNN3D_LAYER_STRIDE_HORIZONTAL = 1
CNN3D_LAYER_PADDING = 'SAME'
cnn3d = tf.nn.conv3d(
    input=u, filter=w,
    strides=[1, CNN3D_LAYER_STRIDE_TEMPORAL, CNN3D_LAYER_STRIDE_VERTICAL, CNN3D_LAYER_STRIDE_HORIZONTAL, 1],
    padding=CNN3D_LAYER_PADDING)

# Batch normalization.
cnn3d_bn = tf.contrib.layers.batch_norm(
    cnn3d,
    data_format='NHWC',  # Matching the ""cnn"" tensor which has shape (?, 9, 120, 160, 96).
    center=True,
    scale=True,
    is_training=training,
    scope='cnn3d-batch_norm')

# Activation function.
cnn3d_bn_relu = tf.nn.relu(cnn3d_bn)
print(cnn3d_bn_relu)  # Tensor(""Relu_2:0"", shape=(?, 9, 120, 160, 96), dtype=float32)
</code></pre>

<p>What I would like to make sure is whether the code above exactly implements batch normalization as described in <a href=""https://arxiv.org/abs/1502.03167"" rel=""noreferrer"">[Ioffe &amp; Szegedy 2015]</a> at the end of Sec. 3.2:</p>

<blockquote>
  <p>For convolutional layers, we additionally want the normalization to obey the convolutional property – so that different elements of the same feature map, at different locations, are normalized in the same way. To achieve this, we jointly normalize all the activations in a minibatch, over all locations. [...] Alg. 2 is modified similarly, so that during inference the BN transform applies the same linear transformation to each activation in a given feature map.</p>
</blockquote>

<p><strong>UPDATE</strong>
I guess the code above is also correct for the 3D conv case. In fact, when I define my model if I print all the trainable variables, I also see the expected numbers of beta and gamma variables. For instance:</p>

<pre class=""lang-py prettyprint-override""><code>Tensor(""conv3a/conv3d_weights/read:0"", shape=(3, 3, 3, 128, 256), dtype=float32)
Tensor(""BatchNorm_2/beta/read:0"", shape=(256,), dtype=float32)
Tensor(""BatchNorm_2/gamma/read:0"", shape=(256,), dtype=float32)
</code></pre>

<p>This looks ok to me since due to BN, one pair of beta and gamma are learned for each feature map (256 in total).</p>

<hr>

<p>[Ioffe &amp; Szegedy 2015]: Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</p>
",
"Tensorflow ""map operation"" for tensor?","<p>I am  adapting the <a href=""https://tensorflow.googlesource.com/tensorflow/+/master/tensorflow/models/image/cifar10/cifar10.py"" rel=""nofollow noreferrer"">cifar10 convolution example</a> to my problem.  I'd like to change the data input from a design that reads images one-at-a-time from a file to a design that operates on an already-in-memory set of images.  The original <code>inputs()</code> function looks like this:</p>

<pre><code>read_input = cifar10_input.read_cifar10(filename_queue)
reshaped_image = tf.cast(read_input.uint8image, tf.float32)
# Crop the central [height, width] of the image.
resized_image = tf.image.resize_image_with_crop_or_pad(reshaped_image,
                                                     width, height)
</code></pre>

<p>In the original version, <code>read_input</code> is a tensor containing one image.</p>

<p>I keep all my images in RAM, so instead of using <code>filename_queue</code>, I have one huge <code>images_tensor = tf.constant(images)</code>, where <code>images_tensor.shape</code> is (something, 32, 32, 3).</p>

<p>My question is very-very basic:  what is the best way to apply some function (<code>tf.image.resize_image_with_crop_or_pad</code> in my case) to all elements of  <code>images_tensor</code>? </p>

<p>Iterating is problematic in tensorflow, with limited slices(<a href=""https://stackoverflow.com/questions/33736795/tensorflow-numpy-like-tensor-indexing"">TensorFlow - numpy-like tensor indexing</a>).  Is there a solution to achieving this using just one command?</p>
",
Error using Tensorflow with GPU,"<p>I've tried a bunch of different Tensorflow examples, which works fine on the CPU but generates the same error when I'm trying to run them on the GPU. One little example is this:</p>

<pre><code>import tensorflow as tf

# Creates a graph.
a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')
b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')
c = tf.matmul(a, b)
# Creates a session with log_device_placement set to True.
sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))
# Runs the op.
print sess.run(c)
</code></pre>

<p>The error is always the same, CUDA_ERROR_OUT_OF_MEMORY:</p>

<pre><code>I tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcublas.so.7.0 locally
I tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcudnn.so.6.5 locally
I tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcufft.so.7.0 locally
I tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcuda.so locally
I tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcurand.so.7.0 locally
I tensorflow/core/common_runtime/local_device.cc:40] Local device intra op parallelism threads: 24
I tensorflow/core/common_runtime/gpu/gpu_init.cc:103] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 0000:0a:00.0
Total memory: 11.25GiB
Free memory: 105.73MiB
I tensorflow/core/common_runtime/gpu/gpu_init.cc:103] Found device 1 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 0000:0b:00.0
Total memory: 11.25GiB
Free memory: 133.48MiB
I tensorflow/core/common_runtime/gpu/gpu_init.cc:127] DMA: 0 1 
I tensorflow/core/common_runtime/gpu/gpu_init.cc:137] 0:   Y Y 
I tensorflow/core/common_runtime/gpu/gpu_init.cc:137] 1:   Y Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:702] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: Tesla K80, pci bus id: 0000:0a:00.0)
I tensorflow/core/common_runtime/gpu/gpu_device.cc:702] Creating TensorFlow device (/gpu:1) -&gt; (device: 1, name: Tesla K80, pci bus id: 0000:0b:00.0)
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Allocating 105.48MiB bytes.
E tensorflow/stream_executor/cuda/cuda_driver.cc:932] failed to allocate 105.48M (110608384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
F tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Check failed: gpu_mem != nullptr  Could not allocate GPU device memory for device 0. Tried to allocate 105.48MiB
Aborted (core dumped)
</code></pre>

<p>I guess that the problem has to do with my configuration rather than the memory usage of this tiny example. Does anyone have any idea? </p>

<p><strong>Edit:</strong></p>

<p>I've found out that the problem may be as simple as someone else running a job on the same GPU, which would explain the little amount of free memory. In that case: sorry for taking up your time...</p>
",
What does tf.nn.conv2d do in tensorflow?,"<p>I was looking at the docs of tensorflow about <code>tf.nn.conv2d</code> <a href=""https://www.tensorflow.org/api_docs/python/tf/nn/conv2d"" rel=""noreferrer"">here</a>. But I can't understand what it does or what it is trying to achieve. It says on the docs, </p>

<blockquote>
  <p>#1 : Flattens the filter to a 2-D matrix with shape </p>
  
  <p><code>[filter_height * filter_width * in_channels, output_channels]</code>.</p>
</blockquote>

<p>Now what does that do? Is that element-wise multiplication or just plain matrix multiplication? I also could not understand the other two points mentioned in the docs. I have written them below :</p>

<blockquote>
  <p># 2: Extracts image patches from the the input tensor to form a virtual tensor of shape </p>
  
  <p><code>[batch, out_height, out_width, filter_height * filter_width * in_channels]</code>.</p>
  
  <p># 3: For each patch, right-multiplies the filter matrix and the image patch vector.</p>
</blockquote>

<p>It would be really helpful if anyone could give an example, a piece of code (extremely helpful) maybe and explain what is going on there and why the operation is like this.</p>

<p>I've tried coding a small portion and printing out the shape of the operation. Still, I can't understand. </p>

<p>I tried something like this:</p>

<pre class=""lang-python prettyprint-override""><code>op = tf.shape(tf.nn.conv2d(tf.random_normal([1,10,10,10]), 
              tf.random_normal([2,10,10,10]), 
              strides=[1, 2, 2, 1], padding='SAME'))

with tf.Session() as sess:
    result = sess.run(op)
    print(result)
</code></pre>

<p>I understand bits and pieces of convolutional neural networks. I studied them <a href=""http://cs231n.github.io/convolutional-networks/"" rel=""noreferrer"">here</a>. But the implementation on tensorflow is not what I expected. So it raised the question.</p>

<p><strong>EDIT</strong>:
So, I implemented a much simpler code. But I can't figure out what's going on. I mean how the results are like this. It would be extremely helpful if anyone could tell me what process yields this output.</p>

<pre class=""lang-python prettyprint-override""><code>input = tf.Variable(tf.random_normal([1,2,2,1]))
filter = tf.Variable(tf.random_normal([1,1,1,1]))

op = tf.nn.conv2d(input, filter, strides=[1, 1, 1, 1], padding='SAME')
init = tf.initialize_all_variables()
with tf.Session() as sess:
    sess.run(init)

    print(""input"")
    print(input.eval())
    print(""filter"")
    print(filter.eval())
    print(""result"")
    result = sess.run(op)
    print(result)
</code></pre>

<p>output</p>

<pre><code>input
[[[[ 1.60314465]
   [-0.55022103]]

  [[ 0.00595062]
   [-0.69889867]]]]
filter
[[[[-0.59594476]]]]
result
[[[[-0.95538563]
   [ 0.32790133]]

  [[-0.00354624]
   [ 0.41650501]]]]
</code></pre>
",
How could I use Batch Normalization in TensorFlow?,"<p>I would like to use Batch Normalization in TensorFlow, since I found it in the source code <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/ops/nn_ops.cc"" rel=""noreferrer""><code>core/ops/nn_ops.cc</code></a>. However, I did not find it documented on tensorflow.org.</p>

<p>BN has different semantics in MLP and CNN, so I am not sure what exactly this BN does.</p>

<p>I <strong>did not</strong> find a method called <code>MovingMoments</code> either.</p>

<p>The C++ code is copied here for reference:</p>

<pre><code>REGISTER_OP(""BatchNormWithGlobalNormalization"")
    .Input(""t: T"")
    .Input(""m: T"")
    .Input(""v: T"")
    .Input(""beta: T"")
    .Input(""gamma: T"")
    .Output(""result: T"")
    .Attr(""T: numbertype"")
    .Attr(""variance_epsilon: float"")
    .Attr(""scale_after_normalization: bool"")
    .Doc(R""doc(
Batch normalization.

t: A 4D input Tensor.
m: A 1D mean Tensor with size matching the last dimension of t.
  This is the first output from MovingMoments.
v: A 1D variance Tensor with size matching the last dimension of t.
  This is the second output from MovingMoments.
beta: A 1D beta Tensor with size matching the last dimension of t.
  An offset to be added to the normalized tensor.
gamma: A 1D gamma Tensor with size matching the last dimension of t.
  If ""scale_after_normalization"" is true, this tensor will be multiplied
  with the normalized tensor.
variance_epsilon: A small float number to avoid dividing by 0.
scale_after_normalization: A bool indicating whether the resulted tensor
  needs to be multiplied with gamma.
)doc"");
</code></pre>
",
How to profile TensorFlow networks?,"<p>Is there a way to calculate the time cost for each node in a TensorFlow network?<br>
I find it hard to locate the performance bottlenecks.</p>

<p>EDIT: The <code>Timeline</code> profiler is really awesome (<a href=""https://stackoverflow.com/a/37774470/3632556"">https://stackoverflow.com/a/37774470/3632556</a>).</p>
",
Attach a queue to a numpy array in tensorflow for data fetch instead of files?,"<p>I have read the <a href=""https://www.tensorflow.org/versions/master/tutorials/deep_cnn/index.html"" rel=""noreferrer"">CNN Tutorial on the TensorFlow</a> and I am trying to use the same model for my project. 
The problem is now in data reading. I have around 25000 images for training and around 5000 for testing and validation each. The files are in png format and I can read them and convert them into the numpy.ndarray. </p>

<p>The CNN example in the tutorials use a queue to fetch the records from the file list provided. I tried to create my own such binary file by reshaping my images into 1-D array and attaching a label value in the front of it. So my data looks like this </p>

<pre><code>[[1,12,34,24,53,...,105,234,102],
 [12,112,43,24,52,...,115,244,98],
....
]
</code></pre>

<p>The single row of the above array is of length <strong>22501</strong> size where the first element is the label.</p>

<p>I dumped the file to using pickle and the tried to read from the file using the 
tf.FixedLengthRecordReader to read from the file as <a href=""https://tensorflow.googlesource.com/tensorflow/+/master/tensorflow/models/image/cifar10/cifar10_input.py#150"" rel=""noreferrer"">demonstrated in example</a></p>

<p>I am doing the same things as given in the <strong>cifar10_input.py</strong> to read the binary file and putting them into the record object.</p>

<p>Now when I read from the files the labels and the image values are different. I can understand the reason for this to be that pickle dumps the extra information of braces and brackets also in the binary file and they change the fixed length record size. </p>

<p>The above example uses the filenames and pass it to a queue to fetch the files and then the queue to read a single record from the file. </p>

<p>I want to know if I can pass the numpy array as defined above instead of the filenames to some reader and it can fetch records one by one from that array instead of the files.</p>
",
Choosing between GeForce or Quadro GPUs to do machine learning via TensorFlow,"<p>Is there any noticeable difference in TensorFlow performance if using Quadro GPUs vs GeForce GPUs? </p>

<p><strong>e.g. does it use double precision operations or something else that would cause a drop in GeForce cards?</strong></p>

<p>I am about to buy a GPU for TensorFlow, and wanted to know if a GeForce would be ok. Thanks and appreciate your help</p>
",
Tensor with unspecified dimension in tensorflow,"<p>I'm playing around with tensorflow and ran into a problem with the following code:</p>

<pre><code>def _init_parameters(self, input_data, labels):

    # the input shape is (batch_size, input_size)
    input_size = tf.shape(input_data)[1]

    # labels in one-hot format have shape (batch_size, num_classes)
    num_classes = tf.shape(labels)[1]

    stddev = 1.0 / tf.cast(input_size, tf.float32)

    w_shape = tf.pack([input_size, num_classes], 'w-shape')
    normal_dist = tf.truncated_normal(w_shape, stddev=stddev, name='normaldist')
    self.w = tf.Variable(normal_dist, name='weights')
</code></pre>

<p>(I'm using <code>tf.pack</code> as suggested in <a href=""https://stackoverflow.com/questions/33711427/tensorflow-initializing-tensor-of-ones"">this question</a>, since I was getting the same error)</p>

<p>When I run it (from a larger script that invokes this one), I get this error:</p>

<pre><code>ValueError: initial_value must have a shape specified: Tensor(""normaldist:0"", shape=TensorShape([Dimension(None), Dimension(None)]), dtype=float32)
</code></pre>

<p>I tried to replicate the process in the interactive shell. Indeed, the dimensions of <code>normal_dist</code> are unspecified, although the supplied values do exist:</p>

<pre><code>In [70]: input_size.eval()
Out[70]: 4

In [71]: num_classes.eval()
Out[71]: 3

In [72]: w_shape.eval()
Out[72]: array([4, 3], dtype=int32)

In [73]: normal_dist.eval()
Out[73]: 
array([[-0.27035281, -0.223277  ,  0.14694688],
       [-0.16527176,  0.02180306,  0.00807841],
       [ 0.22624688,  0.36425814, -0.03099642],
       [ 0.25575709, -0.02765726, -0.26169327]], dtype=float32)

In [78]: normal_dist.get_shape()
Out[78]: TensorShape([Dimension(None), Dimension(None)])
</code></pre>

<p>This is weird. Tensorflow generates the vector but can't say its shape. Am I doing something wrong?</p>
",
Difference between variable_scope and name_scope in TensorFlow,"<p>What is the difference between <code>variable_scope</code> and <code>name_scope</code>? The <a href=""https://www.tensorflow.org/programmers_guide/variable_scope#names_of_ops_in_tfvariable_scope"" rel=""noreferrer"">variable scope tutorial</a> talks about <code>variable_scope</code> implicitly opening <code>name_scope</code>. I also noticed that creating a variable in a <code>name_scope</code> automatically expands its name with the scope name as well. So what is the difference?</p>
",
How to assign value to a tensorflow variable?,"<p>I am trying to assign a new value to a tensorflow variable in python.</p>

<pre><code>import tensorflow as tf
import numpy as np

x = tf.Variable(0)
init = tf.initialize_all_variables()
sess = tf.InteractiveSession()
sess.run(init)

print(x.eval())

x.assign(1)
print(x.eval())
</code></pre>

<p>But the output I get is</p>

<pre><code>0
0
</code></pre>

<p>So the value has not changed. What am I missing?</p>
",
How do you make TensorFlow + Keras fast with a TFRecord dataset?,"<p><strong>What is an example of how to use a TensorFlow TFRecord with a Keras Model and tf.session.run() while keeping the dataset in tensors w/ queue runners?</strong></p>

<p>Below is a snippet that works but it needs the following improvements:</p>

<ul>
<li>Use the <a href=""https://keras.io/models/model/"" rel=""noreferrer"">Model API</a></li>
<li>specify an Input()</li>
<li>Load a dataset from a TFRecord</li>
<li>Run through a dataset in parallel (such as with a queuerunner)</li>
</ul>

<p>Here is the snippet, there are several TODO lines indicating what is needed:</p>

<pre><code>from keras.models import Model
import tensorflow as tf
from keras import backend as K
from keras.layers import Dense, Input
from keras.objectives import categorical_crossentropy
from tensorflow.examples.tutorials.mnist import input_data

sess = tf.Session()
K.set_session(sess)

# Can this be done more efficiently than placeholders w/ TFRecords?
img = tf.placeholder(tf.float32, shape=(None, 784))
labels = tf.placeholder(tf.float32, shape=(None, 10))

# TODO: Use Input() 
x = Dense(128, activation='relu')(img)
x = Dense(128, activation='relu')(x)
preds = Dense(10, activation='softmax')(x)
# TODO: Construct model = Model(input=inputs, output=preds)

loss = tf.reduce_mean(categorical_crossentropy(labels, preds))

# TODO: handle TFRecord data, is it the same?
mnist_data = input_data.read_data_sets('MNIST_data', one_hot=True)

train_step = tf.train.GradientDescentOptimizer(0.5).minimize(loss)

sess.run(tf.global_variables_initializer())

# TODO remove default, add queuerunner
with sess.as_default():
    for i in range(1000):
        batch = mnist_data.train.next_batch(50)
        train_step.run(feed_dict={img: batch[0],
                                  labels: batch[1]})
    print(loss.eval(feed_dict={img: mnist_data.test.images, labels: mnist_data.test.labels}))
</code></pre>

<p><strong>Why is this question relevant?</strong></p>

<ul>
<li>For high performance training without going back to python

<ul>
<li>no <a href=""https://stackoverflow.com/questions/36026892/how-can-i-convert-tfrecords-into-numpy-arrays"">TFRecord to numpy</a> to tensor conversions</li>
</ul></li>
<li><a href=""https://github.com/fchollet/keras/issues/5358"" rel=""noreferrer"">Keras will soon be part of tensorflow</a></li>
<li>Demonstrate how Keras Model() classes can accept tensors for input data correctly.</li>
</ul>

<p><strong>Here is some starter information for a semantic segmentation problem example:</strong></p>

<ul>
<li>example unet Keras model <a href=""https://github.com/ahundt/tf-image-segmentation/blob/cd7df53e6f59ef9e3dff1ed0119e12922ae98a3a/tf_image_segmentation/models/unet.py"" rel=""noreferrer"">unet.py</a>, happens to be for semantic segmentation.</li>
<li><a href=""https://blog.keras.io/keras-as-a-simplified-interface-to-tensorflow-tutorial.html"" rel=""noreferrer"">Keras + Tensorflow Blog Post</a></li>
<li>An <a href=""https://github.com/ahundt/tf-image-segmentation/blob/cd7df53e6f59ef9e3dff1ed0119e12922ae98a3a/tf_image_segmentation/recipes/pascal_voc/FCNs/densenet_fcn.py#L78"" rel=""noreferrer"">attempt at running the unet model a tf session with TFRecords and a Keras model</a> (not working)</li>
<li>Code to create the TFRecords: <a href=""https://github.com/ahundt/tf-image-segmentation/blob/cd7df53e6f59ef9e3dff1ed0119e12922ae98a3a/tf_image_segmentation/utils/tf_records.py"" rel=""noreferrer"">tf_records.py</a></li>
<li>An attempt at running the unet model a tf session with TFRecords and a Keras model is in <a href=""https://github.com/ahundt/tf-image-segmentation/blob/cd7df53e6f59ef9e3dff1ed0119e12922ae98a3a/tf_image_segmentation/recipes/pascal_voc/FCNs/densenet_fcn.py#L78"" rel=""noreferrer"">densenet_fcn.py</a> (not working)</li>
</ul>
",
Choosing from different cost function and activation function of a neural network,"<p>Recently I started toying with neural networks. I was trying to implement an <code>AND</code> gate with Tensorflow. I am having trouble understanding when to use different cost and activation functions. This is a basic neural network with only input and output layers, no hidden layers.</p>

<p>First I tried to implement it in this way. As you can see this is a poor implementation, but I think it gets the job done, at least in some way. So, I tried only the real outputs, no one hot true outputs. For activation functions, I used a sigmoid function and for cost function I used squared error cost function (I think its called that, correct me if I'm wrong). </p>

<p>I've tried using ReLU and Softmax as activation functions (with the same cost function) and it doesn't work. I figured out why they don't work. I also tried the sigmoid function with Cross Entropy cost function, it also doesn't work.</p>

<pre><code>import tensorflow as tf
import numpy

train_X = numpy.asarray([[0,0],[0,1],[1,0],[1,1]])
train_Y = numpy.asarray([[0],[0],[0],[1]])

x = tf.placeholder(""float"",[None, 2])
y = tf.placeholder(""float"",[None, 1])

W = tf.Variable(tf.zeros([2, 1]))
b = tf.Variable(tf.zeros([1, 1]))

activation = tf.nn.sigmoid(tf.matmul(x, W)+b)
cost = tf.reduce_sum(tf.square(activation - y))/4
optimizer = tf.train.GradientDescentOptimizer(.1).minimize(cost)

init = tf.initialize_all_variables()

with tf.Session() as sess:
    sess.run(init)
    for i in range(5000):
        train_data = sess.run(optimizer, feed_dict={x: train_X, y: train_Y})

    result = sess.run(activation, feed_dict={x:train_X})
    print(result)
</code></pre>

<p>after 5000 iterations:</p>

<pre><code>[[ 0.0031316 ]
[ 0.12012422]
[ 0.12012422]
[ 0.85576665]]
</code></pre>

<p><strong>Question 1</strong> - Is there any other activation function and cost function, that can work(learn) for the above network, without changing the parameters(meaning without changing W, x, b). </p>

<p><strong>Question 2</strong> - I read from a StackOverflow post <a href=""https://stackoverflow.com/questions/20850895/neural-networks-activation-function"">here</a>:</p>

<blockquote>
  <p>[Activation Function] selection depends on the problem.</p>
</blockquote>

<p>So there are no cost functions that can be used anywhere? I mean there is no <em>standard</em> cost function that can be used on any neural network. Right? Please correct me on this.</p>

<p><br/></p>

<p>I also implemented the <code>AND</code> gate with a different approach, with the output as one-hot true. As you can see the <code>train_Y</code> <code>[1,0]</code> means that the 0th index is 1, so the answer is 0. I hope you get it. </p>

<p>Here I have used a softmax activation function, with cross entropy as cost function. Sigmoid function as activation function fails miserably. </p>

<pre><code>import tensorflow as tf
import numpy

train_X = numpy.asarray([[0,0],[0,1],[1,0],[1,1]])
train_Y = numpy.asarray([[1,0],[1,0],[1,0],[0,1]])

x = tf.placeholder(""float"",[None, 2])
y = tf.placeholder(""float"",[None, 2])

W = tf.Variable(tf.zeros([2, 2]))
b = tf.Variable(tf.zeros([2]))

activation = tf.nn.softmax(tf.matmul(x, W)+b)

cost = -tf.reduce_sum(y*tf.log(activation))

optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(cost)

init = tf.initialize_all_variables()

with tf.Session() as sess:
    sess.run(init)
    for i in range(5000):
        train_data = sess.run(optimizer, feed_dict={x: train_X, y: train_Y})

    result = sess.run(activation, feed_dict={x:train_X})
    print(result)
</code></pre>

<p>after 5000 iteration</p>

<pre><code>[[  1.00000000e+00   1.41971401e-09]
 [  9.98996437e-01   1.00352429e-03]
 [  9.98996437e-01   1.00352429e-03]
 [  1.40495342e-03   9.98595059e-01]]
</code></pre>

<p><strong>Question 3</strong> So in this case what cost function and activation function can I use? How do I understand what type of cost and activation functions I should use? Is there a standard way or rule, or just experience only? Should I have to try every cost and activation function in a brute force manner? I found an answer <a href=""https://stackoverflow.com/questions/20368015/activation-function-choice-for-neural-network"">here</a>. But I am hoping for a more elaborate explanation. </p>

<p><strong>Question 4</strong> I have noticed that it takes many iterations to converge to a near accurate prediction. I think the convergance rate depends on the learning rate (using too large of will miss the solution) and the cost function (correct me if I'm wrong). So, is there any optimal way (meaning the fastest) or cost function for converging to a correct solution?</p>
",
Is it possible to modify an existing TensorFlow computation graph?,"<p>TensorFlow graph is usually built gradually from inputs to outputs, and then executed. Looking at the Python code, the inputs lists of operations are immutable which suggests that the inputs should not be modified. Does that mean that there is no way to update/modify an existing graph?</p>
",
What is the purpose of graph collections in TensorFlow?,"<p>The API discusses <a href=""https://www.tensorflow.org/versions/master/api_docs/python/framework.html#graph-collections"">Graph Collections</a> which judging from the <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/ops.py#L2071"">code</a> are a general purpose key/data storage. What is the purpose of those collections?</p>
",
Difference between np.mean and tf.reduce_mean in Numpy and Tensorflow?,"<p>In the <a href=""https://www.tensorflow.org/versions/master/tutorials/mnist/beginners/index.html"" rel=""noreferrer"">MNIST beginner tutorial</a>, there is <code>accuracy = tf.reduce_mean(tf.cast(correct_prediction, ""float""))</code> </p>

<p><code>tf.cast</code> basically changes the type of tensor the object is, but what is the difference between <a href=""https://www.tensorflow.org/api_docs/python/tf/reduce_mean"" rel=""noreferrer""><code>tf.reduce_mean</code></a> and <a href=""https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.mean.html"" rel=""noreferrer""><code>np.mean</code></a>? </p>

<p>Here is the doc on <a href=""https://www.tensorflow.org/api_docs/python/tf/reduce_mean"" rel=""noreferrer""><code>tf.reduce_mean</code></a>:</p>

<pre><code>reduce_mean(input_tensor, reduction_indices=None, keep_dims=False, name=None)
input_tensor: The tensor to reduce. Should have numeric type.
reduction_indices: The dimensions to reduce. If `None` (the defaut),
    reduces all dimensions.

# 'x' is [[1., 1. ]]
#         [2., 2.]]
tf.reduce_mean(x) ==&gt; 1.5
tf.reduce_mean(x, 0) ==&gt; [1.5, 1.5]
tf.reduce_mean(x, 1) ==&gt; [1.,  2.]
</code></pre>

<p>For a 1D vector, it looks like <code>np.mean == tf.reduce_mean</code> but I don't understand what's happening in <code>tf.reduce_mean(x, 1) ==&gt; [1.,  2.]</code>. <code>tf.reduce_mean(x, 0) ==&gt; [1.5, 1.5]</code> kind of makes sense, since mean of [1,2] and [1,2] are [1.5,1.5] but what's going on with <code>tf.reduce_mean(x,1)</code>?</p>
",
What's the difference between softmax and softmax_cross_entropy_with_logits?,"<p>I was going through the tensorflow API docs <a href=""https://www.tensorflow.org/versions/master/api_docs/python/nn.html#softmax"" rel=""noreferrer"">here</a>. In the tensorflow documentation, they used a keyword called <code>logits</code>. What is it? In a lot of methods in the API docs it is written like</p>

<pre><code>tf.nn.softmax(logits, name=None)
</code></pre>

<p>If what is written is those <code>logits</code> are only <code>Tensors</code>, why keeping a different name like <code>logits</code>? </p>

<p>Another thing is that there are two methods I could not differentiate. They were</p>

<pre><code>tf.nn.softmax(logits, name=None)
tf.nn.softmax_cross_entropy_with_logits(logits, labels, name=None)
</code></pre>

<p>What are the differences between them? The docs are not clear to me. I know what <code>tf.nn.softmax</code> does. But not the other. An example will be really helpful.</p>
",
How to use image_summary to view images from different batches in Tensorflow?,"<p>I am curious about how image_summary works. There is a parameter called max_images, which controls how many images would be shown. However it seems the summary only displays images from one batch. If we use bigger value of max_iamges, we will just view more images from the batch. Is there a way I can view for example one image from each batch?</p>
",
Tensorflow: How to get all variables from rnn_cell.BasicLSTM & rnn_cell.MultiRNNCell,"<p>I have a setup where I need to initialize an LSTM after the main initialization which uses <code>tf.initialize_all_variables()</code>. I.e. I want to call <code>tf.initialize_variables([var_list])</code> </p>

<p>Is there way to collect all the internal trainable variables for both:</p>

<ul>
<li>rnn_cell.BasicLSTM</li>
<li>rnn_cell.MultiRNNCell</li>
</ul>

<p>so that I can initialize <strong>JUST</strong> these parameters?</p>

<p>The main reason I want this is because I do not want to re-initialize some trained values from earlier.</p>
",
TensorFlow create dataset from numpy array,"<p>TensorFlow as build it a nice way to store data. This is for example used to store the MNIST data in the example:</p>

<pre><code>&gt;&gt;&gt; mnist
&lt;tensorflow.examples.tutorials.mnist.input_data.read_data_sets.&lt;locals&gt;.DataSets object at 0x10f930630&gt;
</code></pre>

<p>Suppose to have a input and output numpy arrays. </p>

<pre><code>&gt;&gt;&gt; x = np.random.normal(0,1, (100, 10))
&gt;&gt;&gt; y = np.random.randint(0, 2, 100)
</code></pre>

<p>How can I transform them in a <code>tf</code> dataset? </p>

<p>I want to use functions like <code>next_batch</code></p>
",
Understanding TensorBoard (weight) histograms,"<p>It is really straightforward to see and understand the scalar values in TensorBoard. However, it's not clear how to understand histogram graphs. </p>

<p>For example, they are the histograms of my network weights.</p>

<p><a href=""https://i.stack.imgur.com/IttNH.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/RVOA5.jpg"" alt=""enter image description here""></a></p>

<p>(After fixing a bug thanks to sunside)
<a href=""https://i.stack.imgur.com/IttNH.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/IttNH.jpg"" alt=""enter image description here""></a>
What is the best way to interpret these? Layer 1 weights look mostly flat, what does this mean?</p>

<p>I added the network construction code here.</p>

<pre><code>X = tf.placeholder(tf.float32, [None, input_size], name=""input_x"")
x_image = tf.reshape(X, [-1, 6, 10, 1])
tf.summary.image('input', x_image, 4)

# First layer of weights
with tf.name_scope(""layer1""):
    W1 = tf.get_variable(""W1"", shape=[input_size, hidden_layer_neurons],
                         initializer=tf.contrib.layers.xavier_initializer())
    layer1 = tf.matmul(X, W1)
    layer1_act = tf.nn.tanh(layer1)
    tf.summary.histogram(""weights"", W1)
    tf.summary.histogram(""layer"", layer1)
    tf.summary.histogram(""activations"", layer1_act)

# Second layer of weights
with tf.name_scope(""layer2""):
    W2 = tf.get_variable(""W2"", shape=[hidden_layer_neurons, hidden_layer_neurons],
                         initializer=tf.contrib.layers.xavier_initializer())
    layer2 = tf.matmul(layer1_act, W2)
    layer2_act = tf.nn.tanh(layer2)
    tf.summary.histogram(""weights"", W2)
    tf.summary.histogram(""layer"", layer2)
    tf.summary.histogram(""activations"", layer2_act)

# Third layer of weights
with tf.name_scope(""layer3""):
    W3 = tf.get_variable(""W3"", shape=[hidden_layer_neurons, hidden_layer_neurons],
                         initializer=tf.contrib.layers.xavier_initializer())
    layer3 = tf.matmul(layer2_act, W3)
    layer3_act = tf.nn.tanh(layer3)

    tf.summary.histogram(""weights"", W3)
    tf.summary.histogram(""layer"", layer3)
    tf.summary.histogram(""activations"", layer3_act)

# Fourth layer of weights
with tf.name_scope(""layer4""):
    W4 = tf.get_variable(""W4"", shape=[hidden_layer_neurons, output_size],
                         initializer=tf.contrib.layers.xavier_initializer())
    Qpred = tf.nn.softmax(tf.matmul(layer3_act, W4)) # Bug fixed: Qpred = tf.nn.softmax(tf.matmul(layer3, W4))
    tf.summary.histogram(""weights"", W4)
    tf.summary.histogram(""Qpred"", Qpred)

# We need to define the parts of the network needed for learning a policy
Y = tf.placeholder(tf.float32, [None, output_size], name=""input_y"")
advantages = tf.placeholder(tf.float32, name=""reward_signal"")

# Loss function
# Sum (Ai*logp(yi|xi))
log_lik = -Y * tf.log(Qpred)
loss = tf.reduce_mean(tf.reduce_sum(log_lik * advantages, axis=1))
tf.summary.scalar(""Q"", tf.reduce_mean(Qpred))
tf.summary.scalar(""Y"", tf.reduce_mean(Y))
tf.summary.scalar(""log_likelihood"", tf.reduce_mean(log_lik))
tf.summary.scalar(""loss"", loss)

# Learning
train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)
</code></pre>
",
Loading a trained Keras model and continue training,"<p>I was wondering if it was possible to save a partly trained Keras model and continue the training after loading the model again.</p>

<p>The reason for this is that I will have more training data in the future and I do not want to retrain the whole model again.</p>

<p>The functions which I am using are:</p>

<pre><code>#Partly train model
model.fit(first_training, first_classes, batch_size=32, nb_epoch=20)

#Save partly trained model
model.save('partly_trained.h5')

#Load partly trained model
from keras.models import load_model
model = load_model('partly_trained.h5')

#Continue training
model.fit(second_training, second_classes, batch_size=32, nb_epoch=20)
</code></pre>

<hr>

<p><strong>Edit 1: added fully working example</strong></p>

<p>With the first dataset after 10 epochs the loss of the last epoch will be 0.0748 and the accuracy 0.9863.</p>

<p>After saving, deleting and reloading the model the loss and accuracy of the model trained on the second dataset will be 0.1711 and 0.9504 respectively.</p>

<p>Is this caused by the new training data or by a completely re-trained model?</p>

<pre><code>""""""
Model by: http://machinelearningmastery.com/
""""""
# load (downloaded if needed) the MNIST dataset
import numpy
from keras.datasets import mnist
from keras.models import Sequential
from keras.layers import Dense
from keras.utils import np_utils
from keras.models import load_model
numpy.random.seed(7)

def baseline_model():
    model = Sequential()
    model.add(Dense(num_pixels, input_dim=num_pixels, init='normal', activation='relu'))
    model.add(Dense(num_classes, init='normal', activation='softmax'))
    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
    return model

if __name__ == '__main__':
    # load data
    (X_train, y_train), (X_test, y_test) = mnist.load_data()

    # flatten 28*28 images to a 784 vector for each image
    num_pixels = X_train.shape[1] * X_train.shape[2]
    X_train = X_train.reshape(X_train.shape[0], num_pixels).astype('float32')
    X_test = X_test.reshape(X_test.shape[0], num_pixels).astype('float32')
    # normalize inputs from 0-255 to 0-1
    X_train = X_train / 255
    X_test = X_test / 255
    # one hot encode outputs
    y_train = np_utils.to_categorical(y_train)
    y_test = np_utils.to_categorical(y_test)
    num_classes = y_test.shape[1]

    # build the model
    model = baseline_model()

    #Partly train model
    dataset1_x = X_train[:3000]
    dataset1_y = y_train[:3000]
    model.fit(dataset1_x, dataset1_y, nb_epoch=10, batch_size=200, verbose=2)

    # Final evaluation of the model
    scores = model.evaluate(X_test, y_test, verbose=0)
    print(""Baseline Error: %.2f%%"" % (100-scores[1]*100))

    #Save partly trained model
    model.save('partly_trained.h5')
    del model

    #Reload model
    model = load_model('partly_trained.h5')

    #Continue training
    dataset2_x = X_train[3000:]
    dataset2_y = y_train[3000:]
    model.fit(dataset2_x, dataset2_y, nb_epoch=10, batch_size=200, verbose=2)
    scores = model.evaluate(X_test, y_test, verbose=0)
    print(""Baseline Error: %.2f%%"" % (100-scores[1]*100))
</code></pre>
",
"tensorflow on GPU: no known devices, despite cuda's deviceQuery returning a ""PASS"" result","<blockquote>
  <p>Note : this question was initially <a href=""https://github.com/tensorflow/tensorflow/issues/7648#issuecomment-280866214"" rel=""noreferrer"">asked on github</a>, but it was asked to be here instead</p>
</blockquote>

<p>I'm having trouble running tensorflow on gpu, and it does not seems to be the usual cuda's configuration problem, because everything seems to indicate cuda is properly setup.</p>

<p>The main symptom: when running tensorflow, my gpu is not detected (<a href=""https://gist.github.com/oelmekki/cafda411bf5c2ea695d984fa98e0995b"" rel=""noreferrer"">the code being run</a>, and <a href=""https://gist.github.com/oelmekki/77235c6b0dde99b3438f190eb557f40f"" rel=""noreferrer"">its output</a>).</p>

<p>What differs from usual issues is that cuda seems properly installed and running <code>./deviceQuery</code> from cuda samples is successful (<a href=""https://gist.github.com/oelmekki/fe65a15daec45aa90ec33b10b51d3aae"" rel=""noreferrer"">output</a>).</p>

<p>I have two graphical cards:</p>

<ul>
<li>an old GTX 650 used for my monitors (I don't want to use that one with tensorflow)</li>
<li>a GTX 1060 that I want to dedicate to tensorflow</li>
</ul>

<p>I use:</p>

<ul>
<li><a href=""https://pypi.python.org/pypi/tensorflow"" rel=""noreferrer"">tensorflow-1.0.0</a></li>
<li>cuda-8.0 (<a href=""https://gist.github.com/oelmekki/6e5e9d7d1ea871e1d73efae307efe9ce"" rel=""noreferrer"">ls -l /usr/local/cuda/lib64/libcud*</a>)</li>
<li>cudnn-5.1.10</li>
<li>python-2.7.12</li>
<li>nvidia-drivers-375.26 (this was installed by cuda and replaced my distro driver package)</li>
</ul>

<p>I've tried:</p>

<ul>
<li>adding <code>/usr/local/cuda/bin/</code> to <code>$PATH</code></li>
<li>forcing gpu placement in tensorflow script using <code>with tf.device('/gpu:1'):</code> (and <code>with tf.device('/gpu:0'):</code> when it failed, for good measure)</li>
<li>whitelisting the gpu I wanted to use with <code>CUDA_VISIBLE_DEVICES</code>, in case the presence of my old unsupported card did cause problems</li>
<li>running the script with sudo (because why not)</li>
</ul>

<p>Here are the outputs of <a href=""https://gist.github.com/oelmekki/7bdcb5cc2f791cea561a60f8b21e87b5"" rel=""noreferrer"">nvidia-smi</a> and <a href=""https://gist.github.com/oelmekki/b83a5a0a72e8924aeb44b70b3598f9b4"" rel=""noreferrer"">nvidia-debugdump -l</a>, in case it's useful.</p>

<p>At this point, I feel like I have followed all the breadcrumbs and have no idea what I could try else. I'm not even sure if I'm contemplating a bug or a configuration problem. Any advice about how to debug this would be greatly appreciated. Thanks!</p>

<p><strong>Update</strong>: with the help of Yaroslav on github, I gathered more debugging info by raising log level, but it doesn't seem to say much about the device selection : <a href=""https://gist.github.com/oelmekki/760a37ca50bf58d4f03f46d104b798bb"" rel=""noreferrer"">https://gist.github.com/oelmekki/760a37ca50bf58d4f03f46d104b798bb</a></p>

<p><strong>Update 2</strong>: Using theano detects gpu correctly, but interestingly it complains about cuDNN being too recent, then fallback to cpu (<a href=""https://gist.github.com/oelmekki/34b6e41a0ff2b17ff9f39bcf56d0635a"" rel=""noreferrer"">code ran</a>, <a href=""https://gist.github.com/oelmekki/11626d6b34058337dae64f1915e5a9fe"" rel=""noreferrer"">output</a>). Maybe that could be the problem with tensorflow as well?</p>
",
Tensorflow installation using SSE instructions with pip,"<p>I successfully installed cpu only tensorflow on ubuntu 16.04 using the default instructions provided <a href=""https://www.tensorflow.org/install/install_linux#installing_with_virtualenv"" rel=""noreferrer"">here</a>. The instructions recommended using virtualenv and pip so I did not build from source. I had no problems installing with these instructions.</p>

<p>I validated my installation using the instructions provided <a href=""https://www.tensorflow.org/install/install_linux#validate_your_installation"" rel=""noreferrer"">further down</a> on the same page, and while the program ran successfully, it output the following warnings.</p>

<pre><code>W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
Hello, TensorFlow!
</code></pre>

<p>all the stackoverflow pages i've looked at only address people who have built from source using bazel, but don't seem to apply to people who used pip. </p>

<p>How do I recompile or update my installation so as to the SSE instructions?</p>
",
TensorFlow error: logits and labels must be same size,"<p>I've been trying to learn TensorFlow by implementing ApproximatelyAlexNet based on various examples on the internet. Basically extending the AlexNet example <a href=""https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3%20-%20Neural%20Networks/alexnet.py"" rel=""noreferrer"">here</a> to take 224x224 RGB images (rather than 28x28 grayscale images), and adding a couple more layers, changing kernel sizes, strides, etc, per other AlexNet implementations I've found online.</p>

<p>Have worked through a number of mismatched shape type errors, but this one has me stumped:</p>

<pre><code>tensorflow.python.framework.errors.InvalidArgumentError: logits and labels must be same size: logits_size=dim { size: 49 } dim { size: 10 } labels_size=dim { size: 1 } dim { size: 10 }
     [[Node: SoftmaxCrossEntropyWithLogits = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/gpu:0""](Softmax, _recv_Placeholder_1_0/_13)]]
     [[Node: gradients/Mean_grad/range_1/_17 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_457_gradients/Mean_grad/range_1"", tensor_type=DT_INT32, _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]
</code></pre>

<p>The 49 dimension is particularly puzzling. For debugging, my batch size is currently 1, if I increase it to 2 then the 49 becomes 98.</p>

<p>If I log the shape of the x and y that I pass to </p>

<pre><code>sess.run(optimizer, feed_dict={x: batchImages, y: batchLabels, keepProb: P_DROPOUT})
</code></pre>

<p>I get</p>

<pre><code>x shape: (1, 150528)
y shape: (1, 10)
</code></pre>

<p>Which is as expected: 150528 = 224 * 224 RGB pixels, and a one-hot vector representing my labels.</p>

<p>Would appreciate any help in figuring this out!</p>

<p><strong>Update:</strong> code exhibiting the fault here:</p>

<p><a href=""https://gist.github.com/j4m3z0r/e70096d0f7bd4bd24c42"" rel=""noreferrer"">https://gist.github.com/j4m3z0r/e70096d0f7bd4bd24c42</a></p>
",
Tensorflow Strides Argument,"<p>I am trying to understand the <strong>strides</strong> argument in tf.nn.avg_pool, tf.nn.max_pool, tf.nn.conv2d. </p>

<p>The <a href=""https://www.tensorflow.org/versions/master/api_docs/python/nn.html#max_pool"" rel=""noreferrer"">documentation</a> repeatedly says </p>

<blockquote>
  <p>strides: A list of ints that has length >= 4. The stride of the sliding window for each dimension of the input tensor.</p>
</blockquote>

<p>My questions are:</p>

<ol>
<li>What do each of the 4+ integers represent?</li>
<li>Why must they have strides[0] = strides[3] = 1 for convnets?</li>
<li>In <a href=""https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/3%20-%20Neural%20Networks/convolutional_network.ipynb"" rel=""noreferrer"">this example</a> we see <code>tf.reshape(_X,shape=[-1, 28, 28, 1])</code>. Why -1?</li>
</ol>

<p>Sadly the examples in the docs for reshape using -1 don't translate too well to this scenario.</p>
",
"In Tensorflow, what is the difference between sampled_softmax_loss and softmax_cross_entropy_with_logits","<p>In tensorflow, there are methods called <a href=""https://www.tensorflow.org/versions/master/api_docs/python/nn.html#softmax_cross_entropy_with_logits"">softmax_cross_entropy_with_logits</a> and <a href=""https://www.tensorflow.org/versions/master/api_docs/python/nn.html#sampled_softmax_loss"">sampled_softmax_loss</a>.</p>

<p>I read the tensorflow document and searched google for more information but I can't find the difference. It looks like to me both calculates the loss using softmax function.</p>

<h3>Using sampled_softmax_loss to calculate the loss</h3>

<p><code>loss = tf.reduce_mean(tf.nn.sampled_softmax_loss(...))</code></p>

<h3>Using softmax_cross_entropy_with_logits to calculate the loss</h3>

<p><code>loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(P, Q))</code></p>

<p>To me, calculating softmax loss is same as calculating softmaxed cross entropy (e.g. cross_entropy(softmax(train_x)))</p>

<p>Could somebody tell me the why there is two different methods and which method should I use in which case?</p>

<p>Thank you.</p>
",
What does tf.nn.embedding_lookup function do?,"<pre><code>tf.nn.embedding_lookup(params, ids, partition_strategy='mod', name=None)
</code></pre>

<p>I cannot understand the duty of this function. Is it like a lookup table? Which means to return the parameters corresponding to each id (in ids)?</p>

<p>For instance, in the <code>skip-gram</code> model if we use <code>tf.nn.embedding_lookup(embeddings, train_inputs)</code>, then for each <code>train_input</code> it finds the correspond embedding?</p>
",
How do you actually apply a trained model?,"<p>I've been slowly going through the tensorflow tutorials, and I assume I will have to again. I don't have a background in ML but am slowly pushing my way up. </p>

<p>Anyway, after reading through the <a href=""https://www.tensorflow.org/versions/master/tutorials/recurrent/index.html"" rel=""noreferrer"">RNN tutorial</a> and running the training code, I am confused. </p>

<p>How does one actually apply the trained model so that it can be used to make language predictions?</p>

<p>I know this is a terrible noobish and simple question, but I believe it will be of use to others, as I have seen it asked and not answered in a satisfactory way.</p>
",
tf.nn.conv2d vs tf.layers.conv2d,"<p>Is there any advantage in using <code>tf.nn.*</code> over <code>tf.layers.*</code>?</p>

<p>Most of the examples in the doc use <code>tf.nn.conv2d</code>, for instance, but it is not clear why they do so.</p>
",
How to write a custom loss function in Tensorflow?,"<p>I am new to <code>tensorflow</code>. I want to write my own custom loss function. Is there any tutorial about this? For example, the hinge loss or a sum_of_square_loss(though this is already in tf)?
Can I do it directly in python or I have to write the cpp code?</p>
",
"In TensorFlow, what is tf.identity used for?","<p>I've seen <code>tf.identity</code> used in a few places, such as the official CIFAR-10 tutorial and the batch-normalization implementation on stackoverflow, but I don't see why it's necessary.</p>

<p>What's it used for? Can anyone give a use case or two?</p>

<p>One proposed answer is that it can be used for transfer between the CPU and GPU. This is not clear to me. Extension to the question, based on <a href=""https://tensorflow.googlesource.com/tensorflow/+/master/tensorflow/models/image/cifar10/cifar10_multi_gpu_train.py"" rel=""noreferrer"">this</a>: <code>loss = tower_loss(scope)</code> is under the GPU block, which suggests to me that all operators defined in <code>tower_loss</code> are mapped to the GPU. Then, at the end of <code>tower_loss</code>, we see <code>total_loss = tf.identity(total_loss)</code> before it's returned. Why? What would be the flaw with not using <code>tf.identity</code> here?</p>
",
Running a Tensorflow model on Android,"<p>I'm trying to figure out the workflow for training and deploying a Tensorflow model on Android.  I'm aware of the other questions similar to this one on StackOverflow, but none of them seem to address the problems I've run into.  </p>

<p>After studying the Android example from the Tensorflow repository, this is what I think the workflow should be:</p>

<ol>
<li>Build and train Tensorflow model in Python.</li>
<li>Create a new graph, and transfer all relevant nodes (i.e. not the nodes responsible for training) to this new graph.  Trained weight variables are imported as constants so that the C++ API can read them.</li>
<li>Develop Android GUI in Java, using the native keyword to stub out a call to the Tensorflow model.</li>
<li>Run javah to generate the C/C++ stub code for the Tensorflow native call.</li>
<li>Fill in the stub by using the Tensorflow C++ API to read in and access the trained/serialized model.</li>
<li>Use Bazel to build BOTH the Java app, the native Tensorflow interface (as a .so file), and generate the APK.</li>
<li><p>Use adb to deploy the APK.</p>

<p>Step 6 is the problem.  Bazel will happily compile a native (to OSX) .dylib that I can call from Java via the JNI.  Android Studio, likewise, will generate a whole bunch of XML code that makes the GUI I want.  However, Bazel wants all of the java app code to be inside the 'WORKSPACE' top-level directory (in the Tensorflow repo), and Android Studio immediately links in all sorts of external libraries from the SDK to make GUIs (I know because my Bazel compile run fails when it can't find these resources).  The only way I can find to force Bazel to cross-compile a .so file is by making it a dependent rule of an Android rule.  Directly cross-compiling a native lib is what I'd prefer to porting my A.S. code to a Bazel project.</p>

<p>How do I square this?  Bazel will supposedly compile Android code, but Android Studio generates code that Bazel can't compile.  All the examples from Google simply give you code from a repo without any clue as to how it was generated.  As far as I know, the XML that's part of an Android Studio app is supposed to be generated, not made by hand.  If it can be made by hand, how do I avoid the need for all those external libraries? </p>

<p>Maybe I'm getting the workflow wrong, or there's some aspect of Bazel/Android Studio that I'm not understanding.  Any help appreciated.  </p></li>
</ol>

<p>Thanks!</p>

<p><strong>Edit:</strong></p>

<p>There were several things that I ended up doing that might have contributed to the library building successfully:</p>

<ol>
<li>I upgraded to the latest Bazel.</li>
<li>I rebuilt TensorFlow from source.</li>
<li><p>I implemented the recommended Bazel BUILD file below, with a few additions (taken from the Android example):</p>

<pre><code>cc_binary(
name = ""libName.so"",
srcs = [""org_tensorflowtest_MyActivity.cc"", 
        ""org_tensorflowtest_MyActivity.h"",
        ""jni.h"",
        ""jni_md.h"",
        "":libpthread.so""],
deps = [""//tensorflow/core:android_tensorflow_lib"",
        ],
copts = [
    ""-std=c++11"",
    ""-mfpu=neon"",
    ""-O2"",
],
linkopts = [""-llog -landroid -lm""],
linkstatic = 1,
linkshared = 1,
)

cc_binary(
     name = ""libpthread.so"",
     srcs = [],
     linkopts = [""-shared""],
     tags = [
         ""manual"",
         ""notap"",
     ],
)
</code></pre></li>
</ol>

<p>I haven't verified that this library can be loaded and used in Android yet; Android Studio 1.5 seems to be very finicky about acknowledging the presence of native libs.</p>
",
steps vs num_epochs in tensorflow getting started tutorial,"<p>I am going through TensorFlow <a href=""https://www.tensorflow.org/get_started/get_started"" rel=""noreferrer"">get started tutorial</a>. In the <code>tf.contrib.learn</code> example, these are two lines of code:</p>

<pre><code>input_fn = tf.contrib.learn.io.numpy_input_fn({""x"":x}, y, batch_size=4, num_epochs=1000)
estimator.fit(input_fn=input_fn, steps=1000)
</code></pre>

<p>I am wondering what is the difference between argument <code>steps</code> in the call to <code>fit</code> function and <code>num_epochs</code> in the <code>numpy_input_fn</code> call. Shouldn't there be just one argument? How are they connected? </p>

<p>I have found that code is somehow taking the <code>min</code> of these two as the number of steps in the toy example of the tutorial.</p>

<hr>

<h3>Edit</h3>

<p>Thanks for all the answers. IMHO, at least, one of the two parameters either <code>num_epochs</code> or <code>steps</code> has to be redundant. We can calculate one from the other. Is there a way I can know how many steps (number of times parameters get updated) my algorithm actually took? </p>

<p>I am curious which one takes precedence. And does it depend on some other parameters?</p>
",
How to create dataset similar to cifar-10,"<p>I want to create a dataset that has the same format as the cifar-10 data set to use with Tensorflow. It should have images and labels. Basically, I'd like to be able to take the cifar-10 code but different images and labels, and run that code. I haven't found any information on how to do this online, and am completely new to machine learning.</p>
",
Tensorflow exception with matmul,"<p>I'm a total beginner to TensorFlow, and I'm trying to multiply two matrices together, but I keep getting an exception that says: </p>

<p>ValueError: Shapes TensorShape([Dimension(2)]) and TensorShape([Dimension(None), Dimension(None)]) must have the same rank</p>

<p>Here's minimal example code:</p>

<pre><code>data = np.array([0.1, 0.2])
x = tf.placeholder(""float"", shape=[2])
T1 = tf.Variable(tf.ones([2,2]))
l1 = tf.matmul(T1, x)
init = tf.initialize_all_variables()

with tf.Session() as sess:
    sess.run(init)
    sess.run(feed_dict={x: data}
</code></pre>

<p>Confusingly, the following very similar code works fine:</p>

<pre><code>data = np.array([0.1, 0.2])
x = tf.placeholder(""float"", shape=[2])
T1 = tf.Variable(tf.ones([2,2]))
init = tf.initialize_all_variables()

with tf.Session() as sess:
    sess.run(init)
    sess.run(T1*x, feed_dict={x: data}
</code></pre>

<p>Can anyone point to what the issue is? I must be missing something obvious here..</p>
",
How to deploy and serve prediction using TensorFlow from API?,"<p>From google tutorial we know how to train a model in TensorFlow. But what is the best way to save a trained model, then serve the prediction using a basic minimal python api in production server.</p>

<p>My question is basically for TensorFlow best practices to save the model and serve prediction on live server without compromising speed and memory issue. Since the API server will be running on the background for forever.</p>

<p>A small snippet of python code will be appreciated. </p>
",
Training on imbalanced data using TensorFlow,"<p><strong>The Situation:</strong></p>

<p>I am wondering how to use TensorFlow optimally when my training data is imbalanced in label distribution between 2 labels. For instance, suppose the <a href=""https://www.tensorflow.org/versions/master/tutorials/mnist/beginners/index.html"" rel=""noreferrer"">MNIST tutorial</a> is simplified to only distinguish between 1's and 0's, where all images available to us are either 1's or 0's. This is straightforward to train using the provided TensorFlow tutorials when we have roughly 50% of each type of image to train and test on. But what about the case where 90% of the images available in our data are 0's and only 10% are 1's? I observe that in this case, TensorFlow routinely predicts my entire test set to be 0's, achieving an accuracy of a meaningless 90%.</p>

<p>One strategy I have used to some success is to pick random batches for training that do have an even distribution of 0's and 1's. This approach ensures that I can still use all of my training data and produced decent results, with less than 90% accuracy, but a much more useful classifier. Since accuracy is somewhat useless to me in this case, my metric of choice is typically area under the ROC curve (AUROC), and this produces a result respectably higher than .50.</p>

<p><strong>Questions:</strong></p>

<p>(1) Is the strategy I have described an accepted or optimal way of training on imbalanced data, or is there one that might work better?</p>

<p>(2) Since the accuracy metric is not as useful in the case of imbalanced data, is there another metric that can be maximized by altering the cost function? I can certainly calculate AUROC post-training, but can I train in such a way as to maximize AUROC?</p>

<p>(3) Is there some other alteration I can make to my cost function to improve my results for imbalanced data? Currently, I am using a default suggestion given in TensorFlow tutorials:</p>

<pre><code>cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y))
optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)
</code></pre>

<p>I have heard this may be possible by up-weighting the cost of miscategorizing the smaller label class, but I am unsure of how to do this.</p>
",
What is SYCL 1.2?,"<p>I am trying to install tensorflow</p>

<pre><code>Please specify the location where ComputeCpp for SYCL 1.2 is installed. [Default is /usr/local/computecpp]: 
Invalid SYCL 1.2 library path. /usr/local/computecpp/lib/libComputeCpp.so cannot be found
</code></pre>

<p>What should I do?What is SYCL 1.2?</p>
",
Tensorflow error using my own data,"<p>I've been playing with the Tensorflow library doing the tutorials. Now I wanted to play with my own data, but I fail horribly. This is perhaps a noob question but I can't figure it out.</p>

<p>I'm using this example: <a href=""https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3%20-%20Neural%20Networks/convolutional_network.py"">https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3%20-%20Neural%20Networks/convolutional_network.py</a></p>

<p>I want to use my own images, for converting my images to use with tensorflow i'm using this: <a href=""https://github.com/HamedMP/ImageFlow/blob/master/ImageFlow.py"">https://github.com/HamedMP/ImageFlow/blob/master/ImageFlow.py</a></p>

<p>Now I change the parameters in the example from this:</p>

<pre><code> n_input = 784
 n_classes = 10
</code></pre>

<p>to this:</p>

<pre><code> n_input = 9216
 n_classes = 2
</code></pre>

<p>I did that because my images are 96 * 96 and there are only 2 classes of my images</p>

<p>I also change the weights and biases to the numbers I need.</p>

<p>I read the data like this:</p>

<pre><code>batch_xs = imgReader.read_images(pathname);
</code></pre>

<p>imgReader being the ImageFlow file</p>

<p>but when I try to run it I gives me an error:</p>

<pre><code> ValueError: Cannot feed value of shape (104, 96, 96, 1) for Tensor
 u'Placeholder:0', which has shape (Dimension(None), Dimension(9216))
</code></pre>

<p>I feel like i'm overlooking something small but I don't see it.</p>
",
tensorflow installation problems,"<p>I tried installing tensorflow on my ubuntu 14.04 64bit machine:</p>

<pre><code>sudo pip2 install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.6.0-cp27-none-linux_x86_64.whl
</code></pre>

<p>for python 2.7</p>

<p>When I run <code>import tensorflow as tf</code> in the console I get error</p>

<blockquote>
  <p>Error importing tensorflow.  Unless you are using bazel, you should
  not try to import tensorflow from its source directory; please exit
  the tensorflow source tree, and relaunch your python interpreter from
  there.</p>
</blockquote>

<p>Google didn't provide a good answer to this, any thoughts?</p>
",
How can I convert a tensor into a numpy array in TensorFlow?,"<p>How to convert a tensor into a numpy array when using Tensorflow with Python bindings?</p>
",
TensorFlow Training,"

<p>Assuming I have a very simple neural network, like multilayer perceptron. For each layer the activation function is sigmoid and the network are fully connected.</p>

<p>In TensorFlow this might be defined like this:</p>

<pre class=""lang-py prettyprint-override""><code>    sess = tf.InteractiveSession()

    # Training Tensor
    x = tf.placeholder(tf.float32, shape = [None, n_fft])
    # Label Tensor
    y_ = tf.placeholder(tf.float32, shape = [None, n_fft])

    # Declaring variable buffer for weights W and bias b
    # Layer structure [n_fft, n_fft, n_fft, n_fft]
    # Input -&gt; Layer 1
    struct_w = [n_fft, n_fft]
    struct_b = [n_fft]
    W1 = weight_variable(struct_w, 'W1')
    b1 = bias_variable(struct_b, 'b1')
    h1 = tf.nn.sigmoid(tf.matmul(x, W1) + b1)

    # Layer1 -&gt; Layer 2
    W2 = weight_variable(struct_w, 'W2')
    b2 = bias_variable(struct_b, 'b2')
    h2 = tf.nn.sigmoid(tf.matmul(h1, W2) + b2)

    # Layer2 -&gt; output
    W3 = weight_variable(struct_w, 'W3')
    b3 = bias_variable(struct_b, 'b3')
    y = tf.nn.sigmoid(tf.matmul(h2, W3) + b3)

    # Calculating difference between label and output using mean square error
    mse = tf.reduce_mean(tf.square(y - y_))

    # Train the Model
    # Gradient Descent
    train_step = tf.train.GradientDescentOptimizer(0.3).minimize(mse)
</code></pre>

<p>The design target for this model is to map a <code>n_fft</code> points fft spectrogram to another <code>n_fft</code> target spectrogram. Let's assume both the training data and target data are of size <code>[3000, n_fft]</code>. They are stored in variables <code>spec_train</code> and <code>spec_target</code>.</p>

<p>Now here comes the question. For TensorFlow is there any difference between these two trainings?</p>

<p>Training 1:</p>

<pre class=""lang-py prettyprint-override""><code>for i in xrange(200):
        train_step.run(feed_dict = {x: spec_train, y_: spec_target})
</code></pre>

<p>Training 2:</p>

<pre class=""lang-py prettyprint-override""><code>for i in xrange(200):
        for j in xrange(3000):
            train = spec_train[j, :].reshape(1, n_fft)
            label = spec_target[j, :].reshape(1, n_fft)
            train_step.run(feed_dict = {x: train, y_: label})
</code></pre>

<p>Thank you very much!</p>
",
Bilinear Tensor Product in TensorFlow,"

<p>I'm working on re-implementing <a href=""http://papers.nips.cc/paper/5028-reasoning-with-neural-tensor-networks-for-knowledge-base-completion.pdf"" rel=""nofollow noreferrer"">this paper</a> and the key operation is a bilinear tensor product. I hardly know what that means, but the paper has a nice little graphic, which I understand. </p>

<p><a href=""https://i.stack.imgur.com/BC9gZ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/BC9gZ.png"" alt=""enter image description here""></a></p>

<p>The key operation is <strong>e_1 * W * e_2</strong>, and I want to know how to implement it in tensorflow, because the rest <em>should</em> be easy.</p>

<p>Basically, given 3D tensor W, slice it into matrices, and for the j'th slice (a matrix), multiply it on each side by <strong>e_1</strong> and <strong>e_2</strong>, resulting in a scalar, which is the jth entry in the resulting vector (the output of this operation).</p>

<p>So I want to perform a product of <strong>e_1</strong>, a d-dimensional vector, <strong>W</strong>, the d x d x k tensor, and <strong>e_2</strong>, another d-dimensional vector. Could this product be expressed concisely in TensorFlow as it is now, or would I have to define my own op somehow?</p>

<p><strong>EARLIER EDITS</strong></p>

<p>Why doesn't multiplying these tensors work, and is there some way to define it more explicitly so that it works? </p>

<pre class=""lang-py prettyprint-override""><code>&gt;&gt;&gt; import tensorflow as tf
&gt;&gt;&gt; tf.InteractiveSession()
&gt;&gt;&gt; a = tf.ones([3, 3, 3])
&gt;&gt;&gt; a.eval()
array([[[ 1.,  1.,  1.],
        [ 1.,  1.,  1.],
        [ 1.,  1.,  1.]],

       [[ 1.,  1.,  1.],
        [ 1.,  1.,  1.],
        [ 1.,  1.,  1.]],

       [[ 1.,  1.,  1.],
        [ 1.,  1.,  1.],
        [ 1.,  1.,  1.]]], dtype=float32)
&gt;&gt;&gt; b = tf.ones([3, 1, 1])
&gt;&gt;&gt; b.eval()
array([[[ 1.]],

       [[ 1.]],

       [[ 1.]]], dtype=float32)
&gt;&gt;&gt; 
</code></pre>

<p>The error message is </p>

<pre class=""lang-py prettyprint-override""><code>ValueError: Shapes TensorShape([Dimension(3), Dimension(3), Dimension(3)]) and TensorShape([Dimension(None), Dimension(None)]) must have the same rank
</code></pre>

<p><strong>CURRENTLY</strong></p>

<p>Turns out that multiplying two 3D tensors doesn't work either with <code>tf.matmul</code>, so but <code>tf.batch_matmul</code> does. <code>tf.batch_matmul</code> will also do 3D tensors and matrices. Then I tried 3D and a vector: </p>

<pre class=""lang-py prettyprint-override""><code>ValueError: Dimensions Dimension(3) and Dimension(1) are not compatible
</code></pre>
",
Tensorflow: ImportError: libcusolver.so.8.0: cannot open shared object file: No such file or directory,"<p>I'm having problems in importing tensorflow in python3:</p>

<pre><code>&gt;&gt;&gt; import tensorflow as tf
Traceback (most recent call last):
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 41, in &lt;module&gt;
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in &lt;module&gt;
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/usr/lib/python3.5/imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""/usr/lib/python3.5/imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: libcusolver.so.8.0: cannot open shared object file: No such file or directory

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/__init__.py"", line 24, in &lt;module&gt;
    from tensorflow.python import *
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/__init__.py"", line 51, in &lt;module&gt;
    from tensorflow.python import pywrap_tensorflow
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 52, in &lt;module&gt;
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 41, in &lt;module&gt;
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in &lt;module&gt;
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/usr/lib/python3.5/imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""/usr/lib/python3.5/imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: libcusolver.so.8.0: cannot open shared object file: No such file or directory


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
</code></pre>

<p>I am using Nvidia drivers version 381.09 beta, as version 375 has this bug: <a href=""https://askubuntu.com/questions/896221/strange-artifacts-along-window-borders-after-waking-computer-from-sleep-mode?noredirect=1&amp;lq=1"">https://askubuntu.com/questions/896221/strange-artifacts-along-window-borders-after-waking-computer-from-sleep-mode?noredirect=1&amp;lq=1</a></p>

<p>I have install CUDA 8.0 and cuDNN-v6.0:</p>

<pre><code>rharish@rharish-GL552VW:~$ cd /usr/local
rharish@rharish-GL552VW:/usr/local$ ls
bin         cuda      etc    include  man   share
computecpp  cuda-8.0  games  lib      sbin  src
</code></pre>

<p>Also, libcusolver.so.8.0 exists in /usr/local/cuda/lib64/:</p>

<p><a href=""https://i.stack.imgur.com/ul5x4.jpg"" rel=""noreferrer"">libcusolver.so.8.0 in 'ls' output</a></p>

<p>I have uninstalled and reinstalled CUDA, cuDNN, and built tensorflow from sources. This problem has been occuring since updating the Nvidia drivers to version 381.09 beta. Any help?</p>
",
Should TensorFlow users prefer SavedModel over Checkpoint or GraphDef?,"<p>From <a href=""https://github.com/tensorflow/tensorflow/tree/master/tensorflow/python/saved_model/"" rel=""noreferrer"">SavedModel Docs</a>,</p>

<blockquote>
  <p>SavedModel, the universal serialization format for TensorFlow models.</p>
</blockquote>

<p>and</p>

<blockquote>
  <p>SavedModel wraps a TensorFlow Saver. The Saver is primarily used to generate the variable checkpoints.</p>
</blockquote>

<p>From my understanding, <code>SavedModel</code> is must if someone wants use TensorFlow Serving. However, I can deploy Tensorflow Model to service server without <code>SavedModel</code>: Freeze graph and export it as <code>GraphDef</code>, and load graph into Session using <code>ReadBinaryProto</code> and <a href=""https://www.tensorflow.org/api_docs/cc/class/tensorflow/session#create"" rel=""noreferrer"">Create</a> in C++ or <a href=""https://godoc.org/github.com/tensorflow/tensorflow/tensorflow/go#Graph.Import"" rel=""noreferrer"">Import</a> in Go.</p>

<p>What is the purpose of SavedModel? Should users prefer SavedModel over Checkpoint or GraphDef to aggregate more data related to the model?</p>
",
tensorflow:AttributeError: 'module' object has no attribute 'mul',"<p>I have used tensorflow for ONE day,but there comes some troubles,when I import tensorflow, there would be AttributeError: 'module' object has no attribute 'XXXXXX'</p>

<h2>Environment</h2>

<p>I use ubuntu14.04, python2.7, CUDA toolkit 8.0 and CuDNN v5.
And versions of my six and protobuf are:
Name: six
Version: 1.10.0
Location: /usr/local/lib/python2.7/dist-packages
Requires: 
Name: protobuf
Version: 3.2.0
Location: /usr/local/lib/python2.7/dist-packages
Requires: six, setuptools</p>

<p>here is my test code:</p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>import tensorflow as tf
a = tf.placeholder(tf.int16)
b = tf.placeholder(tf.int16)
add = tf.add(a, b)
mul = tf.mul(a, b)
with tf.Session() as sess:
    # Run every operation with variable input
    print ""Addition with variables: %i"" % sess.run(add, feed_dict={a: 2, b: 3})
    print ""Multiplication with variables: %i"" % sess.run(mul, feed_dict={a: 2, b: 3})</code></pre>
</div>
</div>
</p>

<p>I get this output:</p>

<p><a href=""https://i.stack.imgur.com/LCHw1.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/LCHw1.png"" alt=""enter image description here""></a></p>

<p>Is there any problem with the tensorflow installation? or any other problems?</p>
",
"TensorFlow version 1.0.0-rc2 on Windows: ""OpKernel ('op: ""BestSplits"" device_type: ""CPU""') for unknown op: BestSplits"" with test code","<p>I installed TensorFlow version 1.0.0-rc2 on Windows 7 SP1 x64 Ultimate (Python 3.5.2 |Anaconda custom (64-bit)) using:</p>

<pre><code>pip install --upgrade https://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-1.0.0rc2-cp35-cp35m-win_amd64.whl
</code></pre>

<p>When I try running the test script from <a href=""https://web.archive.org/web/20170214034751/https://www.tensorflow.org/get_started/os_setup#test_the_tensorflow_installation"" rel=""noreferrer"">https://web.archive.org/web/20170214034751/https://www.tensorflow.org/get_started/os_setup#test_the_tensorflow_installation</a> in Eclipse 4.5 or in the console:</p>

<pre><code>import tensorflow as tf
print('TensorFlow version: {0}'.format(tf.__version__))
hello = tf.constant('Hello, TensorFlow!')
sess = tf.Session()
print(sess.run(hello))
</code></pre>

<p>I obtain some error message:</p>

<pre><code>TensorFlow version: 1.0.0-rc2
'Hello, TensorFlow!'
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflob
w\core\framework\op_kernel.cc:943] OpKernel ('op: ""BestSplits"" device_type: ""CPU""') for unknown op: BestSplits
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""CountExtremelyRandomStats"" device_type: ""CPU""') for unknown op: CountExtremelyRandomStats
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""FinishedNodes"" device_type: ""CPU""') for unknown op: FinishedNodes
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""GrowTree"" device_type: ""CPU""') for unknown op: GrowTree
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""ReinterpretStringToFloat"" device_type: ""CPU""') for unknown op: ReinterpretStringToFloat
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""SampleInputs"" device_type: ""CPU""') for unknown op: SampleInputs
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""ScatterAddNdim"" device_type: ""CPU""') for unknown op: ScatterAddNdim
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""TopNInsert"" device_type: ""CPU""') for unknown op: TopNInsert
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""TopNRemove"" device_type: ""CPU""') for unknown op: TopNRemove
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""TreePredictions"" device_type: ""CPU""') for unknown op: TreePredictions
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""UpdateFertileSlots"" device_type: ""CPU""') for unknown op: UpdateFertileSlots
</code></pre>

<p>Why?</p>

<p>I didn't have such issues with TensorFlow 0.12.1  (installed with <code>pip install tensorflow==0.12.1</code>):</p>

<pre><code>TensorFlow version: 0.12.1
b'Hello, TensorFlow!'
</code></pre>
",
Error setuptools when installing tensorflow,"<p>I followed installation instruction at: <a href=""https://www.tensorflow.org/versions/master/get_started/os_setup.html#pip_install"">https://www.tensorflow.org/versions/master/get_started/os_setup.html#pip_install</a></p>

<p>I am using python 2.7.11 (Anaconda) on Mac OS X 10.11. I checked but setuptools 18.5 is already installed on my computer.</p>

<p>and met the error</p>

<pre><code>sudo pip install --upgrade https://storage.googleapis.com/tensorflow/mac/tensorflow-0.6.0-py2-none-any.whl
The directory '/Users/my_name/Library/Caches/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.
The directory '/Users/my_name/Library/Caches/pip' or its parent directory is not owned by the current user and caching wheels has been disabled. check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.
Collecting tensorflow==0.6.0 from https://storage.googleapis.com/tensorflow/mac/tensorflow-0.6.0-py2-none-any.whl
  Downloading https://storage.googleapis.com/tensorflow/mac/tensorflow-0.6.0-py2-none-any.whl (10.2MB)
    100% |████████████████████████████████| 10.2MB 30kB/s 
Requirement already up-to-date: six&gt;=1.10.0 in /Users/my_name/anaconda/lib/python2.7/site-packages/six-1.10.0-py2.7.egg (from tensorflow==0.6.0)
Collecting protobuf==3.0.0a3 (from tensorflow==0.6.0)
  Downloading protobuf-3.0.0a3.tar.gz (88kB)
    100% |████████████████████████████████| 90kB 2.7MB/s 
Requirement already up-to-date: wheel in /Users/my_name/anaconda/lib/python2.7/site-packages (from tensorflow==0.6.0)
Collecting numpy&gt;=1.8.2 (from tensorflow==0.6.0)
  Downloading numpy-1.10.1.tar.gz (4.0MB)
    100% |████████████████████████████████| 4.1MB 77kB/s 
Collecting setuptools (from protobuf==3.0.0a3-&gt;tensorflow==0.6.0)
  Downloading setuptools-18.8.1-py2.py3-none-any.whl (463kB)
    100% |████████████████████████████████| 466kB 673kB/s 
Installing collected packages: setuptools, protobuf, numpy, tensorflow
  Found existing installation: setuptools 18.5
    Uninstalling setuptools-18.5:
      Successfully uninstalled setuptools-18.5
  Rolling back uninstall of setuptools
Exception:
Traceback (most recent call last):
  File ""/Users/my_name/anaconda/lib/python2.7/site-packages/pip/basecommand.py"", line 211, in main
    status = self.run(options, args)
  File ""/Users/my_name/anaconda/lib/python2.7/site-packages/pip/commands/install.py"", line 311, in run
    root=options.root_path,
  File ""/Users/my_name/anaconda/lib/python2.7/site-packages/pip/req/req_set.py"", line 646, in install
    **kwargs
  File ""/Users/my_name/anaconda/lib/python2.7/site-packages/pip/req/req_install.py"", line 803, in install
    self.move_wheel_files(self.source_dir, root=root)
  File ""/Users/my_name/anaconda/lib/python2.7/site-packages/pip/req/req_install.py"", line 998, in move_wheel_files
    isolated=self.isolated,
  File ""/Users/my_name/anaconda/lib/python2.7/site-packages/pip/wheel.py"", line 242, in move_wheel_files
    name, user=user, home=home, root=root, isolated=isolated
  File ""/Users/my_name/anaconda/lib/python2.7/site-packages/pip/locations.py"", line 181, in distutils_scheme
    d = Distribution(dist_args)
  File ""/Users/my_name/anaconda/lib/python2.7/site-packages/setuptools-18.5-py2.7.egg/setuptools/dist.py"", line 269, in __init__
    for ep in pkg_resources.iter_entry_points('distutils.setup_keywords'):
  File ""/Users/my_name/anaconda/lib/python2.7/site-packages/setuptools-18.5-py2.7.egg/pkg_resources/__init__.py"", line 729, in iter_entry_points
    entries = dist.get_entry_map(group)
  File ""/Users/my_name/anaconda/lib/python2.7/site-packages/setuptools-18.5-py2.7.egg/pkg_resources/__init__.py"", line 2705, in get_entry_map
    self._get_metadata('entry_points.txt'), self
  File ""/Users/my_name/anaconda/lib/python2.7/site-packages/setuptools-18.5-py2.7.egg/pkg_resources/__init__.py"", line 2448, in parse_map
    for group, lines in data:
  File ""/Users/my_name/anaconda/lib/python2.7/site-packages/setuptools-18.5-py2.7.egg/pkg_resources/__init__.py"", line 3046, in split_sections
    for line in yield_lines(s):
  File ""/Users/my_name/anaconda/lib/python2.7/site-packages/setuptools-18.5-py2.7.egg/pkg_resources/__init__.py"", line 2303, in yield_lines
    for ss in strs:
  File ""/Users/my_name/anaconda/lib/python2.7/site-packages/setuptools-18.5-py2.7.egg/pkg_resources/__init__.py"", line 2630, in _get_metadata
    if self.has_metadata(name):
  File ""/Users/my_name/anaconda/lib/python2.7/site-packages/setuptools-18.5-py2.7.egg/pkg_resources/__init__.py"", line 1626, in has_metadata
    return self.egg_info and self._has(self._fn(self.egg_info, name))
  File ""/Users/my_name/anaconda/lib/python2.7/site-packages/setuptools-18.5-py2.7.egg/pkg_resources/__init__.py"", line 1984, in _has
    return zip_path in self.zipinfo or zip_path in self._index()
  File ""/Users/my_name/anaconda/lib/python2.7/site-packages/setuptools-18.5-py2.7.egg/pkg_resources/__init__.py"", line 1864, in zipinfo
    return self._zip_manifests.load(self.loader.archive)
  File ""/Users/my_name/anaconda/lib/python2.7/site-packages/setuptools-18.5-py2.7.egg/pkg_resources/__init__.py"", line 1804, in load
    mtime = os.stat(path).st_mtime
OSError: [Errno 2] No such file or directory: '/Users/my_name/anaconda/lib/python2.7/site-packages/setuptools-18.5-py2.7.egg'
</code></pre>
",
Input to LSTM network tensorflow,"<p>I have a time series of length t (x0, ...,xt) each of the xi is a d-dimension vector i.e. xi=(x0i, x1i, ...., xdi). Thus my input X is of shape [batch_size, d]</p>

<p>The input for the tensorflow LSTM should be of size [batchSize, hidden_size]. 
My question is how should i input my time series to the LSTM. One possible solution that i thought of is to have additional weight matrix, W, of size [d,hidden_size] and to input the LSTM with X*W + B. </p>

<p>Is this correct or should i input something else to the netwoרk?</p>

<p>Thanks</p>
",
Changing the number of threads in TensorFlow on Cifar10,"<p>Whenever I run the cifar10_eval.py, in creates 32 threads as following:</p>

<p><strong>I tensorflow/core/common_runtime/local_device.cc:25] Local device intra op parallelism threads: 32</strong></p>

<p>I think this number of threads is the number of threads running on CPUs, but when I check the usage, only 400-500% of CPUs are used. Is there anyway to change this number of threads?</p>
",
How to rename a variable which respects the name scope?,"<p>Given <code>x, y</code> are tensors, I know I can do </p>

<pre><code>with tf.name_scope(""abc""):
    z = tf.add(x, y, name=""z"")
</code></pre>

<p>So that <code>z</code> is named <code>""abc/z""</code>.</p>

<p>I am wondering if there exists a function <code>f</code> which assign the name directly in the following case:</p>

<pre><code>with tf.name_scope(""abc""):
    z = x + y
    f(z, name=""z"")
</code></pre>

<p>The stupid <code>f</code> I am using now is <code>z = tf.add(0, z, name=""z"")</code></p>
",
How does one train multiple models in a single script in TensorFlow when there are GPUs present?,"<p>Say I have access to a number of GPUs in a single machine (for the sake of argument assume 8GPUs each with max memory of 8GB each in one single machine with some amount of RAM and disk). I wanted to run in <strong>one single script</strong> and in one single machine a program that evaluates multiple models (say 50 or 200) in TensorFlow, each with a different hyper parameter setting (say, step-size, decay rate, batch size, epochs/iterations, etc). At the end of training assume we just record its accuracy and get rid of the model (if you want assume the model is being check pointed every so often, so its fine to just throw away the model and start training from scratch. You may also assume some other data may be recorded like the specific hyper params, train, validation, train errors are recorded as we train etc).</p>

<p>Currently I have a (pseudo-)script that looks as follow:</p>

<pre><code>def train_multiple_modles_in_one_script_with_gpu(arg):
    '''
    trains multiple NN models in one session using GPUs correctly.

    arg = some obj/struct with the params for trianing each of the models.
    '''
    #### try mutliple models
    for mdl_id in range(100):
        #### define/create graph
        graph = tf.Graph()
        with graph.as_default():
            ### get mdl
            x = tf.placeholder(float_type, get_x_shape(arg), name='x-input')
            y_ = tf.placeholder(float_type, get_y_shape(arg))
            y = get_mdl(arg,x)
            ### get loss and accuracy
            loss, accuracy = get_accuracy_loss(arg,x,y,y_)
            ### get optimizer variables
            opt = get_optimizer(arg)
            train_step = opt.minimize(loss, global_step=global_step)
        #### run session
        with tf.Session(graph=graph) as sess:
            # train
            for i in range(nb_iterations):
                batch_xs, batch_ys = get_batch_feed(X_train, Y_train, batch_size)
                sess.run(fetches=train_step, feed_dict={x: batch_xs, y_: batch_ys})
                # check_point mdl
                if i % report_error_freq == 0:
                    sess.run(step.assign(i))
                    #
                    train_error = sess.run(fetches=loss, feed_dict={x: X_train, y_: Y_train})
                    test_error = sess.run(fetches=loss, feed_dict={x: X_test, y_: Y_test})
                    print( 'step %d, train error: %s test_error %s'%(i,train_error,test_error) )
</code></pre>

<p>essentially it tries lots of models in one single run but it builds each model in a separate graph and runs each one in a separate session.</p>

<p>I guess my main worry is that its unclear to me how tensorflow under the hood allocates resources for the GPUs to be used. For example, does it load the (part of the) data set only when a session is ran? When I create a graph and a model, is it brought in the GPU immediately or when is it inserted in the GPU? Do I need to clear/free the GPU each time it tries a new model? I don't actually care too much if the models are ran in parallel in multiple GPU (which can be a nice addition), but I want it to first run everything serially without crashing. Is there anything special I need to do for this to work?</p>

<hr>

<p>Currently I am getting an error that starts as follow:</p>

<pre><code>I tensorflow/core/common_runtime/bfc_allocator.cc:702] Stats:
Limit:                   340000768
InUse:                   336114944
MaxInUse:                339954944
NumAllocs:                      78
MaxAllocSize:            335665152

W tensorflow/core/common_runtime/bfc_allocator.cc:274] ***************************************************xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
W tensorflow/core/common_runtime/bfc_allocator.cc:275] Ran out of memory trying to allocate 160.22MiB.  See logs for memory state.
W tensorflow/core/framework/op_kernel.cc:975] Resource exhausted: OOM when allocating tensor with shape[60000,700]
</code></pre>

<p>and further down the line it says:</p>

<pre><code>ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[60000,700]
         [[Node: standardNN/NNLayer1/Z1/add = Add[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/gpu:0""](standardNN/NNLayer1/Z1/MatMul, b1/read)]]

I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 0000:06:00.0)
</code></pre>

<p>however further down the output file (where it prints) it seems to print fine the errors/messages that should show as training proceeds. Does this mean that it didn't run out of resources? Or was it actually able to use the GPU? If it was able to use the CPU instead of the CPU, when why is this an error only happening when GPU are about to be used?</p>

<p>The weird thing is that the data set is really not that big (all 60K points are 24.5M) and when I run a single model locally in my own computer it seems that the process uses less than 5GB. The GPUs have at least 8GB and the computer with them has plenty of RAM and disk (at least 16GB). Thus, the errors that tensorflow is throwing at me are quite puzzling. What is it trying to do and why are they occurring? Any ideas?</p>

<hr>

<p>After reading the answer that suggests to use the multiprocessing library I came up with the following script:</p>

<pre><code>def train_mdl(args):
    train(mdl,args)

if __name__ == '__main__':
    for mdl_id in range(100):
        # train one model with some specific hyperparms (assume they are chosen randomly inside the funciton bellow or read from a config file or they could just be passed or something)
        p = Process(target=train_mdl, args=(args,))
        p.start()
        p.join()
    print('Done training all models!')
</code></pre>

<p>honestly I am not sure why his answer suggests to use pool, or why there are weird tuple brackets but this is what would make sense for me. Would the resources for tensorflow be re-allocated every time a new process is created in the above loop?</p>
",
How to use Tensorflow Optimizer without recomputing activations in reinforcement learning program that returns control after each iteration?,"<p>EDIT(1/3/16): <a href=""https://github.com/tensorflow/tensorflow/issues/672"" rel=""nofollow noreferrer"">corresponding github issue</a></p>

<p>I'm using Tensorflow (Python interface) to implement a q-learning agent with function approximation trained using stochastic gradient-descent.  At each iteration of the experiment a step function in the agent is called that updates the parameters of the approximator based on the new reward and activation, and then chooses a new action to perform.</p>

<p>Here is the problem(with reinforcement learning jargon):</p>

<ul>
<li>The agent computes its state-action value predictions to choose an action.</li>
<li>Then gives control back another program which simulates a step in the environment.</li>
<li>Now the agent's step function is called for the next iteration.  I want to use Tensorflow's Optimizer class to compute the gradients for me.  However, this requires both the state-action value predictions that I computed last step, AND their graph.  So:

<ul>
<li>If I run the optimizer on the whole graph, then it has to recompute the state-action value predictions.</li>
<li>But, if I store the prediction (for the chosen action) as a variable, then feed it to the optimizer as a placeholder, it no longer has the graph necessary to compute the gradients.</li>
<li>I can't just run it all in the same sess.run() statement, because I have to give up control and return the chosen action in order to get the next observation and reward (to use in the target for the loss function).</li>
</ul></li>
</ul>

<p>So, is there a way that I can (without reinforcement learning jargon):</p>

<ol>
<li>Compute part of my graph, returning value1.</li>
<li>Return value1 to the calling program to compute value2</li>
<li>In the next iteration, use value2 to as part of my loss function for gradient descent WITHOUT recomputing the the part of the graph that computes value1.</li>
</ol>

<p>Of course, I've considered the obvious solutions:</p>

<ol>
<li><p>Just hardcode the gradients:  This would be easy for the really simple approximators I'm using now, but would be really inconvenient if I were experimenting with different filters and activation functions in a big convolutional network.  I'd really like to use the Optimizer class if possible.</p></li>
<li><p>Call the environment simulation from within the agent:  <a href=""https://github.com/asrivat1/DeepLearningVideoGames/blob/master/deep_q_network.py#L135"" rel=""nofollow noreferrer"">This system</a> does this, but it would make mine more complicated, and remove a lot of the modularity and structure.  So, I don't want to do this.</p></li>
</ol>

<p>I've read through the API and whitepaper several times, but can't seem to come up with a solution.  I was trying to come up with some way to feed the target into a graph to calculate the gradients, but couldn't come up with a way to build that graph automatically.</p>

<p>If it turns out this isn't possible in TensorFlow yet, do you think it would be very complicated to implement this as a new operator? (I haven't used C++ in a couple of years, so the TensorFlow source looks a little intimidating.)  Or would I be better off switching to something like Torch, which has the imperative differentiation Autograd, instead of symbolic differentiation?</p>

<p>Thanks for taking the time to help me out on this.  I was trying to make this as concise as I could.</p>

<p>EDIT:  After doing some further searching I came across <a href=""https://stackoverflow.com/questions/32082506/neural-network-reinforcement-learning-requiring-next-state-propagation-for-backp"">this previously asked question</a>.  It's a little different than mine (they are trying to avoid updating an LSTM network twice every iteration in Torch), and doesn't have any answers yet.</p>

<p>Here is some code if that helps:</p>

<pre><code>'''
-Q-Learning agent for a grid-world environment.
-Receives input as raw rbg pixel representation of screen.
-Uses an artificial neural network function approximator with one hidden layer

2015 Jonathon Byrd
'''

import random
import sys
#import copy
from rlglue.agent.Agent import Agent
from rlglue.agent import AgentLoader as AgentLoader
from rlglue.types import Action
from rlglue.types import Observation

import tensorflow as tf
import numpy as np

world_size = (3,3)
total_spaces = world_size[0] * world_size[1]

class simple_agent(Agent):

    #Contants
    discount_factor = tf.constant(0.5, name=""discount_factor"")
    learning_rate = tf.constant(0.01, name=""learning_rate"")
    exploration_rate = tf.Variable(0.2, name=""exploration_rate"")  # used to be a constant :P
    hidden_layer_size = 12

    #Network Parameters - weights and biases
    W = [tf.Variable(tf.truncated_normal([total_spaces * 3, hidden_layer_size], stddev=0.1), name=""layer_1_weights""), 
    tf.Variable(tf.truncated_normal([hidden_layer_size,4], stddev=0.1), name=""layer_2_weights"")]
    b = [tf.Variable(tf.zeros([hidden_layer_size]), name=""layer_1_biases""), tf.Variable(tf.zeros([4]), name=""layer_2_biases"")]

    #Input placeholders - observation and reward
    screen = tf.placeholder(tf.float32, shape=[1, total_spaces * 3], name=""observation"") #input pixel rgb values
    reward = tf.placeholder(tf.float32, shape=[], name=""reward"")

    #last step data
    last_obs = np.array([1, 2, 3], ndmin=4)
    last_act = -1

    #Last step placeholders
    last_screen = tf.placeholder(tf.float32, shape=[1, total_spaces * 3], name=""previous_observation"")
    last_move = tf.placeholder(tf.int32, shape = [], name=""previous_action"")

    next_prediction = tf.placeholder(tf.float32, shape = [], name=""next_prediction"")

    step_count = 0

    def __init__(self):
        #Initialize computational graphs
        self.q_preds = self.Q(self.screen)
        self.last_q_preds = self.Q(self.last_screen)
        self.action = self.choose_action(self.q_preds)
        self.next_pred = self.max_q(self.q_preds)
        self.last_pred = self.act_to_pred(self.last_move, self.last_q_preds) # inefficient recomputation
        self.loss = self.error(self.last_pred, self.reward, self.next_prediction)
        self.train = self.learn(self.loss)
        #Summaries and Statistics
        tf.scalar_summary(['loss'], self.loss)
        tf.scalar_summary('reward', self.reward)
        #w_hist = tf.histogram_summary(""weights"", self.W[0])
        self.summary_op = tf.merge_all_summaries()
        self.sess = tf.Session()
        self.summary_writer = tf.train.SummaryWriter('tensorlogs', graph_def=self.sess.graph_def)


    def agent_init(self,taskSpec):
        print(""agent_init called"")
        self.sess.run(tf.initialize_all_variables())

    def agent_start(self,observation):
        #print(""agent_start called, observation = {0}"".format(observation.intArray))
        o = np.divide(np.reshape(np.asarray(observation.intArray), (1,total_spaces * 3)), 255)
        return self.control(o)

    def agent_step(self,reward, observation):
        #print(""agent_step called, observation = {0}"".format(observation.intArray))
        print(""step, reward: {0}"".format(reward))
        o = np.divide(np.reshape(np.asarray(observation.intArray), (1,total_spaces * 3)), 255)

        next_prediction = self.sess.run([self.next_pred], feed_dict={self.screen:o})[0]

        if self.step_count % 10 == 0:
            summary_str = self.sess.run([self.summary_op, self.train], 
                feed_dict={self.reward:reward, self.last_screen:self.last_obs, 
                self.last_move:self.last_act, self.next_prediction:next_prediction})[0]

            self.summary_writer.add_summary(summary_str, global_step=self.step_count)
        else:
            self.sess.run([self.train], 
                feed_dict={self.screen:o, self.reward:reward, self.last_screen:self.last_obs, 
                self.last_move:self.last_act, self.next_prediction:next_prediction})

        return self.control(o)

    def control(self, observation):
        results = self.sess.run([self.action], feed_dict={self.screen:observation})
        action = results[0]

        self.last_act = action
        self.last_obs = observation

        if (action==0):  # convert action integer to direction character
            action = 'u'
        elif (action==1):
            action = 'l'
        elif (action==2):
            action = 'r'
        elif (action==3):
            action = 'd'
        returnAction=Action()
        returnAction.charArray=[action]
        #print(""return action returned {0}"".format(action))
        self.step_count += 1
        return returnAction

    def Q(self, obs):  #calculates state-action value prediction with feed-forward neural net
        with tf.name_scope('network_inference') as scope:
            h1 = tf.nn.relu(tf.matmul(obs, self.W[0]) + self.b[0])
            q_preds = tf.matmul(h1, self.W[1]) + self.b[1] #linear activation
            return tf.reshape(q_preds, shape=[4])

    def choose_action(self, q_preds):  #chooses action epsilon-greedily
        with tf.name_scope('action_choice') as scope:
            exploration_roll = tf.random_uniform([])
            #greedy_action = tf.argmax(q_preds, 0)  # gets the action with the highest predicted Q-value
            #random_action = tf.cast(tf.floor(tf.random_uniform([], maxval=4.0)), tf.int64)

            #exploration rate updates
            #if self.step_count % 10000 == 0:
                #self.exploration_rate.assign(tf.div(self.exploration_rate, 2))

            return tf.select(tf.greater_equal(exploration_roll, self.exploration_rate), 
                tf.argmax(q_preds, 0),   #greedy_action
                tf.cast(tf.floor(tf.random_uniform([], maxval=4.0)), tf.int64))  #random_action

        '''
        Why does this return NoneType?:

        flag = tf.select(tf.greater_equal(exploration_roll, self.exploration_rate), 'g', 'r')
        if flag == 'g':  #greedy
            return tf.argmax(q_preds, 0) # gets the action with the highest predicted Q-value
        elif flag == 'r':  #random
            return tf.cast(tf.floor(tf.random_uniform([], maxval=4.0)), tf.int64)
        '''

    def error(self, last_pred, r, next_pred):
        with tf.name_scope('loss_function') as scope:
            y = tf.add(r, tf.mul(self.discount_factor, next_pred)) #target
            return tf.square(tf.sub(y, last_pred)) #squared difference error


    def learn(self, loss): #Update parameters using stochastic gradient descent
        #TODO:  Either figure out how to avoid computing the q-prediction twice or just hardcode the gradients.
        with tf.name_scope('train') as scope:
            return tf.train.GradientDescentOptimizer(self.learning_rate).minimize(loss, var_list=[self.W[0], self.W[1], self.b[0], self.b[1]])


    def max_q(self, q_preds):
        with tf.name_scope('greedy_estimate') as scope:
            return tf.reduce_max(q_preds)  #best predicted action from current state

    def act_to_pred(self, a, preds): #get the value prediction for action a
        with tf.name_scope('get_prediction') as scope:
            return tf.slice(preds, tf.reshape(a, shape=[1]), [1])


    def agent_end(self,reward):
        pass

    def agent_cleanup(self):
        self.sess.close()
        pass

    def agent_message(self,inMessage):
        if inMessage==""what is your name?"":
            return ""my name is simple_agent"";
        else:
            return ""I don't know how to respond to your message"";

if __name__==""__main__"":
    AgentLoader.loadAgent(simple_agent())
</code></pre>
",
TensorFlow - Input 'split_dim' of 'Split' Op has type float32 that does not match expected type of int32,"<p>I've installed tensorflow using pip on ubuntu 16.04 LTS, when running this code <a href=""https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/recurrent_network.py"" rel=""noreferrer"">https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/recurrent_network.py</a> i am getting this error</p>

<pre><code>Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes. 
Extracting /tmp/data/train-images-idx3-ubyte.gz 
Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes. Extracting /tmp/data/train-labels-idx1-ubyte.gz 
Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes. 
Extracting /tmp/data/t10k-images-idx3-ubyte.gz
Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes. 
Extracting /tmp/data/t10k-labels-idx1-ubyte.gz 
Traceback (most recent call last): 
    File ""deep.py"", line 71, in &lt;module&gt;
        pred = RNN(x, weights, biases)   
    File ""deep.py"", line 60, in RNN
        x = tf.split(x, n_steps, 0)   
    File ""/home/newuser/.local/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.py"", line 1234, in split
        name=name)   
    File ""/home/newuser/.local/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 3241, in _split
    num_split=num_split, name=name)   
    File ""/home/newuser/.local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 508, in apply_op
    (prefix, dtypes.as_dtype(input_arg.type).name)) 
TypeError: Input 'split_dim' of 'Split' Op has type float32 that does not match expected type of int32.
</code></pre>
",
"Installation of TensorFlow on windows 7 - 'pip3' is not recognized as an internal or external command,","<p>When following the Installing TensorFlow for Windows guide <a href=""https://www.tensorflow.org/install/install_windows"" rel=""noreferrer"">https://www.tensorflow.org/install/install_windows</a>, after executing</p>

<pre><code>C:\&gt; pip3 install --upgrade tensorflow
</code></pre>

<p>I get the following error:</p>

<pre><code>'pip3' is not recognized as an internal or external command,
</code></pre>

<p>It looks like pip3 isn't recognized at all (although PATH to python is set)</p>
",
Tensorflow: How to feed a placeholder variable with a tensor?,"<p>I have a placeholder variable that expects a batch of input images: </p>

<pre><code>input_placeholder = tf.placeholder(tf.float32, [None] + image_shape, name='input_images')
</code></pre>

<p>Now I have 2 sources for the input data:<br>
1) a tensor and<br>
2) some numpy data.  </p>

<p>For the numpy input data, I know how to feed data to the placeholder variable:</p>

<pre><code>sess = tf.Session()
mLoss, = sess.run([loss], feed_dict = {input_placeholder: myNumpyData})
</code></pre>

<p>How can I feed a tensor to that placeholder variable? </p>

<pre><code>mLoss, = sess.run([loss], feed_dict = {input_placeholder: myInputTensor})
</code></pre>

<p>gives me an error:</p>

<pre><code>TypeError: The value of a feed cannot be a tf.Tensor object. Acceptable feed values include Python scalars, strings, lists, or numpy ndarrays.
</code></pre>

<p>I don't want to convert the tensor into a numpy array using <code>.eval()</code>, since that would slow my program down, is there any other way?</p>
",
Python: rewrite a looping numpy math function to run on GPU,"<p>Can someone help me rewrite this one function <em>(the <code>doTheMath</code> function)</em> to do the calculations on the GPU? I used a few good days now trying to get my head around it but to no result. I wonder maybe somebody can help me rewrite this function in whatever way you may seem fit as log as I gives the same result at the end. I tried to use <code>@jit</code> from <code>numba</code> but for some reason it is actually much slower than running the code as usual. With a huge sample size, the goal is to decrease the execution time considerably so naturally I believe the GPU is the fastest way to do it. </p>

<p>I'll explain a little what is actually happening. The real data, which looks almost identical as the sample data created in the code below is divided into sample sizes of approx 5.000.000 rows each sample or around 150MB per file. In total there are around 600.000.000 rows or 20GB of data. I must loop through this data, sample by sample and then row by row in each sample, take the last 2000 (or another) rows as of each line and run the <code>doTheMath</code> function which returns a result. That result is then saved back to the hardrive where I can do some other things with it with another program. As you can see below, I do not need all of the results of all the rows, only those bigger than a specific amount. If I run my function as it is right now in python I get about 62seconds per 1.000.000 rows. This is a very long time considering all the data and how fast it should be done with.</p>

<p>I must mention that I upload the real data file by file to the RAM with the help of <code>data = joblib.load(file)</code> so uploading the data is not the problem as it takes only about 0.29 seconds per file. Once uploaded I run the entire code below. What takes the longest time is the <code>doTheMath</code> function. I am willing to give all of my 500 reputation points I have on stackoverflow as a reward for somebody willing to help me rewrite this simple code to run on the GPU. My interest is specifically in the GPU, I really want to see how it is done on this problem at hand.</p>

<p><strong>EDIT/UPDATE 1:</strong>
Here is a link to a small sample of the real data: <a href=""https://mab.to/Nx8GvwTjQ"" rel=""nofollow noreferrer"">data_csv.zip</a> About 102000 rows of real data1 and 2000 rows for real data2a and data2b. Use <code>minimumLimit = 400</code> on the real sample data </p>

<p><strong>EDIT/UPDATE 2:</strong>
For those following this post here is a short summary of the answers below. Up until now we have 4 answers to the original solution. The one offered by @Divakar are just tweaks to the original code. Of the two tweaks only the first one is actually applicable to this problem, the second one is a good tweak but does not apply here. Out of the other three answers, two of them are CPU based solutions and one tensorflow-GPU try. The Tensorflow-GPU by Paul Panzer seems to be promising but when i actually run it on the GPU it is slower than the original, so the code still needs improvement.</p>

<p>The other two CPU based solutions are submitted by @PaulPanzer (a pure numpy solution) and @MSeifert (a numba solution). Both solutions give very good results and both process data extremely fast compared to the original code. Of the two the one submitted by Paul Panzer is faster. It processes about 1.000.000 rows in about 3 seconds. The only problem is with smaller batchSizes, this can be overcome by either switching to the numba solution offered by MSeifert, or even the original code after all the tweaks that have been discussed below.</p>

<p>I am very happy and thankful to @PaulPanzer and @MSeifert for the work they did on their answers. Still, since this is a question about a GPU based solution, i am waiting to see if anybody is willing to give it a try on a GPU version and see how much faster the data can be processed on the GPU when compared to the current CPU solutions. If there will be no other answers outperforming @PaulPanzer's pure numpy solution then i'll accept his answer as the right one and gets the bounty :) </p>

<p><strong>EDIT/UPDATE 3:</strong>
@Divakar has posted a new answer with a solution for the GPU. After my testings on real data, the speed is not even comparable to the CPU counterpart solutions. The GPU processes about 5.000.000 in about 1,5 seconds. This is incredible :) I am very excited about the GPU solution and i thank @Divakar for posting it. As well as i thank @PaulPanzer and @MSeifert for their CPU solutions :) Now my research continues with an incredible speed due to the GPU :) </p>

<pre><code>import pandas as pd
import numpy as np
import time

def doTheMath(tmpData1, data2a, data2b):
    A = tmpData1[:, 0]
    B = tmpData1[:,1]
    C = tmpData1[:,2]
    D = tmpData1[:,3]
    Bmax = B.max()
    Cmin  = C.min()
    dif = (Bmax - Cmin)
    abcd = ((((A - Cmin) / dif) + ((B - Cmin) / dif) + ((C - Cmin) / dif) + ((D - Cmin) / dif)) / 4)
    return np.where(((abcd &lt;= data2a) &amp; (abcd &gt;= data2b)), 1, 0).sum()

#Declare variables
batchSize = 2000
sampleSize = 5000000
resultArray = []
minimumLimit = 490 #use 400 on the real sample data 

#Create Random Sample Data
data1 = np.matrix(np.random.uniform(1, 100, (sampleSize + batchSize, 4)))
data2a = np.matrix(np.random.uniform(0, 1, (batchSize, 1))) #upper limit
data2b = np.matrix(np.random.uniform(0, 1, (batchSize, 1))) #lower limit
#approx. half of data2a will be smaller than data2b, but that is only in the sample data because it is randomly generated, NOT the real data. The real data2a is always higher than data2b.


#Loop through the data
t0 = time.time()
for rowNr in  range(data1.shape[0]):
    tmp_df = data1[rowNr:rowNr + batchSize] #rolling window
    if(tmp_df.shape[0] == batchSize):
        result = doTheMath(tmp_df, data2a, data2b)
        if (result &gt;= minimumLimit):
            resultArray.append([rowNr , result])
print('Runtime:', time.time() - t0)

#Save data results
resultArray = np.array(resultArray)
print(resultArray[:,1].sum())
resultArray = pd.DataFrame({'index':resultArray[:,0], 'result':resultArray[:,1]})
resultArray.to_csv(""Result Array.csv"", sep=';')
</code></pre>

<p>The PC specs I am working on:</p>

<pre><code>GTX970(4gb) video card; 
i7-4790K CPU 4.00Ghz; 
16GB RAM;
a SSD drive 
running Windows 7; 
</code></pre>

<p>As a side question, would a second video card in SLI help on this problem?</p>
",
Tensorflow multiple sessions with multiple GPUs,"<p>I have a workstation with 2 GPUs and I am trying to run multiple tensorflow jobs at the same time, so I can train more than one model at once, etc.</p>

<p>For example, I've tried to separate the sessions into different resources via the python API using in script1.py:</p>

<pre><code>with tf.device(""/gpu:0""):
    # do stuff
</code></pre>

<p>in script2.py:</p>

<pre><code>with tf.device(""/gpu:1""):
    # do stuff
</code></pre>

<p>in script3.py</p>

<pre><code>with tf.device(""/cpu:0""):
    # do stuff
</code></pre>

<p>If I run each script by itself I can see that it is using the specified device. (Also the models fit very well into a single GPU and doesn't use another one even if both are available.)</p>

<p>However, if one script is running and I try to run another, I always get this error:</p>

<pre><code>I tensorflow/core/common_runtime/local_device.cc:40] Local device intra op parallelism threads: 8
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:909] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
I tensorflow/core/common_runtime/gpu/gpu_init.cc:103] Found device 0 with properties: 
name: GeForce GTX 980
major: 5 minor: 2 memoryClockRate (GHz) 1.2155
pciBusID 0000:01:00.0
Total memory: 4.00GiB
Free memory: 187.65MiB
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:909] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
I tensorflow/core/common_runtime/gpu/gpu_init.cc:103] Found device 1 with properties: 
name: GeForce GTX 980
major: 5 minor: 2 memoryClockRate (GHz) 1.2155
pciBusID 0000:04:00.0
Total memory: 4.00GiB
Free memory: 221.64MiB
I tensorflow/core/common_runtime/gpu/gpu_init.cc:127] DMA: 0 1 
I tensorflow/core/common_runtime/gpu/gpu_init.cc:137] 0:   Y Y 
I tensorflow/core/common_runtime/gpu/gpu_init.cc:137] 1:   Y Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:702] Creating    TensorFlow device (/gpu:0) -&gt; (device: 0, name: GeForce GTX 980, pci bus id: 0000:01:00.0)
I tensorflow/core/common_runtime/gpu/gpu_device.cc:702] Creating TensorFlow device (/gpu:1) -&gt; (device: 1, name: GeForce GTX 980, pci bus id: 0000:04:00.0)
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Allocating 187.40MiB bytes.
E tensorflow/stream_executor/cuda/cuda_driver.cc:932] failed to allocate 187.40M (196505600 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
F tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Check failed: gpu_mem != nullptr  Could not allocate GPU device memory for device 0. Tried to allocate 187.40MiB
Aborted (core dumped)
</code></pre>

<p>It seems each tensorflow process is trying to grab all of the GPUs on the machine when it loads even if not all devices are going to be used to run the model.</p>

<p>I see there is an option to limit the amount of GPU each process uses</p>

<pre><code>tf.GPUOptions(per_process_gpu_memory_fraction=0.5)
</code></pre>

<p>...I haven't tried it, but this seems like it would make two processes try to share 50% of each GPU instead of running each process on a separate GPU...</p>

<p>Does anyone know how to configure tensorflow to use only one GPU and leave the other available for another tensorflow process?</p>
",
How to get a tensorflow op by name?,"<p>You can get a tensor by name with <code>tf.get_default_graph().get_tensor_by_name(""tensor_name:0"")</code></p>

<p>But can you get an operation, such as <code>Optimizer.minimize</code>, or an <code>enqueue</code> operation on a queue?</p>

<p>In my first model I returned all tensors and ops I would need from a <code>build_model</code> function. But the list of tensors got ugly. In later models I tossed all tensors and ops in a dictionary for easier access. This time around I thought I'd just look up tensors by name as I needed them, but I don't know how to do that with ops.</p>

<p>Or is there a better way to do this? I find various tensors and ops are needed all over the place. Training, inference code, test cases, hence the desire for a nice standard way of accessing the various parts of the graph without passing variables all over the place.</p>
",
FailedPreconditionError: Attempting to use uninitialized in Tensorflow,"<p>I am working through the <a href=""https://www.tensorflow.org/tutorials/mnist/pros/index.html"" rel=""noreferrer"">tensor flow tutorial</a>, but am trying to use a numpy or pandas format for the data, so that I can compare it with Scikit-Learn results.</p>

<p>I get the digit recognition data from kaggle - <a href=""https://www.kaggle.com/c/digit-recognizer/data"" rel=""noreferrer"">here</a></p>

<p>The tutorial uses a weird format for uploading the data, where as I'm trying to compare with results from other libraries, so would like to keep it in numpy or pandas format.</p>

<p>Here is the standard tensor flow tutorial code (this all works fine):</p>

<pre><code># Stuff from tensorflow tutorial 
import tensorflow as tf
sess = tf.InteractiveSession()

x = tf.placeholder(""float"", shape=[None, 784])
y_ = tf.placeholder(""float"", shape=[None, 10])

W = tf.Variable(tf.zeros([784,10]))
b = tf.Variable(tf.zeros([10]))

y = tf.nn.softmax(tf.matmul(x,W) + b)

cross_entropy = -tf.reduce_sum(y_*tf.log(y))

train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)
</code></pre>

<p>Here I read the data, strip out the target variables and split the data into testing and training datasets (this all works fine):</p>

<pre><code># Read dataframe from training data
csvfile='train.csv'
from pandas import DataFrame, read_csv
df = read_csv(csvfile)

# Strip off the target data and make it a separate dataframe.
Target=df.label
del df[""label""]

# Split data into training and testing sets
msk = np.random.rand(len(df)) &lt; 0.8
dfTest = df[~msk]
TargetTest = Target[~msk]
df = df[msk]
Target = Target[msk]

# One hot encode the target
OHTarget=pd.get_dummies(Target)
OHTargetTest=pd.get_dummies(TargetTest)
</code></pre>

<p>Now, when I try to run the training step, I get a FailedPreconditionError:</p>

<pre><code>for i in range(100):
    batch = np.array(df[i*50:i*50+50].values)
    batch = np.multiply(batch, 1.0 / 255.0)
    Target_batch = np.array(OHTarget[i*50:i*50+50].values)
    Target_batch = np.multiply(Target_batch, 1.0 / 255.0)
    train_step.run(feed_dict={x: batch, y_: Target_batch})
</code></pre>

<p>Here's the full error:</p>

<pre><code>---------------------------------------------------------------------------
FailedPreconditionError                   Traceback (most recent call last)
&lt;ipython-input-82-967faab7d494&gt; in &lt;module&gt;()
      4     Target_batch = np.array(OHTarget[i*50:i*50+50].values)
      5     Target_batch = np.multiply(Target_batch, 1.0 / 255.0)
----&gt; 6     train_step.run(feed_dict={x: batch, y_: Target_batch})

/Users/user32/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc in run(self, feed_dict, session)
   1265         none, the default session will be used.
   1266     """"""
-&gt; 1267     _run_using_default_session(self, feed_dict, self.graph, session)
   1268
   1269

/Users/user32/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc in _run_using_default_session(operation, feed_dict, graph, session)
   2761                        ""the operation's graph is different from the session's ""
   2762                        ""graph."")
-&gt; 2763   session.run(operation, feed_dict)
   2764
   2765

/Users/user32/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in run(self, fetches, feed_dict)
    343
    344     # Run request and get response.
--&gt; 345     results = self._do_run(target_list, unique_fetch_targets, feed_dict_string)
    346
    347     # User may have fetched the same tensor multiple times, but we

/Users/user32/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in _do_run(self, target_list, fetch_list, feed_dict)
    417         # pylint: disable=protected-access
    418         raise errors._make_specific_exception(node_def, op, e.error_message,
--&gt; 419                                               e.code)
    420         # pylint: enable=protected-access
    421       raise e_type, e_value, e_traceback

FailedPreconditionError: Attempting to use uninitialized value Variable_1
     [[Node: gradients/add_grad/Shape_1 = Shape[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""](Variable_1)]]
Caused by op u'gradients/add_grad/Shape_1', defined at:
  File ""/Users/user32/anaconda/lib/python2.7/runpy.py"", line 162, in _run_module_as_main
    ...........

...which was originally created as op u'add', defined at:
  File ""/Users/user32/anaconda/lib/python2.7/runpy.py"", line 162, in _run_module_as_main
    ""__main__"", fname, loader, pkg_name)
[elided 17 identical lines from previous traceback]
  File ""/Users/user32/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.py"", line 3066, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""&lt;ipython-input-45-59183d86e462&gt;"", line 1, in &lt;module&gt;
    y = tf.nn.softmax(tf.matmul(x,W) + b)
  File ""/Users/user32/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/math_ops.py"", line 403, in binary_op_wrapper
    return func(x, y, name=name)
  File ""/Users/user32/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/gen_math_ops.py"", line 44, in add
    return _op_def_lib.apply_op(""Add"", x=x, y=y, name=name)
  File ""/Users/user32/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/op_def_library.py"", line 633, in apply_op
    op_def=op_def)
  File ""/Users/user32/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1710, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/Users/user32/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 988, in __init__
    self._traceback = _extract_stack()
</code></pre>

<p>Any ideas as to how I can fix this?</p>
",
Tensorflow slicing based on variable,"<p>I've found that indexing still is an open issue in tensorflow <a href=""https://github.com/tensorflow/tensorflow/issues/206"">(#206)</a>, so I'm wondering what I could use as a workaround at the moment. I want to index/slice a row/column of a matrix based on a variable that changes for every training example.</p>

<p>What I've tried so far:</p>

<ol>
<li>Slicing based on placeholder (doesn't work)</li>
</ol>

<p>The following (working) code slices based on a fixed number.</p>

<pre><code>import tensorflow as tf
import numpy as np

x = tf.placeholder(""float"")
y = tf.slice(x,[0],[1])

#initialize
init = tf.initialize_all_variables()
sess = tf.Session()
sess.run(init)

#run
result = sess.run(y, feed_dict={x:[1,2,3,4,5]})
print(result)
</code></pre>

<p>However, it seems that I can't simply replace one of these fixed numbers with a tf.placeholder. The following code gives me the error <em>""TypeError: List of Tensors when single Tensor expected.""</em></p>

<pre><code>import tensorflow as tf
import numpy as np

x = tf.placeholder(""float"")
i = tf.placeholder(""int32"")
y = tf.slice(x,[i],[1])

#initialize
init = tf.initialize_all_variables()
sess = tf.Session()
sess.run(init)

#run
result = sess.run(y, feed_dict={x:[1,2,3,4,5],i:0})
print(result)
</code></pre>

<p>This sounds like the brackets around <em>[i]</em> are too much, but removing them  doesn't help either. How to use a placeholder/variable as index?</p>

<ol start=""2"">
<li>Slicing based on python variable (doesn't backprop/update properly)</li>
</ol>

<p>I've also tried using a normal python variable as index. This does not lead to an error, but the network doesn't learn anything while training. I suppose because the changing variable is not properly registered, the graph is malformed and updates don't work?</p>

<ol start=""3"">
<li>Slicing via one-hot vector + multiplication (works, but is slow)</li>
</ol>

<p>One workaround I found is using a one-hot vector. Making a one-hot vector in numpy, passing this using a placeholder, then doing the slicing via matrix multiplication. This works, but is quite slow.</p>

<p>Any ideas how to efficiently slice/index based on a variable?</p>
",
How to install TensorFlow on Windows?,"<p>I am starting to work with TensorFlow library for deep learning, <a href=""https://www.tensorflow.org/"">https://www.tensorflow.org/</a>. </p>

<p>I found a explicit guide to work on it on linux and Mac but I did not find how to work with it under Windows. I try over the net, but the information are lacking. </p>

<p>I use Visual Studio 2015 for my projects, and I am trying to compile the library with Visual studio Compiler VC14.</p>

<p>How to install it and to use it under Windows?</p>

<p>Can I use <a href=""https://github.com/dslomov/bazel-windows"">Bazel for Windows </a> for production use?</p>
",
Tensorflow python : Accessing individual elements in a tensor,"<p>This question is with respect to accessing individual elements in a tensor, say [[1,2,3]]. I need to access the inner element [1,2,3] (This can be performed using .eval() or sess.run()) but it takes longer when the size of the tensor is huge) </p>

<p>Is there any method to do the same faster?</p>

<p>Thanks in Advance.</p>
",
How to interpret Poolallocator messages in tensorflow?,"<p>While training a tensorflow seq2seq model I see the following messages :</p>

<pre>
W tensorflow/core/common_runtime/gpu/pool_allocator.cc:227] PoolAllocator: After 27282 get requests, put_count=9311 evicted_count=1000 eviction_rate=0.1074 and unsatisfied allocation rate=0.699032
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:239] Raising pool_size_limit_ from 100 to 110
W tensorflow/core/common_runtime/gpu/pool_allocator.cc:227] PoolAllocator: After 13715 get requests, put_count=14458 evicted_count=10000 eviction_rate=0.691659 and unsatisfied allocation rate=0.675684
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:239] Raising pool_size_limit_ from 110 to 121
W tensorflow/core/common_runtime/gpu/pool_allocator.cc:227] PoolAllocator: After 6965 get requests, put_count=6813 evicted_count=5000 eviction_rate=0.733891 and unsatisfied allocation rate=0.741421
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:239] Raising pool_size_limit_ from 133 to 146
W tensorflow/core/common_runtime/gpu/pool_allocator.cc:227] PoolAllocator: After 44 get requests, put_count=9058 evicted_count=9000 eviction_rate=0.993597 and unsatisfied allocation rate=0
W tensorflow/core/common_runtime/gpu/pool_allocator.cc:227] PoolAllocator: After 46 get requests, put_count=9062 evicted_count=9000 eviction_rate=0.993158 and unsatisfied allocation rate=0
W tensorflow/core/common_runtime/gpu/pool_allocator.cc:227] PoolAllocator: After 4 get requests, put_count=1029 evicted_count=1000 eviction_rate=0.971817 and unsatisfied allocation rate=0
W tensorflow/core/common_runtime/gpu/pool_allocator.cc:227] PoolAllocator: After 2 get requests, put_count=1030 evicted_count=1000 eviction_rate=0.970874 and unsatisfied allocation rate=0
W tensorflow/core/common_runtime/gpu/pool_allocator.cc:227] PoolAllocator: After 44 get requests, put_count=6074 evicted_count=6000 eviction_rate=0.987817 and unsatisfied allocation rate=0
W tensorflow/core/common_runtime/gpu/pool_allocator.cc:227] PoolAllocator: After 12 get requests, put_count=6045 evicted_count=6000 eviction_rate=0.992556 and unsatisfied allocation rate=0
W tensorflow/core/common_runtime/gpu/pool_allocator.cc:227] PoolAllocator: After 2 get requests, put_count=1042 evicted_count=1000 eviction_rate=0.959693 and unsatisfied allocation rate=0
W tensorflow/core/common_runtime/gpu/pool_allocator.cc:227] PoolAllocator: After 44 get requests, put_count=6093 evicted_count=6000 eviction_rate=0.984737 and unsatisfied allocation rate=0
W tensorflow/core/common_runtime/gpu/pool_allocator.cc:227] PoolAllocator: After 4 get requests, put_count=1069 evicted_count=1000 eviction_rate=0.935454 and unsatisfied allocation rate=0
W tensorflow/core/common_runtime/gpu/pool_allocator.cc:227] PoolAllocator: After 17722 get requests, put_count=9036 evicted_count=1000 eviction_rate=0.110668 and unsatisfied allocation rate=0.550615
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:239] Raising pool_size_limit_ from 792 to 871
W tensorflow/core/common_runtime/gpu/pool_allocator.cc:227] PoolAllocator: After 6 get requests, put_count=1093 evicted_count=1000 eviction_rate=0.914913 and unsatisfied allocation rate=0
W tensorflow/core/common_runtime/gpu/pool_allocator.cc:227] PoolAllocator: After 6 get requests, put_count=1101 evicted_count=1000 eviction_rate=0.908265 and unsatisfied allocation rate=0
W tensorflow/core/common_runtime/gpu/pool_allocator.cc:227] PoolAllocator: After 3224 get requests, put_count=4684 evicted_count=2000 eviction_rate=0.426985 and unsatisfied allocation rate=0.200062
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:239] Raising pool_size_limit_ from 1158 to 1273
W tensorflow/core/common_runtime/gpu/pool_allocator.cc:227] PoolAllocator: After 17794 get requests, put_count=17842 evicted_count=9000 eviction_rate=0.504428 and unsatisfied allocation rate=0.510228
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:239] Raising pool_size_limit_ from 1400 to 1540
W tensorflow/core/common_runtime/gpu/pool_allocator.cc:227] PoolAllocator: After 31 get requests, put_count=1185 evicted_count=1000 eviction_rate=0.843882 and unsatisfied allocation rate=0
W tensorflow/core/common_runtime/gpu/pool_allocator.cc:227] PoolAllocator: After 40 get requests, put_count=8209 evicted_count=8000 eviction_rate=0.97454 and unsatisfied allocation rate=0
W tensorflow/core/common_runtime/gpu/pool_allocator.cc:227] PoolAllocator: After 0 get requests, put_count=2272 evicted_count=2000 eviction_rate=0.880282 and unsatisfied allocation rate=-nan
W tensorflow/core/common_runtime/gpu/pool_allocator.cc:227] PoolAllocator: After 0 get requests, put_count=2362 evicted_count=2000 eviction_rate=0.84674 and unsatisfied allocation rate=-nan
W tensorflow/core/common_runtime/gpu/pool_allocator.cc:227] PoolAllocator: After 38 get requests, put_count=5436 evicted_count=5000 eviction_rate=0.919794 and unsatisfied allocation rate=0

</pre>

<p>What does it mean , does it mean I am having some resource allocation issues? Am running on Titan X 3500+ CUDA ,12 GB GPU</p>
",
Loss function for class imbalanced binary classifier in Tensor flow,"<p>I am trying to apply deep learning for a binary classification problem with high class imbalance between target classes (500k, 31K). I want to write a custom loss function which should be like: 
minimize(100-((predicted_smallerclass)/(total_smallerclass))*100)</p>

<p>Appreciate any pointers on how I can build this logic. </p>
",
In TensorFlow is there any way to just initialize uninitialised variables?,"<p>The standard way of initializing variables in TensorFlow is</p>

<pre><code>init = tf.initialize_all_variables()
sess = tf.Session()
sess.run(init)
</code></pre>

<p>After running some learning for a while I create a new set of variables but once I initialize them it resets all my existing variables. At the moment my way around this is to save all the variable I need and then reapply them after the tf.initalize_all_variables call. This works but is a bit ugly and clunky. I cannot find anything like this in the docs...</p>

<p>Does anyone know of any good way to just initialize the uninitialized variables?   </p>
",
GPU PoolAllocator explodes the CPU memory,"<p>I made a tensorflow model with relatively common operations (apart from a couple of <code>tf.where</code> and indices handling), but call it with very varying different input shapes (many undefined tensor shapes in the model).</p>

<p>Everything works fine on the CPU. <strong>But when you use the GPU</strong>, the RAM usage (not the GPU memory, the CPU one) steadily increases up to fill the 256GB of the machine and kills itself.</p>

<p>During the process, I get the usual messages :</p>

<pre><code>2017-03-17 16:42:22.366601: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 18347 get requests, put_count=18345 evicted_count=1000 eviction_rate=0.0545108 and unsatisfied allocation rate=0.0763068
2017-03-17 16:42:22.366680: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 4385 to 4823
</code></pre>

<p>Which as far as I understand is the pool allocator for some DMA memory for the GPU. The problem is that it seems to never be satisfied with the eviction rate it gets and never ends allocating more space for itself.</p>

<p>Is this normal behavior? Are they ways to control this? Right now, <strong>I can not train a model for longer than 1h before running out of memory</strong>.</p>

<p>Note: I use the nigthly build version of TF, because of some bugfixes necessary for my current model to run. Also, no operations are added during training because I called <code>tf.get_default_graph().finalize()</code></p>

<p><strong>EDIT :</strong> tried to run with <code>tcmalloc</code> instead of <code>malloc</code>. Did not help. I also used the memory profiler and it is not saying there is a memory leak, memory usage stabilizing at 500MB for tcmalloc even if the memory usage in <code>top</code> is way higher and the program eventually run OOM.
So why is the <code>tcmalloc</code> profiler not agreeing with the memory usage I see in <code>top</code>?</p>

<p><strong>EDIT 2 :</strong> recompiled TF with changed hardcoded params to make it ""work"". See <a href=""https://github.com/tensorflow/tensorflow/issues/8560"" rel=""nofollow noreferrer"">here</a></p>
",
Can I measure the execution time of individual operations with TensorFlow?,"<p>I know I can measure the execution time of a call to <code>sess.run()</code>, but is it possible to get a finer granularity and measure the execution time of individual operations?</p>
",
Tensorflow - casting from int to float strange behavior,"<p>I am working on tensorflow 0.12 and am having problem with casting.
The following snippet of code does a strange thing:</p>

<pre><code>sess = tf.InteractiveSession()
a = tf.constant(1)
b = tf.cast(a, tf.float32)
print b.eval()
</code></pre>

<p>I get a value:
6.86574233e-36</p>

<p>I also tried using <code>tf.to_float()</code> and <code>tf.saturate_cast</code>. Both gave the same result.</p>

<p>Please help.</p>
",
Minimal RNN example in tensorflow,"<p>Trying to implement a minimal toy RNN example in tensorflow.
The goal is to learn a mapping from the input data to the target data, similar to this wonderful concise <a href=""https://github.com/lmjohns3/theanets/blob/master/examples/recurrent-sinusoid.py"" rel=""noreferrer"">example in theanets</a>.</p>

<p><em>Update</em>: We're getting there. The only part remaining is to make it converge (and less convoluted). Could someone help to turn the following into running code or provide a simple example?</p>

<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf
from tensorflow.python.ops import rnn_cell

init_scale = 0.1
num_steps = 7
num_units = 7
input_data = [1, 2, 3, 4, 5, 6, 7]
target = [2, 3, 4, 5, 6, 7, 7]
#target = [1,1,1,1,1,1,1] #converges, but not what we want


batch_size = 1

with tf.Graph().as_default(), tf.Session() as session:
  # Placeholder for the inputs and target of the net
  # inputs = tf.placeholder(tf.int32, [batch_size, num_steps])
  input1 = tf.placeholder(tf.float32, [batch_size, 1])
  inputs = [input1 for _ in range(num_steps)]
  outputs = tf.placeholder(tf.float32, [batch_size, num_steps])

  gru = rnn_cell.GRUCell(num_units)
  initial_state = state = tf.zeros([batch_size, num_units])
  loss = tf.constant(0.0)

  # setup model: unroll
  for time_step in range(num_steps):
    if time_step &gt; 0: tf.get_variable_scope().reuse_variables()
    step_ = inputs[time_step]
    output, state = gru(step_, state)
    loss += tf.reduce_sum(abs(output - target))  # all norms work equally well? NO!
  final_state = state

  optimizer = tf.train.AdamOptimizer(0.1)  # CONVERGEs sooo much better
  train = optimizer.minimize(loss)  # let the optimizer train

  numpy_state = initial_state.eval()
  session.run(tf.initialize_all_variables())
  for epoch in range(10):  # now
    for i in range(7): # feed fake 2D matrix of 1 byte at a time ;)
      feed_dict = {initial_state: numpy_state, input1: [[input_data[i]]]} # no
      numpy_state, current_loss,_ = session.run([final_state, loss,train], feed_dict=feed_dict)
    print(current_loss)  # hopefully going down, always stuck at 189, why!?
</code></pre>
",
How do I pass an OpenCV Mat into a C++ Tensorflow graph?,"<p>In Tensorflow C++ I can load an image file into the graph using</p>

<pre><code>tensorflow::Node* file_reader =  tensorflow::ops::ReadFile(tensorflow::ops::Const(IMAGE_FILE_NAME, b.opts()),b.opts().WithName(input_name));
tensorflow::Node* image_reader = tensorflow::ops::DecodePng(file_reader, b.opts().WithAttr(""channels"", 3).WithName(""png_reader""));
tensorflow::Node* float_caster = tensorflow::ops::Cast(image_reader, tensorflow::DT_FLOAT, b.opts().WithName(""float_caster""));
tensorflow::Node* dims_expander = tensorflow::ops::ExpandDims(float_caster, tensorflow::ops::Const(0, b.opts()), b.opts());
tensorflow::Node* resized = tensorflow::ops::ResizeBilinear(dims_expander, tensorflow::ops::Const({input_height, input_width},b.opts().WithName(""size"")),b.opts());
</code></pre>

<p>For an embedded application I would like to instead pass an OpenCV Mat into this graph.  </p>

<p>How would I convert the Mat to a tensor that could be used as input to tensorflow::ops::Cast or tensorflow::ops::ExpandDims?</p>
",
How to write summaries for multiple runs in Tensorflow,"<p>If you look at the Tensorboard <a href=""https://www.tensorflow.org/tensorboard/cifar.html"">dashboard</a> for the cifar10 demo, it shows data for multiple runs. I am having trouble finding a good example showing how to set the graph up to output data in this fashion. I am currently doing something similar to <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/mnist/mnist_with_summaries.py"">this</a>, but it seems to be combining data from runs and whenever a new run starts I see the warning on the console:</p>

<blockquote>
  <p>WARNING:root:Found more than one graph event per run.Overwritting the graph with the newest event</p>
</blockquote>
",
"TensorFlow wasn't compiled to use SSE (etc.) instructions, but these are available","<p>I am running TensorFlow for the first time and using some example code. I got this error when running my code. Does anybody know why this happened, and how to fix it? Thanks!    </p>

<pre><code>2017-03-31 02:12:59.346109: W c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE instructions, but these are available on your machine and could speed up CPU computations.
2017-03-31 02:12:59.346968: W c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE2 instructions, but these are available on your machine and could speed up CPU computations.
2017-03-31 02:12:59.346975: W c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow libbrary wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
2017-03-31 02:12:59.346979: W c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-03-31 02:12:59.346983: W c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-03-31 02:12:59.346987: W c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-03-31 02:12:59.346991: W c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-03-31 02:12:59.346995: W c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
</code></pre>
",
Tensorflow apply op to each element of a 2d tensor,"<p>What I'm after is the ability to apply a tensorflow op to each element of a 2d tensor e.g.</p>

<pre><code>input = tf.Variable([[1.0, 2.0], [3.0, 4.0])
myCustomOp = ... # some kind of custom op that operates on 1D tensors
finalResult = tf.[thing I'm after](input, myCustomOp)
# when run final result should look like: [myCustomOp([1.0, 2.0]), myCustomOp([3.0, 4.0)]
</code></pre>

<p>Any ideas?</p>
",
Tensorflow: custom data load + asynchronous computation,"<p>This is how-to which I believe is missed from TF examples. </p>

<p>Task: </p>

<ol>
<li>samples for each class are given in separate dir and thus labels are indirect (i.e. by dir) </li>
<li>decoupled load and computations in TF</li>
</ol>

<p>Each separate bit could be found, however I think have them all together in one place will help to save a lot of time for TF beginners (like myself).</p>

<p>Lets tackle <strong>1.</strong> in my case it is two sets of images:</p>

<pre class=""lang-py prettyprint-override""><code># all filenames for .jpg in dir 
#  - list of fnames
#  - list of labels 
def path_fnames(f_path, label, ext = ['.jpg', '.jpeg']):
    f_n = [f_path+'/'+f for f in sorted(os.listdir(f_path)) if os.path.splitext(f)[1].lower() in ext]
    f_l = [label] * len(f_n)
    return f_n, f_l
#     
def dense_to_one_hot(labels_dense, num_classes=10, dtype=np.float32):
    """"""Convert class labels from scalars to one-hot vectors.""""""
    num_labels     = labels_dense.shape[0]
    index_offset   = np.arange(num_labels) * num_classes
    labels_one_hot = np.zeros((num_labels, num_classes),dtype=dtype)
    labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1
    return labels_one_hot

data_dir = '/mnt/dataset/'
dir_1   = '/class_1'
dir_2   = '/class_2'

# --- get filenames for data ---
dpath = [data_dir+dir_1, data_dir+dir_2]

f_n1, f_l1 = path_fnames(dpath[0], 0)
f_n2, f_l2 = path_fnames(dpath[1], 1)

# --- create one-hot labels ---
ohl    = dense_to_one_hot(np.asarray(f_l1+f_l2), num_classes=2, dtype = np.float32)
fnames = f_n1+f_n2;               # one-hot labels created in this sequence
</code></pre>

<p>Now we have all file-names and one-hot labels preloaded. </p>

<p>Lets move to the <strong>2.</strong> </p>

<p>It is based on <a href=""https://stackoverflow.com/questions/34594198/how-to-prefetch-data-using-a-custom-python-function-in-tensorflow/34596212#34596212"" title=""How to prefetch data using a custom python function in tensorflow"">How to prefetch data using a custom python function in tensorflow</a>. In short it has: </p>

<ul>
<li>custom image-reader (replace with yours)</li>
<li>queue <strong>fnl_q</strong> with [filename label] which is used by reader to feed</li>
<li>queue <strong>proc_q</strong> with [sample label] which is used to feed processing <strong>some_op</strong></li>
<li>thread which perform read_op to get [sample label] and enqueue_op to put pair into <strong>proc_q</strong>. Thread is controlled by tf.Coordinator</li>
<li><strong>some_op</strong> which first get data from <strong>proc_q</strong> by dequeue_many() and rest of computation (also could be put in separate thread).</li>
</ul>

<p>Notes: </p>

<ul>
<li>feature_read_op and label_read_op are two separate ops. </li>
<li>I use sleep() to slow down and control op - only for test purposes</li>
<li>i have separated ""feeding"" and ""calculation"" parts - in real case just run them in parallel </li>
</ul>

<pre class=""lang-py prettyprint-override""><code>print 'TF version:', tf.__version__
# --- params ----
im_s       = [30, 30, 1]   # target image size
BATCH_SIZE = 16

# image reader 
# - fnl_queue: queue with [fn l] pairs 
# Notes 
# - to resize:  image_tensor = tf.image.resize_image_with_crop_or_pad(image_tensor, HEIGHT, WIDTH)
# - how about image preprocessing?
def img_reader_jpg(fnl_queue, ch = 3, keep = False):
    fn, label = fnl_queue.dequeue()

    if keep:
        fnl_queue.enqueue([fn, label])

    img_bytes = tf.read_file(fn)
    img_u8    = tf.image.decode_jpeg(img_bytes, channels=ch) 
    img_f32   = tf.cast(img_u8, tf.float32)/256.0  
    #img_4     = tf.expand_dims(img_f32,0)
    return img_f32, label

#  load [feature, label] and enqueue to processing queue
# - sess:             tf session 
# - sess:             tf Coordinator
# - [fr_op, lr_op ]:  feature_read_op label_read_op
# - enqueue_op:       [f l] pairs enqueue op
def load_and_enqueue(sess, coord, feature_read_op, label_read_op , enqueue_op):
    i = 0
    while not coord.should_stop():
        # for testing purpose
        time.sleep(0.1)                     
        #print 'load_and_enqueue i=',i
        #i = i +1

        feature, label = sess.run([feature_read_op, label_read_op ])

        feed_dict = {feature_input: feature,
                     label_input  : label}

        sess.run(enqueue_op, feed_dict=feed_dict)


# --- TF part ---

# filenames and labels are pre-loaded
fv = tf.constant(fnames)
lv = tf.constant(ohl)

#fnl_q    = tf.FIFOQueue(len(fnames), [tf.string, tf.float32])
fnl_q    = tf.RandomShuffleQueue(len(fnames), 0, [tf.string, tf.float32])
do_enq = fnl_q.enqueue_many([fv, lv])

# reading_op: feature_read_op label_read_op 
feature_read_op, label_read_op = img_reader_jpg(fnl_q, ch = im_s[2])

# samples queue
f_s = im_s
l_s = 2
feature_input = tf.placeholder(tf.float32, shape=f_s, name='feature_input')
label_input   = tf.placeholder(tf.float32, shape=l_s, name='label_input')

#proc_q     = tf.RandomShuffleQueue(len(fnames), 0, [tf.float32, tf.float32], shapes=[f_s, l_s])
proc_q     = tf.FIFOQueue(len(fnames), [tf.float32, tf.float32], shapes=[f_s, l_s])
enqueue_op = proc_q.enqueue([feature_input, label_input])

# test: 
# - some op
img_batch, lab_batch = proc_q.dequeue_many(BATCH_SIZE)
some_op   = [img_batch, lab_batch]

# service ops
init_op   = tf.initialize_all_variables()

# let run stuff
with tf.Session() as sess:

    sess.run(init_op)
    sess.run(do_enq)

    print ""fnl_q.size:"", fnl_q.size().eval()
    print ""proc_q.size:"", proc_q.size().eval()

    # --- test thread stuff ---
    #  - fill proc_q
    coord = tf.train.Coordinator()
    t = threading.Thread(target=load_and_enqueue, args = (sess, coord, feature_read_op, label_read_op , enqueue_op))
    t.start()

    time.sleep(2.1)

    coord.request_stop()
    coord.join([t])

    print ""fnl_q.size:"", fnl_q.size().eval()
    print ""proc_q.size:"", proc_q.size().eval()

    #  - process a bit 
    ss = sess.run(some_op)
    print 'ss[0].shape', ss[0].shape 
    print ' ss[1]:\n', ss[1]

    print ""fnl_q.size:"", fnl_q.size().eval()
    print ""proc_q.size:"", proc_q.size().eval() 

print 'ok'
</code></pre>

<p>Typical output:
</p>

<pre><code>TF version: 0.6.0

fnl_q.size: 1225
proc_q.size: 0

fnl_q.size: 1204
proc_q.size: 21

ss[0].shape (16, 30, 30, 1)
 ss[1]:
[[ 0.  1.]
 [ 1.  0.]
 [ 1.  0.]
 [ 0.  1.]
 [ 0.  1.]
 [ 1.  0.]
 [ 1.  0.]
 [ 0.  1.]
 [ 1.  0.]
 [ 0.  1.]
 [ 0.  1.]
 [ 1.  0.]
 [ 1.  0.]
 [ 0.  1.]
 [ 1.  0.]
 [ 0.  1.]]

fnl_q.size: 1204
proc_q.size: 5

ok  
</code></pre>

<p>All as expected </p>

<ul>
<li>batch of pairs [sample label] are created  </li>
<li>pairs are shuffled </li>
</ul>

<p>Only thing left is to apply TF as it is intended to be used by replacing <strong>some_op</strong> :)</p>

<p><strong>And a question:</strong> 
one observed problem problem - in case I use <code>tf.FIFOQueue</code> for file-names and <code>tf.RandomShuffleQueue</code> for samples - shuffling doesn't happen. However other way around (as it code above) it does shuffle perfectly.   </p>

<p>Any problem with shuffling for<br>
<code>tf.RandomShuffleQueue(len(fnames), 0, [tf.float32, tf.float32], shapes=[f_s, l_s])</code> ?</p>

<hr>

<p><strong>ADD: 
The version with two threads:</strong></p>

<ul>
<li>one for re-fill/update/change file name queue </li>
<li>second for fill samples to processing queue.</li>
</ul>

<p>Also added correct way to stop threads.</p>

<pre class=""lang-py prettyprint-override""><code>def load_and_enqueue(sess, coord, feature_read_op, label_read_op , enqueue_op):
    try:
        while not coord.should_stop():
            feature, label = sess.run([feature_read_op, label_read_op ])
            feed_dict = {feature_input: feature,
                         label_input  : label}
            sess.run(enqueue_op, feed_dict=feed_dict)
    except Exception as e:
        return


# periodically check the state of fnl queue and if needed refill it
#  - enqueue_op: 'refill' file-name_label queue 
def enqueue_fnl(sess, coord, fnl_q, enqueue_op):
    try:
        while not coord.should_stop():
            time.sleep(0.5)
            s = sess.run(fnl_q.size())
            if  s &lt; (9*BATCH_SIZE) :
                sess.run(enqueue_op)
    except Exception as e:
        return


#  -- ops for feed part --

# filenames and labels are pre-loaded
fv = tf.constant(fnames)
lv = tf.constant(ohl)

# read op
fnl_q      = tf.RandomShuffleQueue(len(fnames)*2, 0, [tf.string, tf.float32], name = 'fnl_q')  # add some margin for re-fill to fit
do_fnl_enq = fnl_q.enqueue_many([fv, lv])
feature_read_op, label_read_op = img_reader_jpg(fnl_q, ch = IMG_SIZE[2])

# samples queue
feature_input = tf.placeholder(tf.float32, shape=IMG_SIZE, name='feature_input')
label_input   = tf.placeholder(tf.float32, shape=LAB_SIZE, name='label_input')
proc_q        = tf.FIFOQueue(len(fnames)*3, [tf.float32, tf.float32], shapes=[IMG_SIZE, LAB_SIZE], name = 'fe_la_q') 
enqueue_op    = proc_q.enqueue([feature_input, label_input])

# -- ops for trainind end eval
img_batch, lab_batch = proc_q.dequeue_many(BATCH_SIZE)

... here is your model

loss       = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, lab_ph))
optimizer  = tf.train.AdamOptimizer(1e-4).minimize(loss)

with tf.Session() as sess:

    coord = tf.train.Coordinator()
    t_le  = threading.Thread(target=load_and_enqueue, args = (sess, coord, feature_read_op, label_read_op , enqueue_op) , name = 'load_and_enqueue')
    t_re  = threading.Thread(target=enqueue_fnl, args = (sess, coord, fnl_q, do_fnl_enq), name = 'enqueue_fnl')  # re-enq thread i.e. refiling filename queue 
    t_le.start()
    t_re.start()

    try:
    # training
    for step in xrange(823):
        # some proc
        img_v, lab_v = sess.run([img_batch, lab_batch])
        feed_dict = { img_ph   : img_v,
              lab_ph   : lab_v,
              keep_prob: 0.7}
        _, loss_v = sess.run([optimizer, loss], feed_dict = feed_dict)

    except Exception as e:
    print 'Training: Exception:', e


    # stop threads 
    coord.request_stop()                                     # ask to stop
    sess.run(fnl_q.close(cancel_pending_enqueues=True))      # tell proc_q don't wait for enque anymore
    sess.run(proc_q.close(cancel_pending_enqueues=True))     # tell proc_q don't wait for enque anymore
    coord.join([t_le, t_re], stop_grace_period_secs=8)       
</code></pre>
",
No variable to save error in Tensorflow,"<p>I am trying to save the model and then reuse it for classifying my images but unfortunately i am getting errors in restoring the model that i have saved.</p>

<p><strong>The code in which model has been created</strong> :</p>

<pre><code># Deep Learning
# =============
# 
# Assignment 4
# ------------

# In[25]:

# These are all the modules we'll be using later. Make sure you can import them
# before proceeding further.
from __future__ import print_function
import numpy as np
import tensorflow as tf
from six.moves import cPickle as pickle
from six.moves import range


# In[37]:

pickle_file = 'notMNIST.pickle'

with open(pickle_file, 'rb') as f:
  save = pickle.load(f)
  train_dataset = save['train_dataset']
  train_labels = save['train_labels']
  valid_dataset = save['valid_dataset']
  valid_labels = save['valid_labels']
  test_dataset = save['test_dataset']
  test_labels = save['test_labels']
  del save  # hint to help gc free up memory
  print('Training set', train_dataset.shape, train_labels.shape)
  print('Validation set', valid_dataset.shape, valid_labels.shape)
  print('Test set', test_dataset.shape, test_labels.shape)
  print(test_labels)


# Reformat into a TensorFlow-friendly shape:
# - convolutions need the image data formatted as a cube (width by height by #channels)
# - labels as float 1-hot encodings.

# In[38]:

image_size = 28
num_labels = 10
num_channels = 1 # grayscale

import numpy as np

def reformat(dataset, labels):
  dataset = dataset.reshape(
    (-1, image_size, image_size, num_channels)).astype(np.float32)
  #print(np.arange(num_labels))
  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)
  #print(labels[0,:])
  print(labels[0])
  return dataset, labels
train_dataset, train_labels = reformat(train_dataset, train_labels)
valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)
test_dataset, test_labels = reformat(test_dataset, test_labels)
print('Training set', train_dataset.shape, train_labels.shape)
print('Validation set', valid_dataset.shape, valid_labels.shape)
print('Test set', test_dataset.shape, test_labels.shape)
#print(labels[0])


# In[39]:

def accuracy(predictions, labels):
  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))
          / predictions.shape[0])


# Let's build a small network with two convolutional layers, followed by one fully connected layer. Convolutional networks are more expensive computationally, so we'll limit its depth and number of fully connected nodes.

# In[47]:

batch_size = 16
patch_size = 5
depth = 16
num_hidden = 64

graph = tf.Graph()

with graph.as_default():

  # Input data.
  tf_train_dataset = tf.placeholder(
    tf.float32, shape=(batch_size, image_size, image_size, num_channels))
  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))
  tf_valid_dataset = tf.constant(valid_dataset)
  tf_test_dataset = tf.constant(test_dataset)

  # Variables.
  layer1_weights = tf.Variable(tf.truncated_normal(
      [patch_size, patch_size, num_channels, depth], stddev=0.1),name=""layer1_weights"")
  layer1_biases = tf.Variable(tf.zeros([depth]),name = ""layer1_biases"")
  layer2_weights = tf.Variable(tf.truncated_normal(
      [patch_size, patch_size, depth, depth], stddev=0.1),name = ""layer2_weights"")
  layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]),name =""layer2_biases"")
  layer3_weights = tf.Variable(tf.truncated_normal(
      [image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1),name=""layer3_biases"")
  layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]),name = ""layer3_biases"")
  layer4_weights = tf.Variable(tf.truncated_normal(
      [num_hidden, num_labels], stddev=0.1),name = ""layer4_weights"")
  layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]),name = ""layer4_biases"")

  # Model.
  def model(data):
    conv = tf.nn.conv2d(data, layer1_weights, [1, 2, 2, 1], padding='SAME')
    hidden = tf.nn.relu(conv + layer1_biases)
    conv = tf.nn.conv2d(hidden, layer2_weights, [1, 2, 2, 1], padding='SAME')
    hidden = tf.nn.relu(conv + layer2_biases)
    shape = hidden.get_shape().as_list()
    reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])
    hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)
    return tf.matmul(hidden, layer4_weights) + layer4_biases

  # Training computation.
  logits = model(tf_train_dataset)
  loss = tf.reduce_mean(
    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))

  # Optimizer.
  optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)

  # Predictions for the training, validation, and test data.
  train_prediction = tf.nn.softmax(logits)
  valid_prediction = tf.nn.softmax(model(tf_valid_dataset))
  test_prediction = tf.nn.softmax(model(tf_test_dataset))


# In[48]:

num_steps = 1001
#saver = tf.train.Saver()
with tf.Session(graph=graph) as session:
  tf.initialize_all_variables().run()
  print('Initialized')
  for step in range(num_steps):
    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)
    batch_data = train_dataset[offset:(offset + batch_size), :, :, :]
    batch_labels = train_labels[offset:(offset + batch_size), :]
    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}
    _, l, predictions = session.run(
      [optimizer, loss, train_prediction], feed_dict=feed_dict)
    if (step % 50 == 0):
      print('Minibatch loss at step %d: %f' % (step, l))
      print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))
      print('Validation accuracy: %.1f%%' % accuracy(
        valid_prediction.eval(), valid_labels))
  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))
  save_path = tf.train.Saver().save(session, ""/tmp/model.ckpt"")
  print(""Model saved in file: %s"" % save_path)
</code></pre>

<p>Everything works fine and the model is stored in the respective folder .</p>

<p><strong>I have created one more python file where i have tried restoring the model but getting an error there</strong></p>

<pre><code># In[1]:
from __future__ import print_function
import numpy as np
import tensorflow as tf
from six.moves import cPickle as pickle
from six.moves import range


# In[3]:

image_size = 28
num_labels = 10
num_channels = 1 # grayscale
import numpy as np


# In[4]:

def accuracy(predictions, labels):
  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))
          / predictions.shape[0])


# In[8]:

batch_size = 16
patch_size = 5
depth = 16
num_hidden = 64

graph = tf.Graph()

with graph.as_default():

  '''# Input data.
  tf_train_dataset = tf.placeholder(
    tf.float32, shape=(batch_size, image_size, image_size, num_channels))
  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))
  tf_valid_dataset = tf.constant(valid_dataset)
  tf_test_dataset = tf.constant(test_dataset)'''

  # Variables.
  layer1_weights = tf.Variable(tf.truncated_normal(
      [patch_size, patch_size, num_channels, depth], stddev=0.1),name=""layer1_weights"")
  layer1_biases = tf.Variable(tf.zeros([depth]),name = ""layer1_biases"")
  layer2_weights = tf.Variable(tf.truncated_normal(
      [patch_size, patch_size, depth, depth], stddev=0.1),name = ""layer2_weights"")
  layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]),name =""layer2_biases"")
  layer3_weights = tf.Variable(tf.truncated_normal(
      [image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1),name=""layer3_biases"")
  layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]),name = ""layer3_biases"")
  layer4_weights = tf.Variable(tf.truncated_normal(
      [num_hidden, num_labels], stddev=0.1),name = ""layer4_weights"")
  layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]),name = ""layer4_biases"")

  # Model.
  def model(data):
    conv = tf.nn.conv2d(data, layer1_weights, [1, 2, 2, 1], padding='SAME')
    hidden = tf.nn.relu(conv + layer1_biases)
    conv = tf.nn.conv2d(hidden, layer2_weights, [1, 2, 2, 1], padding='SAME')
    hidden = tf.nn.relu(conv + layer2_biases)
    shape = hidden.get_shape().as_list()
    reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])
    hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)
    return tf.matmul(hidden, layer4_weights) + layer4_biases

  '''# Training computation.
  logits = model(tf_train_dataset)
  loss = tf.reduce_mean(
    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))

  # Optimizer.
  optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)'''

  # Predictions for the training, validation, and test data.
  #train_prediction = tf.nn.softmax(logits)
  #valid_prediction = tf.nn.softmax(model(tf_valid_dataset))
  #test_prediction = tf.nn.softmax(model(tf_test_dataset))

# In[17]:

#saver = tf.train.Saver()
with tf.Session() as sess:
  # Restore variables from disk.
  tf.train.Saver().restore(sess, ""/tmp/model.ckpt"")
  print(""Model restored."")
  # Do some work with the model
</code></pre>

<p>error that i am getting is :</p>

<p><strong><em>No variables to save</em></strong> </p>

<p>Any help would be appreciated</p>
",
How to set layer-wise learning rate in Tensorflow?,"<p>I am wondering if there is a way that I can use different learning rate for different layers like what is in Caffe. I am trying to modify a pre-trained model and use it for other tasks. What I want is to speed up the training for new added layers and keep the trained layers at low learning rate in order to prevent them from being distorted. for example, I have a 5-conv-layer pre-trained model. Now I add a new conv layer and fine tune it. The first 5 layers would have learning rate of 0.00001 and the last one would have 0.001. Any idea how to achieve this?</p>
",
How to make an if statement using a boolean Tensor,"<p>How do I make an if statement using a boolean tensor?  To be more precise, I'm trying to compare a tensor of size 1 to a constant, checking to see if the value in the tensor is less than the constant.  I figured out I have to make the constant its own size 1 tensor and use <a href=""https://www.tensorflow.org/versions/master/api_docs/python/control_flow_ops.html#less"" rel=""noreferrer"">this</a> method to check if the first tensor is less than the second tensor, but I'm not sure how to make the resulting boolean tensor fit correctly into an if statement.  Just putting it in as the query for the if statement makes if statement always return true.</p>

<p>EDIT:  This is more or less what the code looked like.  However, I'm getting the error <code>'bool' object has no attribute 'name'</code> regardless of whether it has parameters or not, which makes me think the problem is instead that it's not returning a TensorFlow object.</p>

<pre><code>pred = tf.placeholder(tf.bool)

def if_true(x, y, z):
  #act on x, y, and z
  return True

def if_false():
  return False

# Will be `tf.cond()` in the next release.
from tensorflow.python.ops import control_flow_ops
from functools import partial
x = ...
y = ...
z = ...

result = control_flow_ops.cond(pred, partial(if_true, x, y, z), if_false)
</code></pre>
",
Tensorflow Sequence to sequence model using the seq2seq API ( ver 1.1 and above),"<p>I'm using <strong>TensorFlow v:1.1</strong>, and I would like to implement a <strong>sequence to sequence</strong> model using tf.contrib.seq2seq api.
However I have hard time understanding how to use all the functions (BasicDecoder, Dynamic_decode, Helper, Training Helper ...) provided to build my model. <br> <br>
Here is my setup: I would like to ""translate"" a sequence of feature vector: <strong>(batch_size, encoder_max_seq_len, feature_dim)</strong> into a sequence of a different length <strong>(batch_size, decoder_max_len, 1)</strong>. <br> <br></p>

<p>I already have the <strong>encoder</strong> that is an RNN with LSTM cell, and I get its <strong>final state</strong> that I would like to feed to the decoder as initial input.
I already have the cell for my decoder, MultiRNNCell LSM.
Could you help me building the last part using the functions of <strong>tf.contrib.seq2seq2</strong> and dynamic_decode (an <em>example code</em> or <em>explanations</em> would be much appreciated) ? <br> <br></p>

<p>Here is my code:</p>

<pre><code>import tensorflow as tf
from tensorflow.contrib import seq2seq
from tensorflow.contrib import rnn
import math

from data import gen_sum_2b2

class Seq2SeqModel:
def __init__(self,
             in_size,
             out_size,
             embed_size,
             n_symbols,
             cell_type,
             n_units,
             n_layers):
    self.in_size = in_size
    self.out_size = out_size
    self.embed_size = embed_size
    self.n_symbols = n_symbols
    self.cell_type = cell_type
    self.n_units = n_units
    self.n_layers = n_layers

    self.build_graph()

def build_graph(self):
    self.init_placeholders()
    self.init_cells()
    self.encoder()
    self.decoder_train()
    self.loss()
    self.training()

def init_placeholders(self):
    with tf.name_scope('Placeholders'):
        self.encoder_inputs = tf.placeholder(shape=(None, None, self.in_size), 
                                             dtype=tf.float32, name='encoder_inputs')
        self.decoder_targets = tf.placeholder(shape=(None, None),
                                              dtype=tf.int32, name='decoder_targets')
        self.seqs_len = tf.placeholder(dtype=tf.int32)
        self.batch_size = tf.placeholder(tf.int32, name='dynamic_batch_size')
        self.max_len = tf.placeholder(tf.int32, name='dynamic_seq_len')
        decoder_inputs = tf.reshape(self.decoder_targets, shape=(self.batch_size,
                                    self.max_len, self.out_size))
        self.decoder_inputs = tf.cast(decoder_inputs, tf.float32)
        self.eos_step = tf.ones([self.batch_size, 1], dtype=tf.float32, name='EOS')
        self.pad_step = tf.zeros([self.batch_size, 1], dtype=tf.float32, name='PAD')

def RNNCell(self):
    c = self.cell_type(self.n_units, reuse=None)
    c = rnn.MultiRNNCell([self.cell_type(self.n_units) for i in range(self.n_layers)])
    return c

def init_cells(self):
    with tf.variable_scope('RNN_enc_cell'):
        self.encoder_cell = self.RNNCell()  
    with tf.variable_scope('RNN_dec_cell'):
        self.decoder_cell = rnn.OutputProjectionWrapper(self.RNNCell(), self.n_symbols)

def encoder(self):
    with tf.variable_scope('Encoder'):
        self.init_state = self.encoder_cell.zero_state(self.batch_size, tf.float32) 
        _, self.encoder_final_state = tf.nn.dynamic_rnn(self.encoder_cell, self.encoder_inputs,
                                                        initial_state=self.init_state) 
</code></pre>
",
How does asynchronous training work in distributed Tensorflow?,"<p>I've read <a href=""https://www.tensorflow.org/deploy/distributed"" rel=""noreferrer"">Distributed Tensorflow Doc</a>, and it mentions that in asynchronous training, </p>

<blockquote>
  <p>each replica of the graph has an independent training loop that executes without coordination.</p>
</blockquote>

<p>From what I understand, if we use parameter-server with data parallelism architecture, it means each worker computes gradients and updates its own weights without caring about other workers updates for distributed training Neural Network. As all weights are shared on parameter server (ps), I think ps still has to coordinate (or aggregate) weight updates from all workers in some way. I wonder how does the aggregation work in asynchronous training. Or in more general words, how does asynchronous training work in distributed Tensorflow?</p>
",
"""freeze"" some variables/scopes in tensorflow: stop_gradient vs passing variables to minimize","<p>I am trying to implement <a href=""http://arxiv.org/abs/1406.2661"">Adversarial NN</a>, which requires to 'freeze' one or the other part of the graph during alternating training minibatches. I.e. there two sub-networks: G and D.</p>

<pre><code>G( Z ) -&gt;  Xz
D( X ) -&gt;  Y
</code></pre>

<p>where loss function of <code>G</code> depends on <code>D[G(Z)], D[X]</code>.</p>

<p>First I need to train parameters in D with all G parameters fixed, and then parameters in G with parameters in D fixed. Loss function in first case will be negative loss function in the second case and the update will have to apply to the parameters of whether first or second subnetwork.</p>

<p>I saw that tensorflow has <code>tf.stop_gradient</code> function. For purpose of training the D (downstream) subnetwork I can use this function to block the gradient flow to </p>

<pre><code> Z -&gt; [ G ] -&gt; tf.stop_gradient(Xz) -&gt; [ D ] -&gt; Y
</code></pre>

<p>The <code>tf.stop_gradient</code> is very succinctly annotated with no in-line example (and example <code>seq2seq.py</code> is too long and not that easy to read), but looks like it must be called during the graph creation. <strong>Does it imply that if I want to block/unblock gradient flow in alternating batches, I need to re-create and re-initialize the graph model?</strong> </p>

<p>Also it seems that <strong>one cannot block the gradient flowing through the G (upstream) network by means of <code>tf.stop_gradient</code>, right?</strong></p>

<p>As an alternative I saw that one can pass the list of variables to the optimizer call as <code>opt_op = opt.minimize(cost, &lt;list of variables&gt;)</code>, which would be an easy solution if one could get all variables in the scopes of each subnetwork. <strong>Can one get a <code>&lt;list of variables&gt;</code> for a tf.scope?</strong></p>
",
How do I get the gradient of the loss at a TensorFlow variable?,"<p>The feature I'm after is to be able to tell what the gradient of a given variable is with respect to my error function given some data.</p>

<p>One way to do this would be to see how much the variable has changed after a call to train, but obviously that can vary massively based on the learning algorithm (for example it would be almost impossible to tell with something like RProp) and just isn't very clean.</p>

<p>Thanks in advance.</p>
",
Tensorflow Dictionary lookup with String tensor,"<p>Is there any way to perform a dictionary lookup based on a String tensor in Tensorflow?</p>

<p>In plain Python, I'd do something like</p>

<pre><code>value = dictionary[key]
</code></pre>

<p>. Now I'd like to do the same thing at Tensorflow runtime, when I have my <code>key</code> as a String tensor. Something like</p>

<pre><code>value_tensor = tf.dict_lookup(string_tensor)
</code></pre>

<p>would be nice.</p>
",
How to implement pixel-wise classification for scene labeling in TensorFlow?,"<p>I am working on a deep learning model using <strong>Google's TensorFlow</strong>. The model should be used to <strong>segment and label scenes</strong>. </p>

<ol>
<li>I am using the <strong>SiftFlow dataset</strong> which has <em>33 semantic
classes</em> and <em>images with 256x256 pixels</em>.  </li>
<li>As a result, at my final layer using convolution and deconvolution I arrive at the following tensor(array) <em>[256, 256, 33]</em>. </li>
<li>Next I would like to
apply <em>Softmax</em> and compare the results to a semantic label of size
<em>[256, 256]</em>.</li>
</ol>

<p><strong>Questions:</strong>
Should I apply mean averaging or argmax to my final layer so its shape becomes <em>[256,256,1]</em> and then loop through each pixel and classify as if I were classying <em>256x256</em> instances? If the answer is yes, how, if not, what other options?</p>
",
"Role of ""Flatten"" in Keras","<p>I am trying to understand the role of the <code>Flatten</code> function in Keras. Below is my code, which is a simple two-layer network. It takes in 2-dimensional data of shape (3, 2), and outputs 1-dimensional data of shape (1, 4):</p>

<pre><code>model = Sequential()
model.add(Dense(16, input_shape=(3, 2)))
model.add(Activation('relu'))
model.add(Flatten())
model.add(Dense(4))
model.compile(loss='mean_squared_error', optimizer='SGD')

x = np.array([[[1, 2], [3, 4], [5, 6]]])

y = model.predict(x)

print y.shape
</code></pre>

<p>This prints out that <code>y</code> has shape (1, 4). However, if I remove the <code>Flatten</code> line, then it prints out that <code>y</code> has shape (1, 3, 4).</p>

<p>I don't understand this. From my understanding of neural networks, the <code>model.add(Dense(16, input_shape=(3, 2)))</code> function is creating a hidden fully-connected layer, with 16 nodes. Each of these nodes is connected to each of the 3x2 input elements. Therefore, the 16 nodes at the output of this first layer are already ""flat"". So, the output shape of the first layer should be (1, 16). Then, the second layer takes this as an input, and outputs data of shape (1, 4).</p>

<p>So if the output of the first layer is already ""flat"" and of shape (1, 16), why do I need to further flatten it?</p>

<p>Thanks!</p>
",
Python TensorFlow: How to restart training with optimizer and import_meta_graph?,"<p>I'm trying to restart a model training in TensorFlow by picking up where it left off. I'd like to use the recently added (0.12+ I think) <code>import_meta_graph()</code> so as to not reconstruct the graph. </p>

<p>I've seen solutions for this, e.g. <a href=""https://stackoverflow.com/questions/33759623/tensorflow-how-to-save-restore-a-model-python"">Tensorflow: How to save/restore a model?</a>, but I run into issues with AdamOptimizer, specifically I get a <code>ValueError: cannot add op with name &lt;my weights variable name&gt;/Adam as that name is already used</code> error. <a href=""https://stackoverflow.com/a/33846659"">This can be fixed by initializing</a>, but then my model values are cleared! </p>

<p>There are other answers and some full examples out there, but they always seem older and so don't include the newer <code>import_meta_graph()</code> approach, or don't have a non-tensor optimizer. The closest question I could find is <a href=""https://stackoverflow.com/questions/34500052/tensorflow-saving-and-restoring-session"">tensorflow: saving and restoring session</a> but there is no final clear cut solution and the example is pretty complicated.</p>

<p>Ideally I'd like a simple run-able example starting from scratch, stopping, then picking up again. I have something that works (below), but do also wonder if I'm missing something. Surely I'm not the only one doing this?</p>
",
What is the default kernel initializer in tf.layers.conv2d and tf.layers.dense?,"<p>The official Tensorflow API doc claims that the parameter <em>kernel_initializer</em> defaults to None for tf.layers.conv2d and tf.layers.dense.</p>

<p>However, reading the layers tutorial (<a href=""https://www.tensorflow.org/tutorials/layers"" rel=""noreferrer"">https://www.tensorflow.org/tutorials/layers</a>), I noted that this parameter is not set in the code. For example:</p>

<pre><code># Convolutional Layer #1
  conv1 = tf.layers.conv2d(
      inputs=input_layer,
      filters=32,
      kernel_size=[5, 5],
      padding=""same"",
      activation=tf.nn.relu)
</code></pre>

<p>The example code from the tutorial runs without any errors, so I think the default <em>kernel_initializer</em> is not ""None"". So, which initializer is used?</p>

<p>In another code, I did not set the <em>kernel_initializer</em> of the conv2d and dense layers, and everything was fine. However, when I tried to set the <em>kernel_initializer</em> to tf.truncated_normal_initializer(stddev=0.1, dtype=tf.float32), I got NaN errors. What is going on here? Can anyone help?</p>
",
TensorFlow: Performing this loss computation,"<p>My question and problem is stated below the two blocks of code.</p>

<hr>

<h1>Loss Function</h1>

<pre><code>def loss(labels, logits, sequence_lengths, label_lengths, logit_lengths):    
    scores = []
    for i in xrange(runner.batch_size):
        sequence_length = sequence_lengths[i]
        for j in xrange(length):
            label_length = label_lengths[i, j]
            logit_length = logit_lengths[i, j]

             # get top k indices &lt;==&gt; argmax_k(labels[i, j, 0, :], label_length)
            top_labels = np.argpartition(labels[i, j, 0, :], -label_length)[-label_length:]
            top_logits = np.argpartition(logits[i, j, 0, :], -logit_length)[-logit_length:]

            scores.append(edit_distance(top_labels, top_logits))

    return np.mean(scores)

# Levenshtein distance
def edit_distance(s, t):
    n = s.size
    m = t.size
    d = np.zeros((n+1, m+1))
    d[:, 0] = np.arrange(n+1)
    d[0, :] = np.arrange(n+1)

    for j in xrange(1, m+1):
        for i in xrange(1, n+1):
            if s[i] == t[j]:
                d[i, j] = d[i-1, j-1]
            else:
                d[i, j] = min(d[i-1, j] + 1,
                              d[i, j-1] + 1,
                              d[i-1, j-1] + 1)

    return d[m, n]
</code></pre>

<hr>

<h1>Being used in</h1>

<p>I've tried to flatten my code so that everything is happening in one place. Let me know if there are typos/points of confusion.</p>

<pre><code>sequence_lengths_placeholder = tf.placeholder(tf.int64, shape=(batch_size))
labels_placeholder = tf.placeholder(tf.float32, shape=(batch_size, max_feature_length, label_size))
label_lengths_placeholder = tf.placeholder(tf.int64, shape=(batch_size, max_feature_length))
loss_placeholder = tf.placeholder(tf.float32, shape=(1))

logit_W = tf.Variable(tf.zeros([lstm_units, label_size]))
logit_b = tf.Variable(tf.zeros([label_size]))

length_W = tf.Variable(tf.zeros([lstm_units, max_length]))
length_b = tf.Variable(tf.zeros([max_length]))

lstm = rnn_cell.BasicLSTMCell(lstm_units)
stacked_lstm = rnn_cell.MultiRNNCell([lstm] * layer_count)

rnn_out, state = rnn.rnn(stacked_lstm, features, dtype=tf.float32, sequence_length=sequence_lengths_placeholder)

logits = tf.concat(1, [tf.reshape(tf.matmul(t, logit_W) + logit_b, [batch_size, 1, 2, label_size]) for t in rnn_out])

logit_lengths = tf.concat(1, [tf.reshape(tf.matmul(t, length_W) + length_b, [batch_size, 1, max_length]) for t in rnn_out])

optimizer = tf.train.AdamOptimizer(learning_rate)
global_step = tf.Variable(0, name='global_step', trainable=False)
train_op = optimizer.minimize(loss_placeholder, global_step=global_step)

...
...
# Inside training loop

np_labels, np_logits, sequence_lengths, label_lengths, logit_lengths = sess.run([labels_placeholder, logits, sequence_lengths_placeholder, label_lengths_placeholder, logit_lengths], feed_dict=feed_dict)
loss = loss(np_labels, np_logits, sequence_lengths, label_lengths, logit_lengths)
_ = sess.run([train_op], feed_dict={loss_placeholder: loss})
</code></pre>

<hr>

<h1>My issue</h1>

<p>The issue is that this is returning the error: </p>

<pre><code>  File ""runner.py"", line 63, in &lt;module&gt;
    train_op = optimizer.minimize(loss_placeholder, global_step=global_step)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py"", line 188, in minimize
    name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py"", line 277, in apply_gradients
    (grads_and_vars,))

  ValueError: No gradients provided for any variable: &lt;all my variables&gt;
</code></pre>

<p>So I assume that this is TensorFlow complaining that it can't compute the gradients of my loss because the loss is performed by numpy, outside the scope of TF.</p>

<p>So naturally to fix that I would try and implement this in TensorFlow. The issue is, my <code>logit_lengths</code> and <code>label_lengths</code> are both Tensors, so when I try and access a single element, I'm returned a Tensor of shape []. This is an issue when I'm trying to use <code>tf.nn.top_k()</code> which takes an <code>Int</code> for its <code>k</code> parameter.</p>

<p>Another issue with that is my <code>label_lengths</code> is a Placeholder and since my <code>loss</code> value need to be defined before the <code>optimizer.minimize(loss)</code> call, I also get an error that says a value needs to be passed for the placeholder.</p>

<p>I'm just wondering how I could try and implement this loss function. Or if I'm missing something obvious.</p>

<hr>

<p><strong>Edit:</strong> After some <a href=""http://arxiv.org/pdf/1511.06456v4.pdf"" rel=""noreferrer"">further reading</a> I see that usually losses like the one I describe are used in validation and in training a surrogate loss that minimizes in the same place as the true loss is used. Does anyone know what surrogate loss is used for an edit distance based scenario like mine?</p>
",
TensorFlow in production for real time predictions in high traffic app - how to use?,"<p>What is the right way to use TensorFlow for real time predictions in a high traffic application.</p>

<p>Ideally I would have a server/cluster running tensorflow listening on a port(s) where I can connect from app servers and get predictions similar to the way databases are used.
Training should be done by cron jobs feeding the training data through the network to the same server/cluster.</p>

<p>How does one actually use tensorflow in production? Should I build a setup where the python is running as a server and use the python scripts to get predictions? I'm still new to this but I feel that such script will need to open sessions etc.. which is not scalable. (I'm talking about 100s of predictions/sec).</p>

<p>Any pointer to relevant information will be highly appreciated. I could not find any.</p>
",
List of tensor names in graph in Tensorflow,"<p>The graph object in Tensorflow has a method called ""get_tensor_by_name(name)"". Is there anyway to get a list of valid tensor names?</p>

<p>If not, does anyone know the valid names for the pretrained model inception-v3 <a href=""https://www.tensorflow.org/versions/v0.6.0/tutorials/image_recognition/index.html"">from here</a>? From their example, pool_3, is one valid tensor but a list of all of them would be nice. I looked at <a href=""http://arxiv.org/abs/1512.00567"">the paper referred to</a> and some of the layers seems to correspond to the sizes in table 1 but not all of them.</p>
",
TensorFlow for binary classification,"<p>I am trying to adapt <a href=""https://www.tensorflow.org/versions/0.6.0/how_tos/summaries_and_tensorboard/index.html"" rel=""noreferrer"">this MNIST example</a> to binary classification.</p>

<p>But when changing my <code>NLABELS</code> from <code>NLABELS=2</code> to <code>NLABELS=1</code>, the loss function always returns 0 (and accuracy 1).</p>

<pre><code>from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

from tensorflow.examples.tutorials.mnist import input_data
import tensorflow as tf

# Import data
mnist = input_data.read_data_sets('data', one_hot=True)
NLABELS = 2

sess = tf.InteractiveSession()

# Create the model
x = tf.placeholder(tf.float32, [None, 784], name='x-input')
W = tf.Variable(tf.zeros([784, NLABELS]), name='weights')
b = tf.Variable(tf.zeros([NLABELS], name='bias'))

y = tf.nn.softmax(tf.matmul(x, W) + b)

# Add summary ops to collect data
_ = tf.histogram_summary('weights', W)
_ = tf.histogram_summary('biases', b)
_ = tf.histogram_summary('y', y)

# Define loss and optimizer
y_ = tf.placeholder(tf.float32, [None, NLABELS], name='y-input')

# More name scopes will clean up the graph representation
with tf.name_scope('cross_entropy'):
    cross_entropy = -tf.reduce_mean(y_ * tf.log(y))
    _ = tf.scalar_summary('cross entropy', cross_entropy)
with tf.name_scope('train'):
    train_step = tf.train.GradientDescentOptimizer(10.).minimize(cross_entropy)

with tf.name_scope('test'):
    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))
    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
    _ = tf.scalar_summary('accuracy', accuracy)

# Merge all the summaries and write them out to /tmp/mnist_logs
merged = tf.merge_all_summaries()
writer = tf.train.SummaryWriter('logs', sess.graph_def)
tf.initialize_all_variables().run()

# Train the model, and feed in test data and record summaries every 10 steps

for i in range(1000):
    if i % 10 == 0:  # Record summary data and the accuracy
        labels = mnist.test.labels[:, 0:NLABELS]
        feed = {x: mnist.test.images, y_: labels}

        result = sess.run([merged, accuracy, cross_entropy], feed_dict=feed)
        summary_str = result[0]
        acc = result[1]
        loss = result[2]
        writer.add_summary(summary_str, i)
        print('Accuracy at step %s: %s - loss: %f' % (i, acc, loss)) 
   else:
        batch_xs, batch_ys = mnist.train.next_batch(100)
        batch_ys = batch_ys[:, 0:NLABELS]
        feed = {x: batch_xs, y_: batch_ys}
    sess.run(train_step, feed_dict=feed)
</code></pre>

<p>I have checked the dimensions of both <code>batch_ys</code> (fed into <code>y</code>) and <code>_y</code> and they are both 1xN matrices when <code>NLABELS=1</code> so the problem seems to be prior to that. Maybe something to do with the matrix multiplication?</p>

<p>I actually have got this same problem in a real project, so any help would be appreciated... Thanks!</p>
",
TensorFlow: numpy.repeat() alternative,"<p>I want to compare the predicted values <code>yp</code> from my neural network in a pairwise fashion, and so I was using (back in my old numpy implementation):</p>

<pre><code>idx = np.repeat(np.arange(len(yp)), len(yp))
jdx = np.tile(np.arange(len(yp)), len(yp))
s = yp[[idx]] - yp[[jdx]]
</code></pre>

<p>This basically create a indexing mesh which I then use. <code>idx=[0,0,0,1,1,1,...]</code> while <code>jdx=[0,1,2,0,1,2...]</code>. I do not know if there is a simpler manner of doing it...</p>

<p>Anyhow, TensorFlow has a <code>tf.tile()</code>, but it seems to be lacking a <code>tf.repeat()</code>.</p>

<pre><code>idx = np.repeat(np.arange(n), n)
v2 = v[idx]
</code></pre>

<p>And I get the error:</p>

<pre><code>TypeError: Bad slice index [  0   0   0 ..., 215 215 215] of type &lt;type 'numpy.ndarray'&gt;
</code></pre>

<p>It also does not work to use a TensorFlow constant for the indexing:</p>

<pre><code>idx = tf.constant(np.repeat(np.arange(n), n))
v2 = v[idx]
</code></pre>

<p>-</p>

<pre><code>TypeError: Bad slice index Tensor(""Const:0"", shape=TensorShape([Dimension(46656)]), dtype=int64) of type &lt;class 'tensorflow.python.framework.ops.Tensor'&gt;
</code></pre>

<p>The idea is to convert my <a href=""http://research.microsoft.com/en-us/um/people/cburges/papers/ICML_ranking.pdf"" rel=""noreferrer"">RankNet</a> implementation to TensorFlow.</p>
",
Tensorflow Precision / Recall / F1 score and Confusion matrix,"<p>I would like to know if there is a way to implement the different score function from the scikit learn package like this one :</p>

<pre><code>from sklearn.metrics import confusion_matrix
confusion_matrix(y_true, y_pred)
</code></pre>

<p>into a tensorflow model to get the different score.</p>

<pre><code>with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:
init = tf.initialize_all_variables()
sess.run(init)
for epoch in xrange(1):
        avg_cost = 0.
        total_batch = len(train_arrays) / batch_size
        for batch in range(total_batch):
                train_step.run(feed_dict = {x: train_arrays, y: train_labels})
                avg_cost += sess.run(cost, feed_dict={x: train_arrays, y: train_labels})/total_batch
        if epoch % display_step == 0:
                print ""Epoch:"", '%04d' % (epoch+1), ""cost="", ""{:.9f}"".format(avg_cost)

print ""Optimization Finished!""
correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))
# Calculate accuracy
accuracy = tf.reduce_mean(tf.cast(correct_prediction, ""float""))
print ""Accuracy:"", batch, accuracy.eval({x: test_arrays, y: test_labels})
</code></pre>

<p>Will i have to run the session again to get the prediction ?</p>
",
Predicting the next word using the LSTM ptb model tensorflow example,"<p>I am trying to use the tensorflow <a href=""https://www.tensorflow.org/versions/r0.7/tutorials/recurrent/index.html#lstm"" rel=""nofollow noreferrer"">LSTM model</a> to make next word predictions.</p>

<p>As described in this <a href=""https://stackoverflow.com/questions/33773661/predicting-next-word-using-the-language-model-tensorflow-example"">related question</a> (which has no accepted answer) the example contains pseudocode to extract next word probabilities:</p>

<pre><code>lstm = rnn_cell.BasicLSTMCell(lstm_size)
# Initial state of the LSTM memory.
state = tf.zeros([batch_size, lstm.state_size])

loss = 0.0
for current_batch_of_words in words_in_dataset:
  # The value of state is updated after processing each batch of words.
  output, state = lstm(current_batch_of_words, state)

  # The LSTM output can be used to make next word predictions
  logits = tf.matmul(output, softmax_w) + softmax_b
  probabilities = tf.nn.softmax(logits)
  loss += loss_function(probabilities, target_words)
</code></pre>

<p>I am confused about how to interpret the probabilities vector. I modified the <code>__init__</code> function of the <code>PTBModel</code> in <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/rnn/ptb/ptb_word_lm.py"" rel=""nofollow noreferrer"">ptb_word_lm.py</a> to store the probabilities and logits:</p>

<pre><code>class PTBModel(object):
  """"""The PTB model.""""""

  def __init__(self, is_training, config):
    # General definition of LSTM (unrolled)
    # identical to tensorflow example ...     
    # omitted for brevity ...


    # computing the logits (also from example code)
    logits = tf.nn.xw_plus_b(output,
                             tf.get_variable(""softmax_w"", [size, vocab_size]),
                             tf.get_variable(""softmax_b"", [vocab_size]))
    loss = seq2seq.sequence_loss_by_example([logits],
                                            [tf.reshape(self._targets, [-1])],
                                            [tf.ones([batch_size * num_steps])],
                                            vocab_size)
    self._cost = cost = tf.reduce_sum(loss) / batch_size
    self._final_state = states[-1]

    # my addition: storing the probabilities and logits
    self.probabilities = tf.nn.softmax(logits)
    self.logits = logits

    # more model definition ...
</code></pre>

<p>Then printed some info about them in the <code>run_epoch</code> function:</p>

<pre><code>def run_epoch(session, m, data, eval_op, verbose=True):
  """"""Runs the model on the given data.""""""
  # first part of function unchanged from example

  for step, (x, y) in enumerate(reader.ptb_iterator(data, m.batch_size,
                                                    m.num_steps)):
    # evaluate proobability and logit tensors too:
    cost, state, probs, logits, _ = session.run([m.cost, m.final_state, m.probabilities, m.logits, eval_op],
                                 {m.input_data: x,
                                  m.targets: y,
                                  m.initial_state: state})
    costs += cost
    iters += m.num_steps

    if verbose and step % (epoch_size // 10) == 10:
      print(""%.3f perplexity: %.3f speed: %.0f wps, n_iters: %s"" %
            (step * 1.0 / epoch_size, np.exp(costs / iters),
             iters * m.batch_size / (time.time() - start_time), iters))
      chosen_word = np.argmax(probs, 1)
      print(""Probabilities shape: %s, Logits shape: %s"" % 
            (probs.shape, logits.shape) )
      print(chosen_word)
      print(""Batch size: %s, Num steps: %s"" % (m.batch_size, m.num_steps))

  return np.exp(costs / iters)
</code></pre>

<p>This produces output like this:</p>

<pre><code>0.000 perplexity: 741.577 speed: 230 wps, n_iters: 220
(20, 10000) (20, 10000)
[ 14   1   6 589   1   5   0  87   6   5   3   5   2   2   2   2   6   2  6   1]
Batch size: 1, Num steps: 20
</code></pre>

<p>I was expecting the <code>probs</code> vector to be an array of probabilities, with one for each word in the vocabulary (eg with shape <code>(1, vocab_size)</code>), meaning that I could get the predicted word using <code>np.argmax(probs, 1)</code> as suggested in the other question. </p>

<p>However, the first dimension of the vector is actually equal to the number of steps in the unrolled LSTM (20 if the small config settings are used), which I'm not sure what to do with. To access to the predicted word, do I just need to use the last value (because it's the output of the final step)? Or is there something else that I'm missing?</p>

<p>I tried to understand how the predictions are made and evaluated by looking at the implementation of <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/seq2seq.py"" rel=""nofollow noreferrer"">seq2seq.sequence_loss_by_example</a>, which must perform this evaluation, but this ends up calling <code>gen_nn_ops._sparse_softmax_cross_entropy_with_logits</code>, which doesn't seem to be included in the github repo, so I'm not sure where else to look.</p>

<p>I'm quite new to both tensorflow and LSTMs, so any help is appreciated!  </p>
",
"How to get stable results with TensorFlow, setting random seed","<p>I'm trying to run a neural network multiple times with different parameters in order to calibrate the networks parameters (dropout probabilities, learning rate e.d.). However I am having the problem that running the network while keeping the parameters the same still gives me a different solution when I run the network in a loop as follows:</p>

<pre><code>filename = create_results_file()
for i in range(3):
  g = tf.Graph()
  with g.as_default():
    accuracy_result, average_error = network.train_network(
        parameters, inputHeight, inputWidth, inputChannels, outputClasses)
    f, w = get_csv_writer(filename)
    w.writerow([accuracy_result, ""did run %d"" % i, average_error])
    f.close()
</code></pre>

<p>I am using the following code at the start of my train_network function before setting up the layers and error function of my network:</p>

<pre><code>np.random.seed(1)
tf.set_random_seed(1)
</code></pre>

<p>I have also tried adding this code before the TensorFlow graph creation, but I keep getting different solutions in my results output. </p>

<p>I am using an AdamOptimizer and am initializing network weights using <code>tf.truncated_normal</code>. Additionally I am using <code>np.random.permutation</code> to shuffle the incoming images for each epoch.</p>
",
feed data into a tf.contrib.data.Dataset like a queue,"<p>About the <code>tf.contrib.data.Dataset</code> (from TensorFlow 1.2, see <a href=""https://www.tensorflow.org/versions/r1.2/api_docs/python/tf/contrib/data"" rel=""noreferrer"">here</a> and <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/data/README.md"" rel=""noreferrer"">here</a>) usage:
The way how to get data doesn't really fit any way how I get the data usually. In my case, I have a thread and I receive data there and I don't know in advance when it will end but I see when it ends. Then I wait until I processed all the buffers and then I have finished one epoch. How can I get this logic with the <code>Dataset</code>?</p>

<p>Note that I prefer the <code>Dataset</code> interface over the <code>QueueBase</code> interface because it gives me the iterator interface which I can reinitialize and even reset to a different <code>Dataset</code>. This is more powerful compared to queues which cannot be reopened currently after they are closed (see <a href=""https://stackoverflow.com/questions/41187745/tensorflow-how-can-i-evaluate-a-validation-data-queue-multiple-times-during-tra"">here</a> and <a href=""https://github.com/tensorflow/tensorflow/issues/4535"" rel=""noreferrer"">here</a>).</p>

<p>Maybe a similar question, or the same question: How can I wrap around a <code>Dataset</code> over a queue? I have some thread with reads some data from somewhere and which can feed it and queue it somehow. How do I get the data into the <code>Dataset</code>? I could repeat some dummy tensor infinite times and then use <code>map</code> to just return my <code>queue.dequeue()</code> but that really only gets me back to all the original problems with the queue, i.e. how to reopen the queue.</p>
",
Initializing tensorflow Variable with an array larger than 2GB,"<p>I am trying to initialize a tensorflow <code>Variable</code> with pre-trained <code>word2vec</code> embeddings.</p>

<p>I have the following code:</p>

<pre><code>import tensorflow as tf
from gensim import models

model = models.Word2Vec.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)
X = model.syn0

embeddings = tf.Variable(tf.random_uniform(X.shape, minval=-0.1, maxval=0.1), trainable=False)

sess.run(tf.initialize_all_variables())

sess.run(embeddings.assign(X))
</code></pre>

<p>And I am receiving the following error:</p>

<pre><code>ValueError: Cannot create an Operation with a NodeDef larger than 2GB.
</code></pre>

<p>The array (<code>X</code>) I am trying to assign is of shape <code>(3000000, 300)</code> and its size is 3.6GB. </p>

<p>I am getting the same error if I try <code>tf.convert_to_tensor(X)</code> as well.</p>

<p>I know that it fails due to the fact that the array is larger than 2GB. However, I do not know how to assign an array larger than 2GB to a tensorflow <code>Variable</code></p>
",
Why doesn't my Deep Q Network master a simple Gridworld (Tensorflow)? (How to evaluate a Deep-Q-Net),"<p>I try to familiarize myself with Q-learning and Deep Neural Networks, currently try to implement <a href=""https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf"">Playing Atari with Deep Reinforcement Learning</a>. </p>

<p>To test my implementation and play around with it, I tought I try a simple gridworld. Where i have a N x N Grid and start in the top left corner and finishes at the bottom right. The possible actions are: left, up, right, down. </p>

<p>Even though my implementation has become very similar to <a href=""https://github.com/asrivat1/DeepLearningVideoGames/blob/master/deep_q_network.py"">this</a>(hope its a good one) it dosn't seem to learn anything. Looking at the total steps it needs to finish(I guess the average would be aroung 500 with a gridsize of 10x10, but there also very low and high values), it seams more random than anything else to me.</p>

<p>I tried it with and without convolutional layers and played around with all the parameters but to be honest, I've no idea if something with my implementation is wrong or it needs to train longer(I let it train for quite a time) or what ever. But at least it seams to converge, here the plot of the loss value one training session:</p>

<p><a href=""https://i.stack.imgur.com/JGSVK.png""><img src=""https://i.stack.imgur.com/JGSVK.png"" alt=""Loss image""></a></p>

<p>So what is the problem in this case?</p>

<p>But also and maybe more importantly how can I ""debug"" this Deep-Q-Nets, in supervised training there are training, test and validation sets and for example with precision and recall it is possible to evaluate them. What options do I have for unsupervised learning with Deep-Q-Nets, so that the next time maybe I can fix it myself?</p>

<p>Finally here is the code:</p>

<p>This is the network:</p>

<pre><code>ACTIONS = 5

# Inputs
x = tf.placeholder('float', shape=[None, 10, 10, 4])
y = tf.placeholder('float', shape=[None])
a = tf.placeholder('float', shape=[None, ACTIONS])

# Layer 1 Conv1 - input
with tf.name_scope('Layer1'):
    W_conv1 = weight_variable([8,8,4,8])
    b_conv1 = bias_variable([8])    
    h_conv1 = tf.nn.relu(conv2d(x, W_conv1, 5)+b_conv1)

# Layer 2 Conv2 - hidden1 
with tf.name_scope('Layer2'):
    W_conv2 = weight_variable([2,2,8,8])
    b_conv2 = bias_variable([8])
    h_conv2 = tf.nn.relu(conv2d(h_conv1, W_conv2, 1)+b_conv2)
    h_conv2_max_pool = max_pool_2x2(h_conv2)

# Layer 3 fc1 - hidden 2
with tf.name_scope('Layer3'):
    W_fc1 = weight_variable([8, 32])
    b_fc1 = bias_variable([32])
    h_conv2_flat = tf.reshape(h_conv2_max_pool, [-1, 8])
    h_fc1 = tf.nn.relu(tf.matmul(h_conv2_flat, W_fc1)+b_fc1)

# Layer 4 fc2 - readout
with tf.name_scope('Layer4'):
    W_fc2 = weight_variable([32, ACTIONS])
    b_fc2 = bias_variable([ACTIONS])
    readout = tf.matmul(h_fc1, W_fc2)+ b_fc2

# Training
with tf.name_scope('training'):
    readout_action = tf.reduce_sum(tf.mul(readout, a), reduction_indices=1)
    loss = tf.reduce_mean(tf.square(y - readout_action))
    train = tf.train.AdamOptimizer(1e-6).minimize(loss)

    loss_summ = tf.scalar_summary('loss', loss)
</code></pre>

<p>And here the training:</p>

<pre><code># 0 =&gt; left
# 1 =&gt; up
# 2 =&gt; right
# 3 =&gt; down
# 4 = noop

ACTIONS = 5
GAMMA = 0.95
BATCH = 50
TRANSITIONS = 2000
OBSERVATIONS = 1000
MAXSTEPS = 1000

D = deque()
epsilon = 1

average = 0
for episode in xrange(1000):
    step_count = 0
    game_ended = False

    state = np.array([0.0]*100, float).reshape(100)
    state[0] = 1

    rsh_state = state.reshape(10,10)
    s = np.stack((rsh_state, rsh_state, rsh_state, rsh_state), axis=2)

    while step_count &lt; MAXSTEPS and not game_ended:
        reward = 0
        step_count += 1

        read = readout.eval(feed_dict={x: [s]})[0]

        act = np.zeros(ACTIONS)
        action = random.randint(0,4)
        if len(D) &gt; OBSERVATIONS and random.random() &gt; epsilon:
            action = np.argmax(read)
        act[action] = 1

        # play the game
        pos_idx = state.argmax(axis=0)
        pos = pos_idx + 1

        state[pos_idx] = 0
        if action == 0 and pos%10 != 1: #left
            state[pos_idx-1] = 1
        elif action == 1 and pos &gt; 10: #up
            state[pos_idx-10] = 1
        elif action == 2 and pos%10 != 0: #right
            state[pos_idx+1] = 1
        elif action == 3 and pos &lt; 91: #down
            state[pos_idx+10] = 1
        else: #noop
            state[pos_idx] = 1
            pass

        if state.argmax(axis=0) == pos_idx and reward &gt; 0:
            reward -= 0.0001

        if step_count == MAXSTEPS:
            reward -= 100
        elif state[99] == 1: # reward &amp; finished
            reward += 100
            game_ended = True
        else:
            reward -= 1


        s_old = np.copy(s)
        s = np.append(s[:,:,1:], state.reshape(10,10,1), axis=2)

        D.append((s_old, act, reward, s))
        if len(D) &gt; TRANSITIONS:
            D.popleft()

        if len(D) &gt; OBSERVATIONS:
            minibatch = random.sample(D, BATCH)

            s_j_batch = [d[0] for d in minibatch]
            a_batch = [d[1] for d in minibatch]
            r_batch = [d[2] for d in minibatch]
            s_j1_batch = [d[3] for d in minibatch]

            readout_j1_batch = readout.eval(feed_dict={x:s_j1_batch})
            y_batch = []

            for i in xrange(0, len(minibatch)):
                y_batch.append(r_batch[i] + GAMMA * np.max(readout_j1_batch[i]))

            train.run(feed_dict={x: s_j_batch, y: y_batch, a: a_batch})

        if epsilon &gt; 0.05:
            epsilon -= 0.01
</code></pre>

<p>I appreciate every help and ideas you may have!</p>
",
Multilabel Text Classification using TensorFlow,"<p>The text data is organized as vector with 20,000 elements, like [2, 1, 0, 0, 5, ...., 0]. 
i-th element indicates the frequency of the i-th word in a text. </p>

<p>The ground truth label data is also represented as vector with 4,000 elements, like [0, 0, 1, 0, 1, ...., 0]. 
i-th element indicates whether the i-th label is a positive label for a text. 
The number of labels for a text differs depending on texts. </p>

<p>I have a code for single-label text classification. </p>

<p>How can I edit the following code for multilabel text classification?</p>

<p>Especially, I would like to know following points. </p>

<ul>
<li>How to compute accuracy using TensorFlow. </li>
<li>How to set a threshold which judges whether a label is positive or negative. For instance, if the output is [0.80, 0.43, 0.21, 0.01, 0.32] and the ground truth is [1, 1, 0, 0, 1], the labels with scores over 0.25 should be judged as positive. </li>
</ul>

<p>Thank you. </p>

<pre><code>import tensorflow as tf

# hidden Layer
class HiddenLayer(object):
    def __init__(self, input, n_in, n_out):
        self.input = input

        w_h = tf.Variable(tf.random_normal([n_in, n_out],mean = 0.0,stddev = 0.05))
        b_h = tf.Variable(tf.zeros([n_out]))

        self.w = w_h
        self.b = b_h
        self.params = [self.w, self.b]

    def output(self):
        linarg = tf.matmul(self.input, self.w) + self.b
        self.output = tf.nn.relu(linarg)

        return self.output

# output Layer
class OutputLayer(object):
    def __init__(self, input, n_in, n_out):
        self.input = input

        w_o = tf.Variable(tf.random_normal([n_in, n_out], mean = 0.0, stddev = 0.05))
        b_o = tf.Variable(tf.zeros([n_out]))

        self.w = w_o
        self.b = b_o
        self.params = [self.w, self.b]

    def output(self):
        linarg = tf.matmul(self.input, self.w) + self.b
        self.output = tf.nn.relu(linarg)

        return self.output

# model
def model():
    h_layer = HiddenLayer(input = x, n_in = 20000, n_out = 1000)
    o_layer = OutputLayer(input = h_layer.output(), n_in = 1000, n_out = 4000)

    # loss function
    out = o_layer.output()
    cross_entropy = -tf.reduce_sum(y_*tf.log(out + 1e-9), name='xentropy')    

    # regularization
    l2 = (tf.nn.l2_loss(h_layer.w) + tf.nn.l2_loss(o_layer.w))
    lambda_2 = 0.01

    # compute loss
    loss = cross_entropy + lambda_2 * l2

    # compute accuracy for single label classification task
    correct_pred = tf.equal(tf.argmax(out, 1), tf.argmax(y, 1))
    accuracy = tf.reduce_mean(tf.cast(correct_pred, ""float""))

    return loss, accuracy
</code></pre>
",
"Scalable, Efficient Hierarchical Softmax in Tensorflow?","<p>I'm interested in implementing a hierarchical softmax model that can handle large vocabularies, say on the order of 10M classes. What is the best way to do this to both be scalable to large class counts and efficient? For instance, at least <a href=""https://arxiv.org/abs/1512.04906"" rel=""noreferrer"">one paper</a> has shown that HS can achieve a ~25x speedup for large vocabs when using a 2-level tree where each node <code>sqrt(N)</code> classes. I'm interested also in a more general version for an arbitrary depth tree with an arbitrary branching factor.</p>

<p>There are a few options that I see here:</p>

<p>1) Run <code>tf.gather</code> for every batch, where we gather the indices and splits. This creates problems with large batch sizes and fat trees where now the coefficients are being duplicated a lot, leading to OOM errors.</p>

<p>2) Similar to #1, we could use <code>tf.embedding_lookup</code> which would keep help with OOM errors but now keeps everything on the CPU and slows things down quite a bit.</p>

<p>3) Use <code>tf.map_fn</code> with <code>parallel_iterations=1</code> to process each sample separately and go back to using gather. This is much more scalable but does not really get close to the 25x speedup due to the serialization.</p>

<p>Is there a better way to implement HS? Are there different ways for deep and narrow vs. short and wide trees?</p>
",
How does TensorFlow's MultiRnnCell work?,"<p>Could someone help to explain the inner mechanism of TensorFlow's <code>tf.contrib.rnn.MultiRnnCell</code>? </p>

<p>For example, if I wanted to stack up two basic RNN cells into a <code>MultiRnnCell</code>, what would be the input and output of each basic RNN cell? </p>

<p>I would like to know the details of how it works.</p>
",
"Unable to start TensorFlow within Docker, on Windows","<p>Hope I didn't miss anything.<br>
I've installed docker on my win 7 using this guide:<br>
<a href=""https://docs.docker.com/engine/installation/"">https://docs.docker.com/engine/installation/</a><br>
I opened a new terminal and entered the following command:  </p>

<pre><code>docker run -it b.gcr.io/tensorflow/tensorflow
</code></pre>

<p>All donwloaded and extracted and then I get the following massages:</p>

<pre><code>[I 16:09:55.069 NotebookApp] Writing notebook server cookie secret to /root/.local/share/jupyter/runtime/notebook_cookie_secret 
[W 16:09:55.122 NotebookApp] WARNING: The notebook server is listening on all IP
 addresses and not using encryption. This is not recommended. 
[W 16:09:55.122 NotebookApp] WARNING: The notebook server is listening on all IP
 addresses and not using authentication. This is highly insecure and not recommended.
[I 16:09:55.134 NotebookApp] Serving notebooks from local directory: /notebooks 
[I 16:09:55.134 NotebookApp] 0 active kernels
[I 16:09:55.134 NotebookApp] The Jupyter Notebook is running at: http://[all ip addresses on your system]:8888/ 
[I 16:09:55.134 NotebookApp] Use Control-C to stop this server and shut down all
 kernels (twice to skip confirmation). 
</code></pre>

<p>And then it just gets stuck like this, there's no command line and I can't enter anything... what am I missing?</p>
",
How to read data into Tensorflow?,"<p>I'm trying to read data from CSV files to tensorflow,</p>

<p><a href=""https://www.tensorflow.org/versions/r0.7/how_tos/reading_data/index.html#filenames-shuffling-and-epoch-limits"" rel=""nofollow noreferrer"">https://www.tensorflow.org/versions/r0.7/how_tos/reading_data/index.html#filenames-shuffling-and-epoch-limits</a></p>

<p>The sample code in official document is like this:</p>

<pre><code>col1, col2, col3, col4, col5 = tf.decode_csv(value, record_defaults=record_defaults)
</code></pre>

<p>To read the file, I need to know how many columns and lines in the file beforehand, and if there are 1000 columns, I need to define 1000 variables like <code>col1, col2, col3, col4, col5,..., col1000 ,</code> this doesn't look like an efficient way to read data.</p>

<p>My questions</p>

<ol>
<li><p>What is the best way to read CSV files into Tensorflow ?</p></li>
<li><p>Is there any way to read Database (such as mongoDB) in Tensorflow ?</p></li>
</ol>
",
Tensorflow `set_random_seed` not working,"<h3>Calling <code>tf.set_random_seed(SEED)</code> has no effect that I can tell...</h3>

<p>For example, running the code below several times <em>inside an IPython notebook</em> produces different output each time:</p>

<pre><code>import tensorflow as tf
tf.set_random_seed(42)
sess = tf.InteractiveSession()
a = tf.constant([1, 2, 3, 4, 5])
tf.initialize_all_variables().run()
a_shuf = tf.random_shuffle(a)
print(a.eval())
print(a_shuf.eval())
sess.close()
</code></pre>

<p>If I set the seed explicitly: <code>a_shuf = tf.random_shuffle(a, seed=42)</code>, the output is the same after each run. But why do I need to set the seed if I already call <code>tf.set_random_seed(42)</code>?</p>

<hr>

<p>The equivalent code using numpy just works:</p>

<pre><code>import numpy as np
np.random.seed(42)
a = [1,2,3,4,5]
np.random.shuffle(a)
print(a)
</code></pre>
",
TensorFlow Inference,"<p>I've been digging around on this for a while.  I have found a ton of articles; but none really show just tensorflow inference as a plain inference.  Its always ""use the serving engine"" or using a graph that is pre-coded/defined.</p>

<p>Here is the problem: I have a device which occasionally checks for updated models.  It then needs to load that model and run input predictions through the model.</p>

<p>In keras this was simple: build a model; train the model and the call model.predict().  In scikit-learn same thing.</p>

<p>I am able to grab a new model and load it; I can print out all of the weights; but how in the world do I run inference against it?</p>

<p>Code to load model and print weights:</p>

<pre><code>    with tf.Session() as sess:
        new_saver = tf.train.import_meta_graph(MODEL_PATH + '.meta', clear_devices=True)
        new_saver.restore(sess, MODEL_PATH)
        for var in tf.trainable_variables():
            print(sess.run(var))
</code></pre>

<p>I printed out all of my collections and I have:
['queue_runners', 'variables', 'losses', 'summaries', 'train_op', 'cond_context', 'trainable_variables']</p>

<p>I tried using sess.run(train_op); however that just started kicking up a full training session; which is not what I want to do.  I just want to run inference against a different set of inputs that I provide which are not TF Records.</p>

<p>Just a little more detail:</p>

<p>The device can use C++ or Python; as long as I can produce a .exe.  I can set up a feed dict if I want to feed the system.  I trained with TFRecords; but in production I'm not going to be using TFRecords; its a real/near real time system.</p>

<p>Thanks for any input.  I am posting sample code to this repo: <a href=""https://github.com/drcrook1/CIFAR10/TensorFlow"" rel=""noreferrer"">https://github.com/drcrook1/CIFAR10/TensorFlow</a> which does all the training and sample inference.</p>

<p>Any hints are greatly appreciated! </p>

<p>------------EDITS-----------------
I rebuilt the model to be as below:</p>

<pre><code>def inference(images):
'''
Portion of the compute graph that takes an input and converts it into a Y output
'''
with tf.variable_scope('Conv1') as scope:
    C_1_1 = ld.cnn_layer(images, (5, 5, 3, 32), (1, 1, 1, 1), scope, name_postfix='1')
    C_1_2 = ld.cnn_layer(C_1_1, (5, 5, 32, 32), (1, 1, 1, 1), scope, name_postfix='2')
    P_1 = ld.pool_layer(C_1_2, (1, 2, 2, 1), (1, 2, 2, 1), scope)
with tf.variable_scope('Dense1') as scope:
    P_1 = tf.reshape(C_1_2, (CONSTANTS.BATCH_SIZE, -1))
    dim = P_1.get_shape()[1].value
    D_1 = ld.mlp_layer(P_1, dim, NUM_DENSE_NEURONS, scope, act_func=tf.nn.relu)
with tf.variable_scope('Dense2') as scope:
    D_2 = ld.mlp_layer(D_1, NUM_DENSE_NEURONS, CONSTANTS.NUM_CLASSES, scope)
H = tf.nn.softmax(D_2, name='prediction')
return H
</code></pre>

<p>notice I add the name 'prediction' to the TF operation so I can retrieve it later.</p>

<p>When training I used the input pipeline for tfrecords and input queues.</p>

<pre><code>GRAPH = tf.Graph()
with GRAPH.as_default():
    examples, labels = Inputs.read_inputs(CONSTANTS.RecordPaths,
                                          batch_size=CONSTANTS.BATCH_SIZE,
                                          img_shape=CONSTANTS.IMAGE_SHAPE,
                                          num_threads=CONSTANTS.INPUT_PIPELINE_THREADS)
    examples = tf.reshape(examples, [CONSTANTS.BATCH_SIZE, CONSTANTS.IMAGE_SHAPE[0],
                                     CONSTANTS.IMAGE_SHAPE[1], CONSTANTS.IMAGE_SHAPE[2]])
    logits = Vgg3CIFAR10.inference(examples)
    loss = Vgg3CIFAR10.loss(logits, labels)
    OPTIMIZER = tf.train.AdamOptimizer(CONSTANTS.LEARNING_RATE)
</code></pre>

<p>I am attempting to use feed_dict on the loaded operation in the graph; however now it is just simply hanging....</p>

<pre><code>MODEL_PATH = 'models/' + CONSTANTS.MODEL_NAME + '.model'

images = tf.placeholder(tf.float32, shape=(1, 32, 32, 3))

def run_inference():
'''Runs inference against a loaded model'''
with tf.Session() as sess:
    #sess.run(tf.global_variables_initializer())
    new_saver = tf.train.import_meta_graph(MODEL_PATH + '.meta', clear_devices=True)
    new_saver.restore(sess, MODEL_PATH)
    pred = tf.get_default_graph().get_operation_by_name('prediction')
    rand = np.random.rand(1, 32, 32, 3)
    print(rand)
    print(pred)
    print(sess.run(pred, feed_dict={images: rand}))
    print('done')

run_inference()
</code></pre>

<p>I believe this is not working because the original network was trained using TFRecords.  In the sample CIFAR data set the data is small; our real data set is huge and it is my understanding TFRecords the the default best practice for training a network.  The feed_dict makes great perfect sense from a productionizing perspective; we can spin up some threads and populate that thing from our input systems.</p>

<p>So I guess I have a network that is trained, I can get the predict operation; but how do I tell it to stop using the input queues and start using the feed_dict?  Remember that from the production perspective I do not have access to whatever the scientists did to make it.  They do their thing; and we stick it in production using whatever agreed upon standard.</p>

<p>-------INPUT OPS--------</p>

<p>tf.Operation 'input/input_producer/Const' type=Const, tf.Operation 'input/input_producer/Size' type=Const, tf.Operation 'input/input_producer/Greater/y' type=Const, tf.Operation 'input/input_producer/Greater' type=Greater, tf.Operation 'input/input_producer/Assert/Const' type=Const, tf.Operation 'input/input_producer/Assert/Assert/data_0' type=Const, tf.Operation 'input/input_producer/Assert/Assert' type=Assert, tf.Operation 'input/input_producer/Identity' type=Identity, tf.Operation 'input/input_producer/RandomShuffle' type=RandomShuffle, tf.Operation 'input/input_producer' type=FIFOQueueV2, tf.Operation 'input/input_producer/input_producer_EnqueueMany' type=QueueEnqueueManyV2, tf.Operation 'input/input_producer/input_producer_Close' type=QueueCloseV2, tf.Operation 'input/input_producer/input_producer_Close_1' type=QueueCloseV2, tf.Operation 'input/input_producer/input_producer_Size' type=QueueSizeV2, tf.Operation 'input/input_producer/Cast' type=Cast, tf.Operation 'input/input_producer/mul/y' type=Const, tf.Operation 'input/input_producer/mul' type=Mul, tf.Operation 'input/input_producer/fraction_of_32_full/tags' type=Const, tf.Operation 'input/input_producer/fraction_of_32_full' type=ScalarSummary, tf.Operation 'input/TFRecordReaderV2' type=TFRecordReaderV2, tf.Operation 'input/ReaderReadV2' type=ReaderReadV2,</p>

<p>------END INPUT OPS-----</p>

<p>----UPDATE 3----</p>

<p>I believe what I need to do is to kill the input section of the graph trained with TF Records and rewire the input to the first layer to a new input.  Its kinda like performing surgery; but this is the only way I can find to do inference if I trained using TFRecords as crazy as it sounds...</p>

<p>Full Graph:</p>

<p><a href=""https://i.stack.imgur.com/xFrYN.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/xFrYN.png"" alt=""enter image description here""></a></p>

<p>Section to kill:</p>

<p><a href=""https://i.stack.imgur.com/mRpEf.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/mRpEf.png"" alt=""enter image description here""></a></p>

<p>So I think the question becomes: How does one kill the input section of the graph and replace it with a feed_dict?</p>

<p>A follow up to this would be: is this really the right way to do it?  This seems bonkers.</p>

<p>----END UPDATE 3----</p>

<p>---link to checkpoint files---</p>

<p><a href=""https://drcdata.blob.core.windows.net/checkpoints/CIFAR_10_VGG3_50neuron_1pool_1e-3lr_adam.model.zip?st=2017-05-01T21%3A56%3A00Z&amp;se=2020-05-02T21%3A56%3A00Z&amp;sp=rl&amp;sv=2015-12-11&amp;sr=b&amp;sig=oBCGxlOusB4NOEKnSnD%2FTlRYa5NKNIwAX1IyuZXAr9o%3D"" rel=""noreferrer"">https://drcdata.blob.core.windows.net/checkpoints/CIFAR_10_VGG3_50neuron_1pool_1e-3lr_adam.model.zip?st=2017-05-01T21%3A56%3A00Z&amp;se=2020-05-02T21%3A56%3A00Z&amp;sp=rl&amp;sv=2015-12-11&amp;sr=b&amp;sig=oBCGxlOusB4NOEKnSnD%2FTlRYa5NKNIwAX1IyuZXAr9o%3D</a></p>

<p>--end link to checkpoint files---</p>

<p>-----UPDATE 4 -----</p>

<p>I gave in and just gave a shot at the 'normal' way of performing inference assuming I could have the scientists simply just pickle their models and we could grab the model pickle; unpack it and then run inference on it.  So to test I tried the normal way assuming we already unpacked it...It doesn't work worth a beans either...</p>

<pre><code>import tensorflow as tf
import CONSTANTS
import Vgg3CIFAR10
import numpy as np
from scipy import misc
import time

MODEL_PATH = 'models/' + CONSTANTS.MODEL_NAME + '.model'
imgs_bsdir = 'C:/data/cifar_10/train/'

images = tf.placeholder(tf.float32, shape=(1, 32, 32, 3))

logits = Vgg3CIFAR10.inference(images)

def run_inference():
'''Runs inference against a loaded model'''
with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    new_saver = tf.train.import_meta_graph(MODEL_PATH + '.meta')#, import_scope='1', input_map={'input:0': images})
    new_saver.restore(sess, MODEL_PATH)
    pred = tf.get_default_graph().get_operation_by_name('prediction')
    enq = sess.graph.get_operation_by_name(enqueue_op)
    #tf.train.start_queue_runners(sess)
    print(rand)
    print(pred)
    print(enq)
    for i in range(1, 25):
        img = misc.imread(imgs_bsdir + str(i) + '.png').astype(np.float32) / 255.0
        img = img.reshape(1, 32, 32, 3)
        print(sess.run(logits, feed_dict={images : img}))
        time.sleep(3)
    print('done')

run_inference()
</code></pre>

<p>Tensorflow ends up building a new graph with the inference function from the loaded model; then it appends all the other stuff from the other graph to the end of it.  So then when I populate a feed_dict expecting to get inferences back; I just get a bunch of random garbage as if it were the first pass through the network...</p>

<p>Again; this seems nuts; do I really need to write my own framework for serializing and deserializing random networks?  This has had to have been done before...</p>

<p>-----UPDATE 4 -----</p>

<p>Again; thanks!</p>
",
How to effectively apply gradient clipping in tensor flow?,"<p>Considering the <a href=""https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3%20-%20Neural%20Networks/recurrent_network.py"">example code</a>.</p>

<p>I would like to know How to apply gradient clipping on this network on the RNN where there is a possibility of exploding gradients.</p>

<pre><code>tf.clip_by_value(t, clip_value_min, clip_value_max, name=None)
</code></pre>

<p>This is an example that could be used but where do I introduce this ?
In the def of RNN </p>

<pre><code>    lstm_cell = rnn_cell.BasicLSTMCell(n_hidden, forget_bias=1.0)
    # Split data because rnn cell needs a list of inputs for the RNN inner loop
    _X = tf.split(0, n_steps, _X) # n_steps
tf.clip_by_value(_X, -1, 1, name=None)
</code></pre>

<p>But this doesn't make sense as the tensor _X is the input and not the grad what is to be clipped? </p>

<p>Do I have to define my own Optimizer for this or is there a simpler option?</p>
",
What's difference between tf.sub and just minus operation in tensorflow?,"<p>I am trying to use Tensorflow. Here is an very simple code.</p>

<pre><code>train = tf.placeholder(tf.float32, [1], name=""train"")
W1 = tf.Variable(tf.truncated_normal([1], stddev=0.1), name=""W1"")
loss = tf.pow(tf.sub(train, W1), 2)
step = tf.train.GradientDescentOptimizer(0.1).minimize(loss)
</code></pre>

<p>Just ignore the optimization part (4th line). It will take a floating number and train W1 so as to increase squared difference.</p>

<p>My question is simple. If I use just minus sign instead of 
tf.sub"" as below, what is different? Will it cause a wrong result? </p>

<pre><code>loss = tf.pow(train-W1, 2)
</code></pre>

<p>When I replace it, the result looks the same. If they are the same, why do we need to use the ""tf.add/tf.sub"" things?</p>

<p>Built-in back propagation calculation can be done only by the ""tf.*"" things? </p>
",
How to interpret TensorFlow output?,"<p>How do I interpret the TensorFlow output for building and executing computational graphs on GPGPUs?</p>

<p>Given the following command that executes an arbitrary tensorflow script using the python API.</p>

<blockquote>
  <blockquote>
    <p>python3 tensorflow_test.py > out</p>
  </blockquote>
</blockquote>

<p>The first part <code>stream_executor</code> seems like its loading dependencies.</p>

<pre><code>I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally
</code></pre>

<p>What is a <code>NUMA</code> node?</p>

<pre><code>I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:900] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
</code></pre>

<p>I assume this is when it finds the available GPU</p>

<pre><code>I tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: 
name: Tesla K40c
major: 3 minor: 5 memoryClockRate (GHz) 0.745
pciBusID 0000:01:00.0
Total memory: 11.25GiB
Free memory: 11.15GiB
</code></pre>

<p>Some gpu initialization? what is DMA?</p>

<pre><code>I tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 
I tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: Tesla K40c, pci bus id: 0000:01:00.0)
</code></pre>

<p>Why does it throw an error <code>E</code>?</p>

<pre><code>E tensorflow/stream_executor/cuda/cuda_driver.cc:932] failed to allocate 11.15G (11976531968 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
</code></pre>

<p>Great answer to what the <code>pool_allocator</code> does: <a href=""https://stackoverflow.com/a/35166985/4233809"">https://stackoverflow.com/a/35166985/4233809</a></p>

<pre><code>I tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 3160 get requests, put_count=2958 evicted_count=1000 eviction_rate=0.338066 and unsatisfied allocation rate=0.412025
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:256] Raising pool_size_limit_ from 100 to 110
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 1743 get requests, put_count=1970 evicted_count=1000 eviction_rate=0.507614 and unsatisfied allocation rate=0.456684
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:256] Raising pool_size_limit_ from 256 to 281
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 1986 get requests, put_count=2519 evicted_count=1000 eviction_rate=0.396983 and unsatisfied allocation rate=0.264854
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:256] Raising pool_size_limit_ from 655 to 720
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 28728 get requests, put_count=28680 evicted_count=1000 eviction_rate=0.0348675 and unsatisfied allocation rate=0.0418407
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:256] Raising pool_size_limit_ from 1694 to 1863
</code></pre>
",
Siamese Neural Network in TensorFlow,"<p>I'm trying to implement a Siamese Neural Network in TensorFlow (<a href=""https://i.stack.imgur.com/qfxH8.png"" rel=""noreferrer"">siamese network example</a>) but I cannot really find any working example on the Internet (see <a href=""https://papers.nips.cc/paper/769-signature-verification-using-a-siamese-time-delay-neural-network.pdf"" rel=""noreferrer"">Yann LeCun paper</a>).</p>

<p>The architecture I'm trying to build would consist of two LSTMs sharing weights and only connected at the end of the network.</p>

<p>My question is: how to build two different neural networks sharing their weights (tied weights) in TensorFlow and how to connect them at the end?</p>

<p>Thanks :)</p>

<p><strong>Edit</strong>: I implemented a simple and working example of a siamese network <a href=""https://github.com/benmyara/deeplearning-tensorflow/blob/master/notebooks/1_siamese.ipynb"" rel=""noreferrer"">here</a> on MNIST.</p>
",
overcome Graphdef cannot be larger than 2GB in tensorflow,"<p>I am using tensorflow's <a href=""https://github.com/tensorflow/tensorflow/tree/master/tensorflow/models/image/imagenet"" rel=""nofollow noreferrer"">imageNet trained model</a> to extract the last pooling layer's features as representation vectors for a new dataset of images. </p>

<p>The model as is predicts on a new image as follows: </p>

<pre><code>python classify_image.py --image_file new_image.jpeg 
</code></pre>

<p>I edited the main function so that I can take a folder of images and return the prediction on all images at once and write the feature vectors in a csv file. Here is how I did that: </p>

<pre><code>def main(_):
  maybe_download_and_extract()
  #image = (FLAGS.image_file if FLAGS.image_file else
  #         os.path.join(FLAGS.model_dir, 'cropped_panda.jpg'))
  #edit to take a directory of image files instead of a one file
  if FLAGS.data_folder:
    images_folder=FLAGS.data_folder
    list_of_images = os.listdir(images_folder)
  else: 
    raise ValueError(""Please specify image folder"")

  with open(""feature_data.csv"", ""wb"") as f:
    feature_writer = csv.writer(f, delimiter='|')

    for image in list_of_images:
      print(image) 
      current_features = run_inference_on_image(images_folder+""/""+image)
      feature_writer.writerow([image]+current_features)
</code></pre>

<p>It worked just fine for around 21 images but then crashed with the following error: </p>

<pre><code>  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1912, in as_graph_def
    raise ValueError(""GraphDef cannot be larger than 2GB."")
ValueError: GraphDef cannot be larger than 2GB.
</code></pre>

<p>I thought by calling the method <code>run_inference_on_image(images_folder+""/""+image)</code> the previous image data would be overwritten to only consider the new image data, which doesn't seem to be the case. How to resolve this issue? </p>
",
What is Bazel in TensorFlow? When do I need to build again?,"<p>I'm new to Bazel. I'm not sure how this thing works. On the <a href=""https://www.tensorflow.org/versions/v0.6.0/get_started/os_setup.html"" rel=""noreferrer"">TF website</a>, there's this section on ""Create the pip package and install"".</p>

<pre><code>$ bazel build -c opt //tensorflow/tools/pip_package:build_pip_package

# To build with GPU support: 
$ bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package

$ bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg

# The name of the .whl file will depend on your platform. 
$ pip install /tmp/tensorflow_pkg/tensorflow-0.5.0-cp27-none-linux_x86_64.whl
</code></pre>

<p>Here's the situation:</p>

<ol>
<li>There's a new commit on the master branch of TensorFlow and I merge it into my fork.</li>
<li>I need to rebuild the wheel and do a pip install of the new wheel (correct me if I am wrong).</li>
<li>I ./configure first, then bazel build, then bazel-bin, then pip install.</li>
</ol>

<p>Is this the correct way to properly update changes from master? The bazel build step takes a really long time.</p>
",
Does tensorflow use automatic or symbolic gradients?,"<p>I haven't been able to find a clear statement of whether tensorflow uses automatic or symbolic differentiation.</p>

<p>I skimmed the tensorflow <a href=""http://download.tensorflow.org/paper/whitepaper2015.pdf"" rel=""nofollow noreferrer"">paper</a> and they mention automatic gradients, but it is unclear if they just mean symbolic gradients, as they also mention that it has that capability.</p>
",
Clarification on tf.Tensor.set_shape(),"<p>I have an image that is 478 x 717 x 3 = 1028178 pixels, with a rank of 1. I verified it by calling tf.shape and tf.rank.</p>

<p>When I call image.set_shape([478, 717, 3]), it throws the following error.</p>

<pre><code>""Shapes %s and %s must have the same rank"" % (self, other)) 
ValueError: Shapes (?,) and (478, 717, 3) must have the same rank
</code></pre>

<p>I tested again by first casting to 1028178, but the error still exists.</p>

<pre><code>ValueError: Shapes (1028178,) and (478, 717, 3) must have the same rank
</code></pre>

<p>Well, that does make sense because one is of rank 1 and the other is of rank 3. However, why is it necessary to throw an error, as the total number of pixels still match.</p>

<p>I could of course use tf.reshape and it works, but I think that's not optimal.</p>

<p>As stated on the TensorFlow FAQ</p>

<blockquote>
  <p>What is the difference between x.set_shape() and x = tf.reshape(x)?</p>
  
  <p>The tf.Tensor.set_shape() method updates the static shape of a Tensor
  object, and it is typically used to provide additional shape
  information when this cannot be inferred directly. It does not change
  the dynamic shape of the tensor.</p>
  
  <p>The tf.reshape() operation creates a new tensor with a different dynamic shape.</p>
</blockquote>

<p>Creating a new tensor involves memory allocation and that could potentially be more costly when more training examples are involved. Is this by design, or am I  missing something here?</p>
",
Confused about conv2d_transpose,"<p>I'm getting this error message when using <code>conv2d_transpose</code>:</p>

<pre><code>W tensorflow/core/common_runtime/executor.cc:1102] 0x7fc81f0d6250 Compute status: Invalid argument: Conv2DBackpropInput: Number of rows of out_backprop doesn't match computed: actual = 32, computed = 4
 [[Node: generator/g_h1/conv2d_transpose = Conv2DBackpropInput[T=DT_FLOAT, padding=""SAME"", strides=[1, 2, 2, 1], use_cudnn_on_gpu=true, _device=""/job:localhost/replica:0/task:0/cpu:0""](generator/g_h1/conv2d_transpose/output_shape, generator/g_h1/w/read, _recv_l_0)]]
</code></pre>

<p>However, it occurs after the graph is built while compiling the loss function (Adam). Any ideas on what would cause this? I suspect it's related to the input dimensions but I'm not sure exactly why.</p>

<p>Full error: <a href=""https://gist.github.com/jimfleming/75d88e888044615dd6e3"">https://gist.github.com/jimfleming/75d88e888044615dd6e3</a></p>

<p>Relevant code:</p>

<pre><code># l shape: [batch_size, 32, 32, 4]

output_shape = [self.batch_size, 8, 8, 128]
filter_shape = [7, 7, 128, l.get_shape()[-1]]
strides = [1, 2, 2, 1]
with tf.variable_scope(""g_h1""):
    w = tf.get_variable('w', filter_shape, initializer=tf.random_normal_initializer(stddev=0.02))
    h1 = tf.nn.conv2d_transpose(l, w, output_shape=output_shape, strides=strides, padding='SAME')
    h1 = tf.nn.relu(h1)

output_shape = [self.batch_size, 16, 16, 128]
filter_shape = [7, 7, 128, h1.get_shape()[-1]]
strides = [1, 2, 2, 1]
with tf.variable_scope(""g_h2""):
    w = tf.get_variable('w', filter_shape, initializer=tf.random_normal_initializer(stddev=0.02))
    h2 = tf.nn.conv2d_transpose(h1, w,output_shape=output_shape,  strides=strides, padding='SAME')
    h2 = tf.nn.relu(h2)

output_shape = [self.batch_size, 32, 32, 3]
filter_shape = [5, 5, 3, h2.get_shape()[-1]]
strides = [1, 2, 2, 1]
with tf.variable_scope(""g_h3""):
    w = tf.get_variable('w', filter_shape, initializer=tf.random_normal_initializer(stddev=0.02))
    h3 = tf.nn.conv2d_transpose(h2, w,output_shape=output_shape,  strides=strides, padding='SAME')
    h3 = tf.nn.tanh(h3)
</code></pre>
",
Tensorflow Different ways to Export and Run graph in C++,"<p>For importing your trained network to the C++ you need to export your network to be able to do so. After searching a lot and finding almost no information about it, it was clarified that we should use <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/freeze_graph.py"" rel=""noreferrer"">freeze_graph()</a> to be able to do it.</p>

<p>Thanks to the new 0.7 version of Tensorflow, they added <a href=""https://www.tensorflow.org/versions/r0.7/api_docs/python/train.html#export_meta_graph"" rel=""noreferrer"">documentation</a> of it. </p>

<p>After looking into documentations, I found that there are few similar methods, can you tell what is the difference between <code>freeze_graph()</code> and:
 <code>tf.train.export_meta_graph</code> as it has similar parameters, but it seems it can also be used for importing models to C++ (I just guess the difference is that for using the file output by this method you can only use <code>import_graph_def()</code> or it's something else?)</p>

<p>Also one question about how to use <code>write_graph()</code>:
In documentations the <code>graph_def</code> is given by <code>sess.graph_def</code> but in examples in <code>freeze_graph()</code> it is <code>sess.graph.as_graph_def()</code>. What is the difference between these two? </p>

<p>This question is related to <a href=""https://github.com/tensorflow/tensorflow/issues/615"" rel=""noreferrer"">this issue.</a></p>

<p>Thank you!</p>
",
Tensorflow Tensorboard default port,"<p>Is there a way to change the default port (""6006"") on tensorboard so we could open multiple tensorboard? Maybe an option like --port=""8008"" ?</p>
",
Tensorflow get all variables in scope,"<p>I have some variables created within a certain scope like this:</p>

<pre><code>with tf.variable_scope(""my_scope""):
  createSomeVariables()
  ...
</code></pre>

<p>I then want to get the list of all the variables in ""my_scope"" so I can pass it to an optimizer. What is the right way to do this?</p>
",
TensorFlow using LSTMs for generating text,"<p>I would like to use tensorflow to generate text and have been modifying the LSTM tutorial (<a href=""https://www.tensorflow.org/versions/master/tutorials/recurrent/index.html#recurrent-neural-networks"">https://www.tensorflow.org/versions/master/tutorials/recurrent/index.html#recurrent-neural-networks</a>) code to do this, however my initial solution seems to generate nonsense, even after training for a long time, it does not improve. I fail to see why. The idea is to start with a zero matrix and then generate one word at a time.</p>

<p>This is the code, to which I've added the two functions below
<a href=""https://tensorflow.googlesource.com/tensorflow/+/master/tensorflow/models/rnn/ptb/ptb_word_lm.py"">https://tensorflow.googlesource.com/tensorflow/+/master/tensorflow/models/rnn/ptb/ptb_word_lm.py</a></p>

<p>The generator looks as follows</p>

<pre><code>def generate_text(session,m,eval_op):

    state = m.initial_state.eval()

    x = np.zeros((m.batch_size,m.num_steps), dtype=np.int32)

    output = str()
    for i in xrange(m.batch_size):
        for step in xrange(m.num_steps):
            try:
                # Run the batch 
                # targets have to bee set but m is the validation model, thus it should not train the neural network
                cost, state, _, probabilities = session.run([m.cost, m.final_state, eval_op, m.probabilities],
                                                            {m.input_data: x, m.targets: x, m.initial_state: state})

                # Sample a word-id and add it to the matrix and output
                word_id = sample(probabilities[0,:])
                output = output + "" "" + reader.word_from_id(word_id)
                x[i][step] = word_id

            except ValueError as e:
                print(""ValueError"")

    print(output)
</code></pre>

<p>I have added the variable ""probabilities"" to the ptb_model and it is simply a softmax over the logits.</p>

<pre><code>self._probabilities = tf.nn.softmax(logits)
</code></pre>

<p>And the sampling:</p>

<pre><code>def sample(a, temperature=1.0):
    # helper function to sample an index from a probability array
    a = np.log(a) / temperature
    a = np.exp(a) / np.sum(np.exp(a))
    return np.argmax(np.random.multinomial(1, a, 1))
</code></pre>
",
"TensorFlow, why was python the chosen language?","<p>I recently started studying deep learning and other ML techniques, and I started searching for frameworks that simplify the process of build a net and training it, then I found TensorFlow, having little experience in the field, for me, it seems that speed is a big factor for making a big ML system even more if working with deep learning, so why python was chosen by Google to make TensorFlow? Wouldn't it be better to make it over an language that can be compiled and not interpreted?</p>

<p>What are the advantages of using Python over a language like C++ for machine learning?</p>
",
TensorFlow: getting variable by name,"<p>When using the TensorFlow Python API, I created a variable (without specifying its <code>name</code> in the constructor), and its <code>name</code> property had the value <code>""Variable_23:0""</code>.  When I try to select this variable using <code>tf.get_variable(""Variable23"")</code>, a new variable called <code>""Variable_23_1:0""</code> is created instead. How do I correctly select <code>""Variable_23""</code> instead of creating a new one? </p>

<p>What I want to do is select the variable by name, and reinitialize it so I can finetune weights.</p>
",
Using a pre-trained word embedding (word2vec or Glove) in TensorFlow,"<p>I've recently reviewed an interesting implementation for <a href=""http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/"">convolutional text classification</a>. However all TensorFlow code I've reviewed uses a random (not pre-trained) embedding vectors like the following:</p>

<pre><code>with tf.device('/cpu:0'), tf.name_scope(""embedding""):
    W = tf.Variable(
        tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0),
        name=""W"")
    self.embedded_chars = tf.nn.embedding_lookup(W, self.input_x)
    self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)
</code></pre>

<p>Does anybody know how to use the results of Word2vec or a GloVe pre-trained word embedding instead of a random one?</p>
",
"TensorFlow: ""Attempting to use uninitialized value"" in variable initialization","<p>I am trying to implement multivariate linear regression in Python using TensorFlow, but have run into some logical and implementation issues. My code throws the following error:</p>

<pre><code>Attempting to use uninitialized value Variable
Caused by op u'Variable/read'
</code></pre>

<p>Ideally the <code>weights</code> output should be <code>[2, 3]</code></p>

<pre><code>def hypothesis_function(input_2d_matrix_trainingexamples,
                        output_matrix_of_trainingexamples,
                        initial_parameters_of_hypothesis_function,
                        learning_rate, num_steps):
    # calculate num attributes and num examples
    number_of_attributes = len(input_2d_matrix_trainingexamples[0])
    number_of_trainingexamples = len(input_2d_matrix_trainingexamples)

    #Graph inputs
    x = []
    for i in range(0, number_of_attributes, 1):
        x.append(tf.placeholder(""float""))
    y_input = tf.placeholder(""float"")

    # Create Model and Set Model weights
    parameters = []
    for i in range(0, number_of_attributes, 1):
        parameters.append(
            tf.Variable(initial_parameters_of_hypothesis_function[i]))

    #Contruct linear model
    y = tf.Variable(parameters[0], ""float"")
    for i in range(1, number_of_attributes, 1):
        y = tf.add(y, tf.multiply(x[i], parameters[i]))

    # Minimize the mean squared errors
    loss = tf.reduce_mean(tf.square(y - y_input))
    optimizer = tf.train.GradientDescentOptimizer(learning_rate)
    train = optimizer.minimize(loss)

    #Initialize the variables
    init = tf.initialize_all_variables()

    # launch the graph
    session = tf.Session()
    session.run(init)
    for step in range(1, num_steps + 1, 1):
        for i in range(0, number_of_trainingexamples, 1):
            feed = {}
            for j in range(0, number_of_attributes, 1):
                array = [input_2d_matrix_trainingexamples[i][j]]
                feed[j] = array
            array1 = [output_matrix_of_trainingexamples[i]]
            feed[number_of_attributes] = array1
            session.run(train, feed_dict=feed)

    for i in range(0, number_of_attributes - 1, 1):
        print (session.run(parameters[i]))

array = [[0.0, 1.0, 2.0], [0.0, 2.0, 3.0], [0.0, 4.0, 5.0]]
hypothesis_function(array, [8.0, 13.0, 23.0], [1.0, 1.0, 1.0], 0.01, 200)
</code></pre>
",
Tensorflow: How to get a tensor by name?,"<p>I'm having trouble recovering a tensor by name, I don't even know if it's possible.</p>

<p>I have a function that creates my graph:</p>

<pre><code>def create_structure(tf, x, input_size,dropout):    
 with tf.variable_scope(""scale_1"") as scope:
  W_S1_conv1 = deep_dive.weight_variable_scaling([7,7,3,64], name='W_S1_conv1')
  b_S1_conv1 = deep_dive.bias_variable([64])
  S1_conv1 = tf.nn.relu(deep_dive.conv2d(x_image, W_S1_conv1,strides=[1, 2, 2, 1], padding='SAME') + b_S1_conv1, name=""Scale1_first_relu"")
.
.
.
return S3_conv1,regularizer
</code></pre>

<p>I want to access the variable S1_conv1 outside this function. I tried:</p>

<pre><code>with tf.variable_scope('scale_1') as scope_conv: 
 tf.get_variable_scope().reuse_variables()
 ft=tf.get_variable('Scale1_first_relu')
</code></pre>

<p>But that is giving me an error:</p>

<p>ValueError: Under-sharing: Variable scale_1/Scale1_first_relu does not exist, disallowed. Did you mean to set reuse=None in VarScope?</p>

<p>But this works:</p>

<pre><code>with tf.variable_scope('scale_1') as scope_conv: 
 tf.get_variable_scope().reuse_variables()
 ft=tf.get_variable('W_S1_conv1')
</code></pre>

<p>I can get around this with</p>

<pre><code>return S3_conv1,regularizer, S1_conv1
</code></pre>

<p>but I don't want to do that.</p>

<p>I think my problem is that S1_conv1 is not really a variable, it's just a tensor. Is there a way to do what I want?</p>
",
TensorFlow: cast a float64 tensor to float32,"<p>I am trying to use: <code>train = optimizer.minimize(loss)</code> but the standard optimizers do not work with <code>tf.float64</code>. Therefore I want to truncate my <code>loss</code> from <code>tf.float64</code> to only <code>tf.float32</code>.  </p>

<pre><code>Traceback (most recent call last):
  File ""q4.py"", line 85, in &lt;module&gt;
    train = optimizer.minimize(loss)
  File ""/Library/Python/2.7/site-packages/tensorflow/python/training/optimizer.py"", line 190, in minimize
    colocate_gradients_with_ops=colocate_gradients_with_ops)
  File ""/Library/Python/2.7/site-packages/tensorflow/python/training/optimizer.py"", line 229, in compute_gradients
    self._assert_valid_dtypes([loss])
  File ""/Library/Python/2.7/site-packages/tensorflow/python/training/optimizer.py"", line 354, in _assert_valid_dtypes
    dtype, t.name, [v for v in valid_dtypes]))
ValueError: Invalid type tf.float64 for Add_1:0, expected: [tf.float32].
</code></pre>
",
Compute gradient norm of each part of composite loss function,"<p>Assume I have the following loss function:</p>

<pre><code>loss_a = tf.reduce_mean(my_loss_fn(model_output, targets))
loss_b = tf.reduce_mean(my_other_loss_fn(model_output, targets))
loss_final = loss_a + tf.multiply(alpha, loss_b)
</code></pre>

<p>To visualize the norm of the gradients w.r.t to <code>loss_final</code> one could do this:</p>

<pre><code>optimizer = tf.train.AdamOptimizer(learning_rate=0.001)
grads_and_vars = optimizer.compute_gradients(loss_final)
grads, _ = list(zip(*grads_and_vars))
norms = tf.global_norm(grads)
gradnorm_s = tf.summary.scalar('gradient norm', norms)
train_op = optimizer.apply_gradients(grads_and_vars, name='train_op')
</code></pre>

<p>However, I would like to plot the norm of the gradients w.r.t to <code>loss_a</code> and <code>loss_b</code> separately. How can I do this in the <em>most efficient</em> way? Do I have to call <code>compute_gradients(..)</code> on both <code>loss_a</code> and <code>loss_b</code> separately and then add those two gradients together before passing them to <code>optimizer.apply_gradients(..)</code>? I know that this would mathematically be correct due to the summation rule, but it just seems a bit cumbersome and I also don't know how you would implement the summation of the gradients correctly. Also, <code>loss_final</code> is rather simple, because it's just a summation. What if <code>loss_final</code> was more complicated, e.g. a division?</p>

<p>I'm using Tensorflow 0.12.</p>
",
How to add if condition in a TensorFlow graph?,"<p>Let's say I have following code:</p>

<pre><code>x = tf.placeholder(""float32"", shape=[None, ins_size**2*3], name = ""x_input"")
condition = tf.placeholder(""int32"", shape=[1, 1], name = ""condition"")
W = tf.Variable(tf.zeros([ins_size**2*3,label_option]), name = ""weights"")
b = tf.Variable(tf.zeros([label_option]), name = ""bias"")

if condition &gt; 0:
    y = tf.nn.softmax(tf.matmul(x, W) + b)
else:
    y = tf.nn.softmax(tf.matmul(x, W) - b)  
</code></pre>

<p>Would the <code>if</code> statement work in the calculation (I do not think so)? If not, how can I add an <code>if</code> statement into the TensorFlow calculation graph? </p>
",
TensorFlow: Unpooling,"<p>Is there TensorFlow native function that does unpooling for Deconvolutional Networks ? </p>

<p>I have written this in normal python, but it is getting complicated when want to translate it to TensorFlow as it's objects does not even support item assignment at the moment, and I think this is a great inconvenience with TF.</p>
",
Tensorflow dense gradient explanation?,"<p>I recently implemented a model and when I ran it I received this warning:</p>

<pre><code>UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. 
This may consume a large amount of memory.
""Converting sparse IndexedSlices to a dense Tensor of unknown shape. ""
</code></pre>

<p>With some similar parameter settings (embedding dimensionalities) suddenly the model is ridiculously slow.</p>

<ol>
<li>What does this warning imply? It appears that something I've done has caused all of the gradients to be dense and so backprop is doing dense matrix computations</li>
<li>If it's that there is an issue with the model that's causing this, how can I identify it and fix it?</li>
</ol>
",
Disable Tensorflow debugging information,"<p>By debugging information I mean what TensorFlow shows in my terminal about loaded libraries and found devices etc. not the python errors.</p>

<pre><code>I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:900] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
I tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: 
name: Graphics Device
major: 5 minor: 2 memoryClockRate (GHz) 1.0885
pciBusID 0000:04:00.0
Total memory: 12.00GiB
Free memory: 11.83GiB
I tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 
I tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:717] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: Graphics Device, pci bus id: 0000:04:00.0)
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 1.0KiB
...
</code></pre>
",
What's the difference of name scope and a variable scope in tensorflow?,"<p>What's the differences between these functions?</p>

<blockquote>
  <p><code>tf.variable_op_scope(values, name, default_name, initializer=None)</code></p>
  
  <p>Returns a context manager for defining an op that creates variables.
  This context manager validates that the given values are from the same graph, ensures that that graph is the default graph, and pushes a name scope and a variable scope.</p>
</blockquote>

<hr>

<blockquote>
  <p><code>tf.op_scope(values, name, default_name=None)</code></p>
  
  <p>Returns a context manager for use when defining a Python op.
  This context manager validates that the given values are from the same graph, ensures that that graph is the default graph, and pushes a name scope.            </p>
</blockquote>

<hr>

<blockquote>
  <p><code>tf.name_scope(name)</code></p>
  
  <p>Wrapper for <code>Graph.name_scope()</code> using the default graph.
  See <code>Graph.name_scope()</code> for more details.</p>
</blockquote>

<hr>

<blockquote>
  <p><code>tf.variable_scope(name_or_scope, reuse=None, initializer=None)</code></p>
  
  <p>Returns a context for variable scope.
  Variable scope allows to create new variables and to share already created ones while providing checks to not create or share by accident. For details, see the Variable Scope How To, here we present only a few basic examples.  </p>
</blockquote>
",
Adding multiple layers to TensorFlow causes loss function to become Nan,"<p>I'm writing a neural-network classifier in TensorFlow/Python for the <a href=""http://yaroslavvb.blogspot.kr/2011/09/notmnist-dataset.html"">notMNIST</a> dataset.  I've implemented l2 regularization and dropout on the hidden layers.  It works fine as long as there is only one hidden layer, but when I added more layers (to improve accuracy), the loss function increases rapidly at each step, becoming NaN by step 5.  I tried temporarily disabling Dropout and L2 regularization, but I get the same behavior as long as there are 2+ layers.  I even rewrote my code from scratch (doing some refactoring to make it more flexible), but with the same results.  The number and size of layers is controlled by <code>hidden_layer_spec</code>.  What am I missing?</p>

<pre><code>#works for np.array([1024]) with about 96.1% accuracy
hidden_layer_spec = np.array([1024, 300])
num_hidden_layers = hidden_layer_spec.shape[0]
batch_size = 256
beta = 0.0005

epochs = 100
stepsPerEpoch = float(train_dataset.shape[0]) / batch_size
num_steps = int(math.ceil(float(epochs) * stepsPerEpoch))

l2Graph = tf.Graph()
with l2Graph.as_default():
  #with tf.device('/cpu:0'):
      # Input data. For the training data, we use a placeholder that will be fed
      # at run time with a training minibatch.
      tf_train_dataset = tf.placeholder(tf.float32,
                                        shape=(batch_size, image_size * image_size))
      tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))
      tf_valid_dataset = tf.constant(valid_dataset)
      tf_test_dataset = tf.constant(test_dataset)

      weights = []
      biases = []
      for hi in range(0, num_hidden_layers + 1):
        width = image_size * image_size if hi == 0 else hidden_layer_spec[hi - 1]
        height = num_labels if hi == num_hidden_layers else hidden_layer_spec[hi]
        weights.append(tf.Variable(tf.truncated_normal([width, height]), name = ""w"" + `hi + 1`))
        biases.append(tf.Variable(tf.zeros([height]), name = ""b"" + `hi + 1`))
        print(`width` + 'x' + `height`)

      def logits(input, addDropoutLayer = False):
        previous_layer = input
        for hi in range(0, hidden_layer_spec.shape[0]):
          previous_layer = tf.nn.relu(tf.matmul(previous_layer, weights[hi]) + biases[hi])
          if addDropoutLayer:
            previous_layer = tf.nn.dropout(previous_layer, 0.5)
        return tf.matmul(previous_layer, weights[num_hidden_layers]) + biases[num_hidden_layers]

      # Training computation.
      train_logits = logits(tf_train_dataset, True)

      l2 = tf.nn.l2_loss(weights[0])
      for hi in range(1, len(weights)):
        l2 = l2 + tf.nn.l2_loss(weights[0])
      loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(train_logits, tf_train_labels)) + beta * l2

      # Optimizer.
      global_step = tf.Variable(0)  # count the number of steps taken.
      learning_rate = tf.train.exponential_decay(0.5, global_step, int(stepsPerEpoch) * 2, 0.96, staircase = True)
      optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)

      # Predictions for the training, validation, and test data.
      train_prediction = tf.nn.softmax(train_logits)
      valid_prediction = tf.nn.softmax(logits(tf_valid_dataset))
      test_prediction = tf.nn.softmax(logits(tf_test_dataset))
      saver = tf.train.Saver()

with tf.Session(graph=l2Graph) as session:
  tf.initialize_all_variables().run()
  print(""Initialized"")
  for step in range(num_steps):
    # Pick an offset within the training data, which has been randomized.
    # Note: we could use better randomization across epochs.
    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)
    # Generate a minibatch.
    batch_data = train_dataset[offset:(offset + batch_size), :]
    batch_labels = train_labels[offset:(offset + batch_size), :]
    # Prepare a dictionary telling the session where to feed the minibatch.
    # The key of the dictionary is the placeholder node of the graph to be fed,
    # and the value is the numpy array to feed to it.
    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}
    _, l, predictions = session.run(
      [optimizer, loss, train_prediction], feed_dict=feed_dict)
    if (step % 500 == 0):
      print(""Minibatch loss at step %d: %f"" % (step, l))
      print(""Learning rate: "" % learning_rate)
      print(""Minibatch accuracy: %.1f%%"" % accuracy(predictions, batch_labels))
      print(""Validation accuracy: %.1f%%"" % accuracy(
        valid_prediction.eval(), valid_labels))
  print(""Test accuracy: %.1f%%"" % accuracy(test_prediction.eval(), test_labels))
  save_path = saver.save(session, ""l2_degrade.ckpt"")
  print(""Model save to "" + `save_path`)
</code></pre>
",
Tensorflow: How to replace or modify gradient?,"<p>I would like to replace or modify the gradient of an op or portion of the graph in tensorflow.  It would be ideal if I can use the existing gradient in the calculation.</p>

<p>In some ways this is the opposite to what <code>tf.stop_gradient()</code> does: instead of adding a calculation which is ignored when calculating gradients, I want a calculation which is only used when calculating gradients.</p>

<p>A simple example would be something which simply scales gradients by multiplying them with a constant (but does not multiply the forward calculation by a constant).  Another example would be something which clips the gradients to a given range.</p>
",
Error running basic tensorflow example,"<p>I have just reinstalled latest tensorflow on ubuntu:</p>

<pre><code>$ sudo pip install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.7.1-cp27-none-linux_x86_64.whl
[sudo] password for ubuntu: 
The directory '/home/ubuntu/.cache/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.
The directory '/home/ubuntu/.cache/pip' or its parent directory is not owned by the current user and caching wheels has been disabled. check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.
Collecting tensorflow==0.7.1 from https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.7.1-cp27-none-linux_x86_64.whl
  Downloading https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.7.1-cp27-none-linux_x86_64.whl (13.8MB)
    100% |████████████████████████████████| 13.8MB 32kB/s 
Requirement already up-to-date: six&gt;=1.10.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==0.7.1)
Requirement already up-to-date: protobuf==3.0.0b2 in /usr/local/lib/python2.7/dist-packages (from tensorflow==0.7.1)
Requirement already up-to-date: wheel in /usr/local/lib/python2.7/dist-packages (from tensorflow==0.7.1)
Requirement already up-to-date: numpy&gt;=1.8.2 in /usr/local/lib/python2.7/dist-packages (from tensorflow==0.7.1)
Requirement already up-to-date: setuptools in /usr/local/lib/python2.7/dist-packages (from protobuf==3.0.0b2-&gt;tensorflow==0.7.1)
Installing collected packages: tensorflow
  Found existing installation: tensorflow 0.7.1
    Uninstalling tensorflow-0.7.1:
      Successfully uninstalled tensorflow-0.7.1
Successfully installed tensorflow-0.7.1
</code></pre>

<p>When following the directions to test it fails with <strong>cannot import name pywrap_tensorflow</strong>:</p>

<pre><code>$ ipython

/git/tensorflow/tensorflow/__init__.py in &lt;module&gt;()
     21 from __future__ import print_function
     22 
---&gt; 23 from tensorflow.python import *

/git/tensorflow/tensorflow/python/__init__.py in &lt;module&gt;()
     43 _default_dlopen_flags = sys.getdlopenflags()
     44 sys.setdlopenflags(_default_dlopen_flags | ctypes.RTLD_GLOBAL)
---&gt; 45 from tensorflow.python import pywrap_tensorflow
     46 sys.setdlopenflags(_default_dlopen_flags)
     47 

ImportError: cannot import name pywrap_tensorflow
</code></pre>

<p>Is there an additional change needed to my python or ubuntu/bash environment?</p>
",
Working with multiple graphs in TensorFlow,"<p>Can someone explain to me how <code>name_scope</code> works in TensorFlow?  </p>

<p>Suppose I have the following code:</p>

<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf

g1 = tf.Graph()
with g1.as_default() as g:
    with g.name_scope( ""g1"" ) as scope:
        matrix1 = tf.constant([[3., 3.]])
        matrix2 = tf.constant([[2.],[2.]])
        product = tf.matmul(matrix1, matrix2)

tf.reset_default_graph()

g2 = tf.Graph()
with g2.as_default() as g:
    with g.name_scope( ""g2"" ) as scope:
        matrix1 = tf.constant([[4., 4.]])
        matrix2 = tf.constant([[5.],[5.]])
        product = tf.matmul(matrix1, matrix2)

tf.reset_default_graph()

with tf.Session( graph = g1 ) as sess:
    result = sess.run( product )
    print( result )
</code></pre>

<p>When I run this code I get the following error message:</p>

<pre><code>Tensor Tensor(""g2/MatMul:0"", shape=(1, 1), dtype=float32) is not an element of this graph.
</code></pre>

<p>I agree ""g2/MatMul"" is not an element of graph <code>g1</code>, but why is it selecting ""g2/MatMul"" when the session graph is set to <code>g1</code>? Why doesn't it select ""g1/MatMul""?</p>

<hr>

<h3>Edit</h3>

<p>The following code seems to work:</p>

<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf

g1 = tf.Graph()
with g1.as_default() as g:
    with g.name_scope( ""g1"" ) as g1_scope:
        matrix1 = tf.constant([[3., 3.]])
        matrix2 = tf.constant([[2.],[2.]])
        product = tf.matmul( matrix1, matrix2, name = ""product"")

tf.reset_default_graph()

g2 = tf.Graph()
with g2.as_default() as g:
    with g.name_scope( ""g2"" ) as g2_scope:
        matrix1 = tf.constant([[4., 4.]])
        matrix2 = tf.constant([[5.],[5.]])
        product = tf.matmul( matrix1, matrix2, name = ""product"" )

tf.reset_default_graph()

use_g1 = False

if ( use_g1 ):
    g = g1
    scope = g1_scope
else:
    g = g2
    scope = g2_scope

with tf.Session( graph = g ) as sess:
    tf.initialize_all_variables()
    result = sess.run( sess.graph.get_tensor_by_name( scope + ""product:0"" ) )
    print( result )
</code></pre>

<p>By flipping the switch <code>use_g1</code>, graph <code>g1</code> or <code>g2</code> will run in the session. Is this the way name scoping was meant to work?</p>
",
Understanding LSTM model using tensorflow for sentiment analysis,"<p>I am trying to learn LSTM model for sentiment analysis using Tensorflow, I have gone through the <a href=""http://colah.github.io/posts/2015-08-Understanding-LSTMs/"" rel=""noreferrer"">LSTM model</a>.</p>

<p>Following code <strong>(create_sentiment_featuresets.py)</strong> generates the lexicon from <em>5000 positive sentences and 5000 negative sentences</em>.</p>

<pre><code>import nltk
from nltk.tokenize import word_tokenize
import numpy as np
import random
from collections import Counter
from nltk.stem import WordNetLemmatizer

lemmatizer = WordNetLemmatizer()

def create_lexicon(pos, neg):
    lexicon = []
    with open(pos, 'r') as f:
        contents = f.readlines()
        for l in contents[:len(contents)]:
            l= l.decode('utf-8')
            all_words = word_tokenize(l)
            lexicon += list(all_words)
    f.close()

    with open(neg, 'r') as f:
        contents = f.readlines()    
        for l in contents[:len(contents)]:
            l= l.decode('utf-8')
            all_words = word_tokenize(l)
            lexicon += list(all_words)
    f.close()

    lexicon = [lemmatizer.lemmatize(i) for i in lexicon]
    w_counts = Counter(lexicon)
    l2 = []
    for w in w_counts:
        if 1000 &gt; w_counts[w] &gt; 50:
            l2.append(w)
    print(""Lexicon length create_lexicon: "",len(lexicon))
    return l2

def sample_handling(sample, lexicon, classification):
    featureset = []
    print(""Lexicon length Sample handling: "",len(lexicon))
    with open(sample, 'r') as f:
        contents = f.readlines()
        for l in contents[:len(contents)]:
            l= l.decode('utf-8')
            current_words = word_tokenize(l.lower())
            current_words= [lemmatizer.lemmatize(i) for i in current_words]
            features = np.zeros(len(lexicon))
            for word in current_words:
                if word.lower() in lexicon:
                    index_value = lexicon.index(word.lower())
                    features[index_value] +=1
            features = list(features)
            featureset.append([features, classification])
    f.close()
    print(""Feature SET------"")
    print(len(featureset))
    return featureset

def create_feature_sets_and_labels(pos, neg, test_size = 0.1):
    global m_lexicon
    m_lexicon = create_lexicon(pos, neg)
    features = []
    features += sample_handling(pos, m_lexicon, [1,0])
    features += sample_handling(neg, m_lexicon, [0,1])
    random.shuffle(features)
    features = np.array(features)

    testing_size = int(test_size * len(features))

    train_x = list(features[:,0][:-testing_size])
    train_y = list(features[:,1][:-testing_size])
    test_x = list(features[:,0][-testing_size:])
    test_y = list(features[:,1][-testing_size:])
    return train_x, train_y, test_x, test_y

def get_lexicon():
    global m_lexicon
    return m_lexicon
</code></pre>

<p>The following code <strong>(sentiment_analysis.py)</strong> is for <em>sentiment analysis using simple neural network model</em> and is working fine</p>

<pre><code>from create_sentiment_featuresets import create_feature_sets_and_labels
from create_sentiment_featuresets import get_lexicon
import tensorflow as tf
import numpy as np
# extras for testing
from nltk.tokenize import word_tokenize 
from nltk.stem import WordNetLemmatizer
lemmatizer = WordNetLemmatizer()
#- end extras

train_x, train_y, test_x, test_y = create_feature_sets_and_labels('pos.txt', 'neg.txt')


# pt A-------------

n_nodes_hl1 = 1500
n_nodes_hl2 = 1500
n_nodes_hl3 = 1500

n_classes = 2
batch_size = 100
hm_epochs = 10

x = tf.placeholder(tf.float32)
y = tf.placeholder(tf.float32)

hidden_1_layer = {'f_fum': n_nodes_hl1,
                'weight': tf.Variable(tf.random_normal([len(train_x[0]), n_nodes_hl1])),
                'bias': tf.Variable(tf.random_normal([n_nodes_hl1]))}
hidden_2_layer = {'f_fum': n_nodes_hl2,
                'weight': tf.Variable(tf.random_normal([n_nodes_hl1, n_nodes_hl2])),
                'bias': tf.Variable(tf.random_normal([n_nodes_hl2]))}
hidden_3_layer = {'f_fum': n_nodes_hl3,
                'weight': tf.Variable(tf.random_normal([n_nodes_hl2, n_nodes_hl3])),
                'bias': tf.Variable(tf.random_normal([n_nodes_hl3]))}
output_layer = {'f_fum': None,
                'weight': tf.Variable(tf.random_normal([n_nodes_hl3, n_classes])),
                'bias': tf.Variable(tf.random_normal([n_classes]))}


def nueral_network_model(data):
    l1 = tf.add(tf.matmul(data, hidden_1_layer['weight']), hidden_1_layer['bias'])
    l1 = tf.nn.relu(l1)
    l2 = tf.add(tf.matmul(l1, hidden_2_layer['weight']), hidden_2_layer['bias'])
    l2 = tf.nn.relu(l2)
    l3 = tf.add(tf.matmul(l2, hidden_3_layer['weight']), hidden_3_layer['bias'])
    l3 = tf.nn.relu(l3)
    output = tf.matmul(l3, output_layer['weight']) + output_layer['bias']
    return output

# pt B--------------

def train_neural_network(x):
    prediction = nueral_network_model(x)
    cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(logits= prediction, labels= y))
    optimizer = tf.train.AdamOptimizer(learning_rate= 0.001).minimize(cost)

    with tf.Session() as sess:
        sess.run(tf.global_variables_initializer())
        for epoch in range(hm_epochs):
            epoch_loss = 0
            i = 0
            while i &lt; len(train_x):
                start = i
                end = i+ batch_size
                batch_x = np.array(train_x[start: end])
                batch_y = np.array(train_y[start: end])
                _, c = sess.run([optimizer, cost], feed_dict= {x: batch_x, y: batch_y})
                epoch_loss += c
                i+= batch_size
            print('Epoch', epoch+ 1, 'completed out of ', hm_epochs, 'loss:', epoch_loss)

        correct= tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))
        accuracy = tf.reduce_mean(tf.cast(correct, 'float'))
        print('Accuracy:', accuracy.eval({x:test_x, y:test_y}))

        # testing --------------
        m_lexicon= get_lexicon()
        print('Lexicon length: ',len(m_lexicon))        
        input_data= ""David likes to go out with Kary""       
        current_words= word_tokenize(input_data.lower())
        current_words = [lemmatizer.lemmatize(i) for i in current_words]
        features = np.zeros(len(m_lexicon))
        for word in current_words:
            if word.lower() in m_lexicon:
                index_value = m_lexicon.index(word.lower())
                features[index_value] +=1

        features = np.array(list(features)).reshape(1,-1)
        print('features length: ',len(features))
        result = sess.run(tf.argmax(prediction.eval(feed_dict={x:features}), 1))
        print(prediction.eval(feed_dict={x:features}))
        if result[0] == 0:
            print('Positive: ', input_data)
        elif result[0] == 1:
            print('Negative: ', input_data)

train_neural_network(x)
</code></pre>

<p><strong>I am trying to modify the above (sentiment_analysis.py) for LSTM model</strong>
after reading the <a href=""https://pythonprogramming.net/rnn-tensorflow-python-machine-learning-tutorial/?completed=/recurrent-neural-network-rnn-lstm-machine-learning-tutorial/"" rel=""noreferrer"">RNN w/ LSTM cell example in TensorFlow and Python</a> which is for LSTM on <em>mnist image dataset</em>:</p>

<p>Some how through many hit and run trails, I was able to get the below running  code <strong>(sentiment_demo_lstm.py)</strong> :</p>

<pre><code>import tensorflow as tf
from tensorflow.contrib import rnn
from create_sentiment_featuresets import create_feature_sets_and_labels
from create_sentiment_featuresets import get_lexicon

import numpy as np

# extras for testing
from nltk.tokenize import word_tokenize 
from nltk.stem import WordNetLemmatizer
lemmatizer = WordNetLemmatizer()
#- end extras

train_x, train_y, test_x, test_y = create_feature_sets_and_labels('pos.txt', 'neg.txt')

n_steps= 100
input_vec_size= len(train_x[0])
hm_epochs = 8
n_classes = 2
batch_size = 128
n_hidden = 128

x = tf.placeholder('float', [None, input_vec_size, 1])
y = tf.placeholder('float')

def recurrent_neural_network(x):
    layer = {'weights': tf.Variable(tf.random_normal([n_hidden, n_classes])),   # hidden_layer, n_classes
            'biases': tf.Variable(tf.random_normal([n_classes]))}

    h_layer = {'weights': tf.Variable(tf.random_normal([1, n_hidden])), # hidden_layer, n_classes
            'biases': tf.Variable(tf.random_normal([n_hidden], mean = 1.0))}

    x = tf.transpose(x, [1,0,2])
    x = tf.reshape(x, [-1, 1])
    x= tf.nn.relu(tf.matmul(x, h_layer['weights']) + h_layer['biases'])

    x = tf.split(x, input_vec_size, 0)

    lstm_cell = rnn.BasicLSTMCell(n_hidden, state_is_tuple=True)
    outputs, states = rnn.static_rnn(lstm_cell, x, dtype= tf.float32)
    output = tf.matmul(outputs[-1], layer['weights']) + layer['biases']

    return output

def train_neural_network(x):
    prediction = recurrent_neural_network(x)
    cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(logits= prediction, labels= y))
    optimizer = tf.train.AdamOptimizer(learning_rate= 0.001).minimize(cost)

    with tf.Session() as sess:
        sess.run(tf.global_variables_initializer())

        for epoch in range(hm_epochs):
            epoch_loss = 0
            i = 0
            while (i+ batch_size) &lt; len(train_x):
                start = i
                end = i+ batch_size
                batch_x = np.array(train_x[start: end])
                batch_y = np.array(train_y[start: end])
                batch_x = batch_x.reshape(batch_size ,input_vec_size, 1)
                _, c = sess.run([optimizer, cost], feed_dict= {x: batch_x, y: batch_y})
                epoch_loss += c
                i+= batch_size
            print('--------Epoch', epoch+ 1, 'completed out of ', hm_epochs, 'loss:', epoch_loss)

        correct= tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))
        accuracy = tf.reduce_mean(tf.cast(correct, 'float'))

        print('Accuracy:', accuracy.eval({x:np.array(test_x).reshape(-1, input_vec_size, 1), y:test_y}))

        # testing --------------
        m_lexicon= get_lexicon()
        print('Lexicon length: ',len(m_lexicon))
        input_data= ""Mary does not like pizza""  #""he seems to to be healthy today""  #""David likes to go out with Kary""

        current_words= word_tokenize(input_data.lower())
        current_words = [lemmatizer.lemmatize(i) for i in current_words]
        features = np.zeros(len(m_lexicon))
        for word in current_words:
            if word.lower() in m_lexicon:
                index_value = m_lexicon.index(word.lower())
                features[index_value] +=1
        features = np.array(list(features)).reshape(-1, input_vec_size, 1)
        print('features length: ',len(features))

        result = sess.run(tf.argmax(prediction.eval(feed_dict={x:features}), 1))
        print('RESULT: ', result)
        print(prediction.eval(feed_dict={x:features}))
        if result[0] == 0:
            print('Positive: ', input_data)
        elif result[0] == 1:
            print('Negative: ', input_data)

train_neural_network(x)
</code></pre>

<p><strong>Output of</strong> </p>

<pre><code>print(train_x[0])
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

print(train_y[0])
[0, 1]
</code></pre>

<p><code>len(train_x)= 9596</code>, <code>len(train_x[0]) = 423</code>  meaning <code>train_x</code> is a list of 9596x423 ?</p>

<p>Tough I have a running code now, I still have lots of doubts.</p>

<ol>
<li><p>In <strong>sentiment_demo_lstm</strong>, I am not able to understand the following part</p>

<pre><code>x = tf.transpose(x, [1,0,2])
x = tf.reshape(x, [-1, 1])
x = tf.split(x, input_vec_size, 0)
</code></pre>

<p>I have print the following shapes:</p>

<pre><code>x = tf.placeholder('float', [None, input_vec_size, 1]) ==&gt; TensorShape([Dimension(None), Dimension(423), Dimension(1)]))
x = tf.transpose(x, [1,0,2]) ==&gt; TensorShape([Dimension(423), Dimension(None), Dimension(1)]))
x = tf.reshape(x, [-1, 1]) ==&gt; TensorShape([Dimension(None), Dimension(1)]))
x = tf.split(x, input_vec_size, 0) ==&gt; ?
</code></pre></li>
<li><p>Here I took the number of hidden layers as 128, does it need to be same as the number of inputs i.e. <code>len(train_x)= 9596</code></p></li>
<li><p>The value 1 in</p>

<pre><code>x = tf.placeholder('float', [None, input_vec_size, 1])
</code></pre>

<p>and</p>

<pre><code>x = tf.reshape(x, [-1, 1])
</code></pre>

<p>is because <code>train_x[0]</code> is 428x<strong>1</strong>  ?</p></li>
<li><p>The following is in order to match the placeholder</p>

<pre><code>batch_x = np.array(train_x[start: end]) ==&gt; (128, 423)
batch_x = batch_x.reshape(batch_size ,input_vec_size, 1) ==&gt; (128, 423, 1)
</code></pre>

<p><strong><code>x = tf.placeholder('float', [None, input_vec_size, 1])</code></strong>  dimensions, right?</p></li>
<li><p>If I modified the code:</p>

<pre><code>while (i+ batch_size) &lt; len(train_x):
</code></pre>

<p>as</p>

<pre><code>while i &lt; len(train_x):
</code></pre>

<p>I get the following error:</p>

<pre><code>Traceback (most recent call last):
  File ""sentiment_demo_lstm.py"", line 131, in &lt;module&gt;
    train_neural_network(x)
  File ""sentiment_demo_lstm.py"", line 86, in train_neural_network
    batch_x = batch_x.reshape(batch_size ,input_vec_size, 1)
ValueError: cannot reshape array of size 52452 into shape (128,423,1)
</code></pre></li>
</ol>

<p>=> I can't include the last 124 records/feature-sets while training?</p>
",
How to train a RNN with LSTM cells for time series prediction,"<p>I'm currently trying to build a simple model for predicting time series. The goal would be to train the model with a sequence so that the model is able to predict future values.</p>

<p>I'm using tensorflow and lstm cells to do so. The model is trained with truncated backpropagation through time. My question is how to structure the data for training.</p>

<p>For example let's assume we want to learn the given sequence:</p>

<pre><code>[1,2,3,4,5,6,7,8,9,10,11,...]
</code></pre>

<p>And we unroll the network for <code>num_steps=4</code>.</p>

<p><strong>Option 1</strong></p>

<pre><code>input data               label     
1,2,3,4                  2,3,4,5
5,6,7,8                  6,7,8,9
9,10,11,12               10,11,12,13
...
</code></pre>

<p><strong>Option 2</strong></p>

<pre><code>input data               label     
1,2,3,4                  2,3,4,5
2,3,4,5                  3,4,5,6
3,4,5,6                  4,5,6,7
...
</code></pre>

<p><strong>Option 3</strong></p>

<pre><code>input data               label     
1,2,3,4                  5
2,3,4,5                  6
3,4,5,6                  7
...
</code></pre>

<p><strong>Option 4</strong></p>

<pre><code>input data               label     
1,2,3,4                  5
5,6,7,8                  9
9,10,11,12               13
...
</code></pre>

<p>Any help would be appreciated.</p>
",
Tensor flow toggle between CPU/GPU,"<p>Having installed tensorflow GPU (running on a measly NVIDIA GeForce 950), I would like to compare performance with the CPU. </p>

<p>I am running the tensorFlow MNIST tutorial code, and have noticed a dramatic increase in speed--estimated anyways (I ran the CPU version 2 days ago on a laptop i7 with a batch size of 100, and this on a desktop GPU, batch size of 10)--between the CPU and the GPU when I switched...but I only noticed the speed increase when I lowered the batch size on the GPU to 10 from 100...</p>

<p>Now I lack an objective measure for what I am gaining.  </p>

<p>Is there a way to toggle between the CPU and GPU tensor flows?</p>
",
How does TensorFlow name tensors?,"<p>I wonder if this is the correct understanding: </p>

<p>All tensors are derived from some operation, and operations are either given a name in the constructor, or given the default name for a particular kind of operation. If the name is not unique, TensorFlow automatically handles this by appending <code>""_1""</code>, <code>""_2""</code>, etc. An operation with n tensor outputs name these tensors <code>""op_name:0""</code>, <code>""op_name:1""</code>, ..., <code>""op_name:n-1""</code>.</p>

<p>One problem seems to arise: if <code>x</code> is a <code>tf.Variable</code>, then <code>x.name</code> gives <code>""variable_name:0""</code>. This is confusing: to what does <code>""variable_name""</code> refer?</p>
",
Loading SavedModel is a lot slower than loading a tf.train.Saver checkpoint,"<p>I changed from <code>tf.train.Saver</code> to the SavedModel format which surprisingly means loading my model from disk is a lot slower (instead of a couple of seconds it takes minutes). Why is this and what can I do to load the model faster?</p>

<p>I used to do this:</p>

<pre><code># Save model
saver = tf.train.Saver()
save_path = saver.save(session, model_path)

# Load model
saver = tf.train.import_meta_graph(model_path + '.meta')
saver.restore(session, model_path)
</code></pre>

<p>But now I do this:</p>

<pre><code># Save model
builder = tf.saved_model.builder.SavedModelBuilder(model_path)
builder.add_meta_graph_and_variables(session, [tf.saved_model.tag_constants.TRAINING])
builder.save()

# Load model
tf.saved_model.loader.load(session, [tf.saved_model.tag_constants.TRAINING], model_path)
</code></pre>
",
TensorFlow: libcudart.so.7.5: cannot open shared object file: No such file or directory,"<p>I am running TensorFlow on Ubuntu 15.10. When I enter <code>pip show tensorflow</code>, I see that TF has been installed properly.</p>

<p>However, when I write <code>import tensorflow as tf</code>, I get the following error message:</p>

<pre><code>Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""/home/me/anaconda2/lib/python2.7/site-packages/tensorflow/__init__.py"", line 23, in &lt;module&gt;
    from tensorflow.python import *
  File ""/home/me/anaconda2/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in &lt;module&gt;
    from tensorflow import contrib
  File ""/home/me/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/__init__.py"", line 23, in &lt;module&gt;
    from tensorflow.contrib import layers
  File ""/home/me/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/layers/__init__.py"", line 68, in &lt;module&gt;
    from tensorflow.contrib.layers.python.layers import *
  File ""/home/me/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/layers/python/layers/__init__.py"", line 22, in &lt;module&gt;
    from tensorflow.contrib.layers.python.layers.initializers import *
  File ""/home/me/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/layers/python/layers/initializers.py"", line 24, in &lt;module&gt;
    from tensorflow.python.ops import random_ops
  File ""/home/me/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/random_ops.py"", line 23, in &lt;module&gt;
    from tensorflow.python.framework import ops
  File ""/home/me/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 39, in &lt;module&gt;
    from tensorflow.python.framework import versions
  File ""/home/me/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/versions.py"", line 22, in &lt;module&gt;
    from tensorflow.python import pywrap_tensorflow
  File ""/home/me/anaconda2/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 28, in &lt;module&gt;
    _pywrap_tensorflow = swig_import_helper()
  File ""/home/me/anaconda2/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)
ImportError: libcudart.so.7.5: cannot open shared object file: No such file or directory
</code></pre>

<p>For what it's worth, I have followed the instructions <a href=""https://www.tensorflow.org/versions/r0.7/get_started/os_setup.html#optional-linux-enable-gpu-support"" rel=""noreferrer"">here</a> and set my <code>LD_LIBRARY_PATH</code> and <code>CUDA_HOME</code> environment variables.</p>

<p>Any advice?</p>

<p><strong>EDIT:</strong></p>

<p>I have installed CUDA 7.5 and added these to my <code>.profile</code> file:</p>

<pre><code>export LD_LIBRARY_PATH=""/usr/local/cuda-7.5/lib64""
export CUDA_HOME=/usr/local/cuda-7.5
</code></pre>

<p>However, I continue to see the same error message.</p>

<p><strong>EDIT:</strong></p>

<p>I see the following output when I run <code>ldd /usr/local/cuda-7.5/lib64/libcudart.so.7.5</code>:</p>

<pre><code>linux-vdso.so.1 =&gt;  (0x00007ffdac7ea000)
libc.so.6 =&gt; /lib/x86_64-linux-gnu/libc.so.6 (0x00007fc27a281000)
libdl.so.2 =&gt; /lib/x86_64-linux-gnu/libdl.so.2 (0x00007fc27a07d000)
libpthread.so.0 =&gt; /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007fc279e5e000)
librt.so.1 =&gt; /lib/x86_64-linux-gnu/librt.so.1 (0x00007fc279c56000)
/lib64/ld-linux-x86-64.so.2 (0x00005604f5406000)
</code></pre>

<p><strong>EDIT:</strong></p>

<p>If it is relevant, I use GeForce GT640.</p>

<p><strong>EDIT:</strong></p>

<p>I followed <a href=""https://stackoverflow.com/a/36161286/3559686"">@tommus' advice</a> and called <code>source ~/.profile</code> before running TensorFlow, and now it works like a charm.</p>

<p>Thanks to everyone who tried to help me in the comments -- this is my first experience with any kind of Linux distribution, and I am really <strong>very appreciative</strong> of all the patient assistance I have received so far :-) You guys are absolutely awesome!</p>
",
Gradient Descent vs Adagrad vs Momentum in TensorFlow,"<p>I'm studyng <em>TensorFlow</em> and how to use it, even if I'm not an expert of neural network and deep learnig (just the bases).</p>

<p>Following tutorials I don't understand the real and practice difference between the three optimizers for a loss. I look at the <a href=""https://www.tensorflow.org/versions/r0.7/api_docs/python/train.html#optimizers"" rel=""noreferrer"">API</a> and I understand the principles, but my questions are:</p>

<p><strong>1. When is preferable to use one instead of the others ?</strong></p>

<p><strong>2. Are there important differences to know ?</strong></p>
",
Convert Keras model to TensorFlow protobuf,"<p>We're currently training various neural networks using Keras, which is ideal because it has a nice interface and is relatively easy to use, but we'd like to be able to apply them in our production environment. </p>

<p>Unfortunately the production environment is C++, so our plan is to:</p>

<ul>
<li>Use the TensorFlow backend to save the model to a protobuf</li>
<li>Link our production code to TensorFlow, and then load in the protobuf</li>
</ul>

<p>Unfortunately I don't know how to access the TensorFlow saving utilities from Keras, which normally saves to HDF5 and JSON. How do I save to protobuf?</p>
",
Debugging batching in Tensorflow Serving (no effect observed),"<p>I have a small web server that gets input in terms of sentences and needs to return a model prediction using Tensorflow Serving. It's working all fine and well using our single GPU, but now I'd like to enable batching such that Tensorflow Serving waits a bit to group incoming sentences before processing them together in one batch on the GPU.</p>

<p>I'm using the <a href=""https://github.com/tensorflow/serving/blob/6c25856d900064eef5b686d6001df59800aaa247/tensorflow_serving/model_servers/main.cc"" rel=""noreferrer"">predesigned server framework</a> with the <a href=""https://github.com/tensorflow/serving/blob/6c25856d900064eef5b686d6001df59800aaa247/tensorflow_serving/batching/basic_batch_scheduler.h"" rel=""noreferrer"">predesigned batching framework</a> using the initial release of Tensorflow Serving. I'm enabling batching using the <code>--batching</code> flag and have set <code>batch_timeout_micros = 10000</code> and <code>max_batch_size = 1000</code>. The logging does confirm that batching is enabled and that the GPU is being used.</p>

<p>However, when sending requests to the serving server the batching has minimal effect. Sending 50 requests at the same time almost linearly scales in terms of time usage with sending 5 requests. Interestingly, the <code>predict()</code> function of the server is run once for each request (see <a href=""https://github.com/tensorflow/serving/blob/6c25856d900064eef5b686d6001df59800aaa247/tensorflow_serving/model_servers/main.cc#L179"" rel=""noreferrer"">here</a>), which suggests to me that the batching is not being handled properly.</p>

<p>Am I missing something? How do I check what's wrong with the batching?</p>

<hr>

<p>Note that this is different from <a href=""https://stackoverflow.com/questions/42519010/how-to-do-batching-in-tensorflow-serving"">How to do batching in Tensorflow Serving?</a> as that question only examines how to send multiple requests from a single client, but not how to enable Tensorflow Serving's behind-the-scenes batching for multiple separate requests.</p>
",
How do display different runs in TensorBoard?,"<p>TensorBoard seems to have a feature to display multiple different runs and toggle them. </p>

<p><a href=""https://i.stack.imgur.com/5E3eQ.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/5E3eQ.png"" alt=""enter image description here""></a></p>

<p>How can I make multiple runs show up here and how can assign a name to them to differentiate them?</p>
",
Change default GPU in TensorFlow,"<p>Based on the documentation, the default GPU is the one with the lowest id:</p>

<blockquote>
  <p>If you have more than one GPU in your system, the GPU with the lowest
  ID will be selected by default.</p>
</blockquote>

<p>Is it possible to change this default from command line or one line of code?</p>
",
Flatten batch in tensorflow,"<p>I have an input to tensorflow of shape <code>[None, 9, 2]</code> (where the <code>None</code> is batch).</p>

<p>To perform further actions (e.g. matmul) on it I need to transform it to <code>[None, 18]</code> shape. How to do it? </p>
",
Make predictions using a tensorflow graph from a keras model,"<p>I have a model trained using Keras with Tensorflow as my backend, but now I need to turn my model into a tensorflow graph for a certain application. I attempted to do this and make predictions to insure that it is working correctly, but when comparing to the results gathered from model.predict() I get very different values. For instance:</p>

<pre><code>from keras.models import load_model
import tensorflow as tf

model = load_model('model_file.h5')

x_placeholder = tf.placeholder(tf.float32, shape=(None,7214,1))
y = model(x_placeholder)

x = np.ones((1,7214,1))


with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    print(""Predictions from:\ntf graph:      ""+str(sess.run(y, feed_dict={x_placeholder:x})))
    print(""keras predict: ""+str(model.predict(x)))
</code></pre>

<p>returns:</p>

<pre><code>Predictions from:
tf graph:      [[-0.1015993   0.07432419  0.0592984 ]]
keras predict: [[ 0.39339241  0.57949686 -3.67846966]]
</code></pre>

<p>The values from keras predict are correct, but the tf graph results are not.</p>

<p>If it helps to know the final intended application, I am creating a jacobian matrix with the tf.gradients() function, but currently it does not return the correct results when comparing to theano's jacobian function, which gives the correct jacobian. Here is my tensorflow jacobian code:</p>

<pre><code>x = tf.placeholder(tf.float32, shape=(None,7214,1))
y = tf.reshape(model(x)[0],[-1])
y_list = tf.unstack(y)

jacobian_list = [tf.gradients(y_, x)[0] for y_ in y_list]
jacobian = tf.stack(jacobian_list)
</code></pre>

<p>EDIT: Model code</p>

<pre><code>import numpy as np

from keras.models import Sequential
from keras.layers import Dense, InputLayer, Flatten
from keras.layers.convolutional import Conv1D
from keras.layers.convolutional import MaxPooling1D
from keras.optimizers import Adam
from keras.callbacks import EarlyStopping, ReduceLROnPlateau

# activation function used following every layer except for the output layers
activation = 'relu'

# model weight initializer
initializer = 'he_normal'

# shape of input data that is fed into the input layer
input_shape = (None,7214,1)

# number of filters used in the convolutional layers
num_filters = [4,16]

# length of the filters in the convolutional layers
filter_length = 8

# length of the maxpooling window 
pool_length = 4

# number of nodes in each of the hidden fully connected layers
num_hidden_nodes = [256,128]

# number of samples fed into model at once during training
batch_size = 64

# maximum number of interations for model training
max_epochs = 30

# initial learning rate for optimization algorithm
lr = 0.0007

# exponential decay rate for the 1st moment estimates for optimization algorithm
beta_1 = 0.9

# exponential decay rate for the 2nd moment estimates for optimization algorithm
beta_2 = 0.999

# a small constant for numerical stability for optimization algorithm
optimizer_epsilon = 1e-08

model = Sequential([

    InputLayer(batch_input_shape=input_shape),

    Conv1D(kernel_initializer=initializer, activation=activation, padding=""same"", filters=num_filters[0], kernel_size=filter_length),

    Conv1D(kernel_initializer=initializer, activation=activation, padding=""same"", filters=num_filters[1], kernel_size=filter_length),

    MaxPooling1D(pool_size=pool_length),

    Flatten(),

    Dense(units=num_hidden_nodes[0], kernel_initializer=initializer, activation=activation),

    Dense(units=num_hidden_nodes[1], kernel_initializer=initializer, activation=activation),

    Dense(units=3, activation=""linear"", input_dim=num_hidden_nodes[1]),
]) 

# compile model
loss_function = mean squared error
early_stopping_min_delta = 0.0001
early_stopping_patience = 4
reduce_lr_factor = 0.5
reuce_lr_epsilon = 0.0009
reduce_lr_patience = 2
reduce_lr_min = 0.00008

optimizer = Adam(lr=lr, beta_1=beta_1, beta_2=beta_2, epsilon=optimizer_epsilon, decay=0.0)

early_stopping = EarlyStopping(monitor='val_loss',     min_delta=early_stopping_min_delta, 
                                   patience=early_stopping_patience, verbose=2, mode='min')

reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.5, epsilon=reuce_lr_epsilon, 
                              patience=reduce_lr_patience,     min_lr=reduce_lr_min, mode='min', verbose=2)

model.compile(optimizer=optimizer, loss=loss_function)

model.fit(train_x, train_y, validation_data=(cv_x, cv_y),
      epochs=max_epochs, batch_size=batch_size, verbose=2,
      callbacks=[reduce_lr,early_stopping])

model.save('model_file.h5')
</code></pre>
",
How do I check if keras is using gpu version of tensorflow?,"<p>When I run a keras script, I get the following output:</p>

<pre><code>Using TensorFlow backend.
2017-06-14 17:40:44.621761: W 
tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow 
library wasn't compiled to use SSE4.1 instructions, but these are 
available on your machine and could speed up CPU computations.
2017-06-14 17:40:44.621783: W 
tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow 
library wasn't compiled to use SSE4.2 instructions, but these are 
available on your machine and could speed up CPU computations.
2017-06-14 17:40:44.621788: W 
tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow 
library wasn't compiled to use AVX instructions, but these are 
available on your machine and could speed up CPU computations.
2017-06-14 17:40:44.621791: W 
tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow 
library wasn't compiled to use AVX2 instructions, but these are 
available on your machine and could speed up CPU computations.
2017-06-14 17:40:44.621795: W 
tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow 
library wasn't compiled to use FMA instructions, but these are 
available 
on your machine and could speed up CPU computations.
2017-06-14 17:40:44.721911: I 
tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:901] successful 
NUMA node read from SysFS had negative value (-1), but there must be 
at least one NUMA node, so returning NUMA node zero
2017-06-14 17:40:44.722288: I 
tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 
with properties: 
name: GeForce GTX 850M
major: 5 minor: 0 memoryClockRate (GHz) 0.9015
pciBusID 0000:0a:00.0
Total memory: 3.95GiB
Free memory: 3.69GiB
2017-06-14 17:40:44.722302: I 
tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 
2017-06-14 17:40:44.722307: I 
tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y 
2017-06-14 17:40:44.722312: I 
tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating 
TensorFlow device (/gpu:0) -&gt; (device: 0, name: GeForce GTX 850M, 
pci bus id: 0000:0a:00.0)
</code></pre>

<p>What does this mean? Am I using GPU or CPU version of tensorflow?</p>

<p>Before installing keras, I was working with the GPU version of tensorflow. </p>

<p>Also <code>sudo pip3 list</code> shows <code>tensorflow-gpu(1.1.0)</code> and nothing like <code>tensorflow-cpu</code>.</p>

<p>Running the command mentioned on [this stackoverflow question], gives the following:</p>

<pre><code>The TensorFlow library wasn't compiled to use SSE4.1 instructions, 
but these are available on your machine and could speed up CPU 
computations.
2017-06-14 17:53:31.424793: W 
tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow 
library wasn't compiled to use SSE4.2 instructions, but these are 
available on your machine and could speed up CPU computations.
2017-06-14 17:53:31.424803: W 
tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow 
library wasn't compiled to use AVX instructions, but these are 
available on your machine and could speed up CPU computations.
2017-06-14 17:53:31.424812: W 
tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow 
library wasn't compiled to use AVX2 instructions, but these are 
available on your machine and could speed up CPU computations.
2017-06-14 17:53:31.424820: W 
tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow 
library wasn't compiled to use FMA instructions, but these are 
available on your machine and could speed up CPU computations.
2017-06-14 17:53:31.540959: I 
tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:901] successful 
NUMA node read from SysFS had negative value (-1), but there must be 
at least one NUMA node, so returning NUMA node zero
2017-06-14 17:53:31.541359: I 
tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 
with properties: 
name: GeForce GTX 850M
major: 5 minor: 0 memoryClockRate (GHz) 0.9015
pciBusID 0000:0a:00.0
Total memory: 3.95GiB
Free memory: 128.12MiB
2017-06-14 17:53:31.541407: I 
tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 
2017-06-14 17:53:31.541420: I 
tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y 
2017-06-14 17:53:31.541441: I 
tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating 
TensorFlow device (/gpu:0) -&gt; (device: 0, name: GeForce GTX 850M, 
pci bus id: 0000:0a:00.0)
2017-06-14 17:53:31.547902: E 
tensorflow/stream_executor/cuda/cuda_driver.cc:893] failed to 
allocate 128.12M (134348800 bytes) from device: 
CUDA_ERROR_OUT_OF_MEMORY
Device mapping:
/job:localhost/replica:0/task:0/gpu:0 -&gt; device: 0, name: GeForce 
GTX 850M, pci bus id: 0000:0a:00.0
2017-06-14 17:53:31.549482: I 
tensorflow/core/common_runtime/direct_session.cc:257] Device 
mapping:
/job:localhost/replica:0/task:0/gpu:0 -&gt; device: 0, name: GeForce 
GTX 850M, pci bus id: 0000:0a:00.0
</code></pre>
",
What's the difference between tf.placeholder and tf.Variable?,"<p>I'm a newbie to TensorFlow. I'm confused about the difference between <code>tf.placeholder</code> and <code>tf.Variable</code>. In my view, <code>tf.placeholder</code> is used for input data, and <code>tf.Variable</code> is used to store the state of data. This is all what I know. </p>

<p>Could someone explain to me more in detail about their differences? In particular, when to use <code>tf.Variable</code> and when to use <code>tf.placeholder</code>? </p>
",
Convert Keras model to C++,"<p>I am using Keras (with Theano) to train my CNN model. Does anyone has idea how can I use it in my C++ application? Does anyone tried something similar? I have idea to write some python code that will generate a c++ code with network functions - any suggestion on it?</p>

<p>I found a similar question <a href=""https://stackoverflow.com/questions/36412098/convert-keras-model-to-tensorflow-protobuf"">here</a> how to use Tensorflow Keras model in C++ but without answer.</p>
",
LSTM Autoencoder,"<p>I'm trying to build a LSTM autoencoder with the goal of getting a fixed sized vector from a sequence, which represents the sequence as good as possible. This autoencoder consists of two parts:</p>

<ul>
<li><code>LSTM</code> Encoder: Takes a sequence and returns an output vector (<code>return_sequences = False</code>)</li>
<li><code>LSTM</code> Decoder: Takes an output vector and returns a sequence (<code>return_sequences = True</code>)</li>
</ul>

<p>So, in the end, the encoder is a <strong>many to one</strong> LSTM and the decoder is a <strong>one to many</strong> LSTM.</p>

<p><a href=""https://i.stack.imgur.com/kwhAP.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/kwhAP.jpg"" alt=""enter image description here""></a></p>

<p>On a high level the coding looks like this (similar as described <a href=""https://github.com/fchollet/keras/issues/5138"" rel=""noreferrer"">here</a>):</p>

<pre><code>encoder = Model(...)
decoder = Model(...)

autoencoder = Model(encoder.inputs, decoder(encoder(encoder.inputs)))

autoencoder.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

autoencoder.fit(data, data,
          batch_size=100,
          epochs=1500)
</code></pre>

<p>The shape (number of training examples, sequence length, input dimension) of the <code>data</code> array is <code>(1200, 10, 5)</code> and looks like this:</p>

<pre><code>array([[[1, 0, 0, 0, 0],
        [0, 1, 0, 0, 0],
        [0, 0, 1, 0, 0],
        ..., 
        [0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0]],
        ... ]
</code></pre>

<p><strong>Problem:</strong> I am not sure how to proceed, especially how to integrate <code>LSTM</code> to <code>Model</code> and how to get the decoder to generate a sequence from a vector.</p>

<p>I am using <code>keras</code> with <code>tensorflow</code> backend.</p>

<p><strong>EDIT:</strong> If someone wants to try out, here is my procedure to generate random sequences with moving ones (including padding):</p>

<pre><code>import random
import math

def getNotSoRandomList(x):
    rlen = 8
    rlist = [0 for x in range(rlen)]
    if x &lt;= 7:
        rlist[x] = 1
    return rlist


sequence = [[getNotSoRandomList(x) for x in range(round(random.uniform(0, 10)))] for y in range(5000)]

### Padding afterwards

from keras.preprocessing import sequence as seq

data = seq.pad_sequences(
    sequences = sequence,
    padding='post',
    maxlen=None,
    truncating='post',
    value=0.
)
</code></pre>
",
ValueError: Trying to share variable rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel,"<p>This it the code:</p>

<pre><code>X = tf.placeholder(tf.float32, [batch_size, seq_len_1, 1], name='X')
labels = tf.placeholder(tf.float32, [None, alpha_size], name='labels')

rnn_cell = tf.contrib.rnn.BasicLSTMCell(512)
m_rnn_cell = tf.contrib.rnn.MultiRNNCell([rnn_cell] * 3, state_is_tuple=True)
pre_prediction, state = tf.nn.dynamic_rnn(m_rnn_cell, X, dtype=tf.float32)
</code></pre>

<p>This is full error:</p>

<blockquote>
  <p><strong>ValueError:</strong> Trying to share variable rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel, but specified shape (1024, 2048) and found shape (513, 2048).</p>
</blockquote>

<p>I'm using a GPU version of tensorflow. </p>
",
Appropriate Deep Learning Structure for multi-class classification,"<p>I have the following data</p>

<pre><code>         feat_1    feat_2 ... feat_n   label
gene_1   100.33     10.2  ... 90.23    great
gene_2   13.32      87.9  ... 77.18    soso
....
gene_m   213.32     63.2  ... 12.23    quitegood
</code></pre>

<p>The size of <code>M</code> is large ~30K rows, and <code>N</code> is much smaller ~10 columns.
My question is what is the appropriate Deep Learning structure to learn
and test the data like above.</p>

<p>At the end of the day, the user will give a vector of genes with expression.</p>

<pre><code>gene_1   989.00
gene_2   77.10
...
gene_N   100.10
</code></pre>

<p>And the system will label which label does each gene apply e.g. great or soso, etc...</p>

<p>By structure I mean one of these:</p>

<ul>
<li>Convolutional Neural Network (CNN)</li>
<li>Autoencoder</li>
<li>Deep Belief Network (DBN)</li>
<li>Restricted Boltzman Machine</li>
</ul>
",
"In Tensorflow, get the names of all the Tensors in a graph","<p>I am creating neural nets with <code>Tensorflow</code> and <code>skflow</code>; for some reason I want to get the values of some inner tensors for a given input, so I am using <code>myClassifier.get_layer_value(input, ""tensorName"")</code>, <code>myClassifier</code> being a <code>skflow.estimators.TensorFlowEstimator</code>. </p>

<p>However, I find it difficult to find the correct syntax of the tensor name, even knowing its name (and I'm getting confused between operation and tensors), so I'm using tensorboard to plot the graph and look for the name.</p>

<p>Is there a way to enumerate all the tensors in a graph without using tensorboard?</p>
",
Which Google Cloud Platform service is the easiest for running Tensorflow?,"<p>While working on Udacity Deep Learning assignments, I encountered memory problem. I need to switch to a cloud platform. I worked with AWS EC2 before but now I would like to try Google Cloud Platform (GCP). I will need at least 8GB memory. I know how to use docker locally but never tried it on the cloud.</p>

<ol>
<li>Is there any ready-made solution for running Tensorflow on GCP?</li>
<li>If not, which service (Compute Engine or Container Engine) would make it easier to get started?</li>
<li>Any other tip is also appreciated!</li>
</ol>
",
How to load sparse data with TensorFlow?,"<p>There is a small snippet about loading sparse data but I have no idea how to use it.</p>

<blockquote>
  <p>SparseTensors don't play well with queues. If you use SparseTensors you have to decode the string records using tf.parse_example after batching (instead of using tf.parse_single_example before batching).</p>
</blockquote>

<p><a href=""https://www.tensorflow.org/versions/r0.8/how_tos/reading_data/index.html#sparse-input-data"">Source</a></p>

<p>I guess I don't really get how the data is loaded.</p>

<p>The data I want to load is in the <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.datasets.dump_svmlight_file.html"">SVM Light</a> format</p>

<p>The way I am thinking of this is to convert the training set to the TFRecords file format and then load this converted data with tensorflow. The thing is I don't know how I am supposed to format my data so that tensorflow parses it as sparseTensors.</p>

<p>Here is a snippet extracted from <a href=""https://github.com/tensorflow/tensorflow/blob/r0.8/tensorflow/examples/how_tos/reading_data/convert_to_records.py"">one</a> the examples available on GitHub:</p>

<pre><code>def convert_to(images, labels, name):
  num_examples = labels.shape[0]
  if images.shape[0] != num_examples:
    raise ValueError(""Images size %d does not match label size %d."" %
                     (images.shape[0], num_examples))
  rows = images.shape[1]
  cols = images.shape[2]
  depth = images.shape[3]

  filename = os.path.join(FLAGS.directory, name + '.tfrecords')
  print('Writing', filename)
  writer = tf.python_io.TFRecordWriter(filename)
  for index in range(num_examples):
    image_raw = images[index].tostring()
    example = tf.train.Example(features=tf.train.Features(feature={
        'height': _int64_feature(rows),
        'width': _int64_feature(cols),
        'depth': _int64_feature(depth),
        'label': _int64_feature(int(labels[index])),
        'image_raw': _bytes_feature(image_raw)}))
    writer.write(example.SerializeToString())
  writer.close()
</code></pre>

<p>It encodes the image data as one big blob. The difference with my data is that not every feature is populated. I could be persisting my data in the same way but I am unsure this is the way to use the features.</p>

<p>That could not matter since I will be decoding things on the other hand but is there a better way to do this for sparse data ?</p>

<p>As for the reading, <a href=""https://github.com/tensorflow/tensorflow/blob/r0.8/tensorflow/examples/how_tos/reading_data/fully_connected_reader.py"">here</a> is one example that reads dense tensor data.</p>

<p>I got that I was suppose to swap <code>tf.parse_single_example</code> with <code>tf.parse_example</code> and do it after batching.</p>

<p>However, how do I tell tensorflow that my data is sparse ? How do I associate the features indexes I have with the feature values in the tensor ? How can I do batching before even having loaded the data ?</p>

<p>EDIT 1:</p>

<p>Here is what I tried, I get a <code>ValueError: Shape () must have rank 1</code> error:</p>

<pre><code>from tqdm import *

def convert_to_tensor_file(path, out_file_name):

    feature_set = set()

    filename = os.path.join(FLAGS.directory, out_file_name + '.tfrecords')
    writer = tf.python_io.TFRecordWriter(filename)

    with open(path, 'r') as f:
        for line in tqdm(f):
            data = line.strip().split(' ')
            features = {
                ""label"": _int64_feature(int(data[0]))
            }
            for feature in data[1:]:
                index, value = feature.split(':')

                feature_set.add(index)

                features[index] = _int64_feature(int(value))

            example = tf.train.Example(features=tf.train.Features(feature=features))
            writer.write(example.SerializeToString())
        writer.close()

    return feature_set

feature_set = convert_to_tensor_file(TRAIN, 'train')

def load_tensor_file(name):
    filename = os.path.join(FLAGS.directory, name + '.tfrecords')

    features = {
        'label': tf.FixedLenFeature([], tf.int64),
    }

    for feature in feature_set:
        features[feature] = tf.VarLenFeature(tf.int64)

    with tf.name_scope('input'):
        filename_queue = tf.train.string_input_producer([filename])

        reader = tf.TFRecordReader()
        _, serialized_example = reader.read(filename_queue)
        features = tf.parse_example(serialized_example, features=features)

load_tensor_file('train')
</code></pre>

<p>Thank you,</p>
",
Train Tensorflow Object Detection on own dataset,"<p>After spending a couple days trying to achieve this task, I would like to share my experience of how I went about answering the question:</p>

<p><em>How do I use <a href=""https://github.com/tensorflow/models/tree/master/research/object_detection"" rel=""nofollow noreferrer"">TS Object Detection</a> to train using my own dataset?</em></p>
",
How can I solve 'ran out of gpu memory' in TensorFlow,"<p>I ran the MNIST demo in TensorFlow with 2 conv layers and a full-conect layer, I got an message that 'ran out of memeory trying to allocate 2.59GiB' , but it shows that total memory is 4.69GiB, and free memory is 3.22GiB, how can it stop with 2.59GiB? And with larger network, how can I manage  gpu memory? I concern only how to make best use of the gpu memory and wanna know how it happened,  not how to pre-allocating memory</p>
",
How to convert numpy arrays to standard TensorFlow format?,"<p>I have two numpy arrays</p>

<ul>
<li>one that contains captcha images and </li>
<li>another that contains the corresponding labels(in one-hot vector format)</li>
</ul>

<p><strong>I want to load these into TensorFlow so I can classify them using a neural network. How can this be done ?</strong></p>

<p>What shape do the numpy arrays need to have? </p>

<p>Additional Info - My images are 60(height) by 160(width) pixels each and each of them have 5 alphanumeric characters</p>

<p><img src=""https://i.stack.imgur.com/lSZYb.png"" alt=""Here is a sample image."">
Each label is a 5 by 62 array. </p>
",
How to get the dimensions of a tensor (in TensorFlow) at graph construction time?,"<p>I am trying an Op that is not behaving as expected.</p>

<pre><code>graph = tf.Graph()
with graph.as_default():
  train_dataset = tf.placeholder(tf.int32, shape=[128, 2])
  embeddings = tf.Variable(
    tf.random_uniform([50000, 64], -1.0, 1.0))
  embed = tf.nn.embedding_lookup(embeddings, train_dataset)
  embed = tf.reduce_sum(embed, reduction_indices=0)
</code></pre>

<p>So I need to know the dimensions of the Tensor <code>embed</code>. I know that it can be done at the run time but it's too much work for such a simple operation. What's the easier way to do it?</p>
",
How do I set up TensorFlow in the Google cloud?,"<p>How do I set up a TensorFlow in the Google cloud? I understand how to create a Google Compute Engine instance, and how to run TensorFlow locally; and a <a href=""http://googleresearch.blogspot.com/2016/03/machine-learning-in-cloud-with.html"" rel=""noreferrer"">recent Google blog post</a> suggests that there ought to be a way to create a Google Compute Engine instance and run TensorFlow applications in the cloud: </p>

<blockquote>
  <p>Machine Learning projects can come in many sizes, and as we’ve seen
  with our open source offering TensorFlow, projects often need to scale
  up. Some small tasks are best handled with a local solution running on
  one’s desktop, while large scale applications require both the scale
  and dependability of a hosted solution. Google Cloud Machine Learning
  aims to support the full range and provide a seamless transition from
  local to cloud environment.</p>
</blockquote>

<p>Even if I'm reading a bit much into this, it has to be the case, given what competing platforms such as Microsoft's Azure offer, that there's a way to set up TensorFlow applications (developed locally and ""seamlessly"" scaled up into the cloud, presumably using GPUs) in the Google cloud.</p>

<p>For example, I'd like to work locally in my IDE tuning the features and code for my project, running limited training and validation there, and push the code periodically to the cloud to run train there with (arbitrarily) greater resources, and then save and download the trained model. Or perhaps even better, just run the graphs (or parts of graphs) in the cloud using tunable resources. </p>

<p>Is there a way to do this; is one planned? How do I set up TensorFlow in the Google cloud?</p>
",
Tensorflow: Confusion regarding the adam optimizer,"<p>I'm confused regarding as to how the adam optimizer actually works in tensorflow.</p>

<p>The way I read the <a href=""https://www.tensorflow.org/versions/r0.9/api_docs/python/train.html#AdamOptimizer"" rel=""noreferrer"">docs</a>, it says that the learning rate is changed every gradient descent iteration. </p>

<p>But when I call the function I give it a learning rate. And I don't call the function to let's say, do one epoch (implicitly calling # iterations so as to go through my data training). I call the function for each batch explicitly like</p>

<pre><code>for epoch in epochs
     for batch in data
          sess.run(train_adam_step, feed_dict={eta:1e-3})
</code></pre>

<p>So my eta cannot be changing. And I'm not passing a time variable in. Or is this some sort of generator type thing where upon session creation <code>t</code> is incremented each time I call the optimizer?</p>

<p>Assuming it is some generator type thing and the learning rate is being invisibly reduced: How could I get to run the adam optimizer without decaying the learning rate? It seems to me like <a href=""https://www.tensorflow.org/versions/r0.9/api_docs/python/train.html#RMSPropOptimizer"" rel=""noreferrer"">RMSProp</a> is basically the same, the only thing I'd have to do to make it equal (learning rate disregarded) is to change the hyperparameters <code>momentum</code> and <code>decay</code> to match <code>beta1</code> and <code>beta2</code> respectively. Is that correct?</p>
",
How to understand the term `tensor` in TensorFlow?,"<p>I'm new to TensorFlow. While I'm reading the documents. I found the term <code>tensor</code> really confusing. 
What's the relationship between <code>tensor</code> and <code>Variable</code>, <code>tensor</code> vs <code>tf.constant</code>, 'tensor' vs <code>tf.placeholder</code>? Are they all types of tensors?</p>
",
Custom metric based on tensorflow's streaming metrics returns NaN,"<p>I'm trying to define the F1-score as a custom metric in TensorFlow for a <code>DNNClassifier</code>. To do that, I wrote a function</p>

<pre><code>def metric_fn(predictions=[], labels=[], weights=[]):
    P, _ = tf.contrib.metrics.streaming_precision(predictions, labels)
    R, _ = tf.contrib.metrics.streaming_recall(predictions, labels)
    if P + R == 0:
        return 0
    return 2*(P*R)/(P+R)
</code></pre>

<p>that uses <code>streaming_precision</code> and <code>streaming_recall</code> from TensorFlow to calulate the F1 score. After that I made a new entry to the validation_metrics:</p>

<pre><code>validation_metrics = {
    ""accuracy"":
        tf.contrib.learn.MetricSpec(
            metric_fn=tf.contrib.metrics.streaming_accuracy,
            prediction_key=tf.contrib.learn.PredictionKey.CLASSES),
    ""precision"":
        tf.contrib.learn.MetricSpec(
            metric_fn=tf.contrib.metrics.streaming_precision,
            prediction_key=tf.contrib.learn.PredictionKey.CLASSES),
    ""recall"":
        tf.contrib.learn.MetricSpec(
            metric_fn=tf.contrib.metrics.streaming_recall,
            prediction_key=tf.contrib.learn.PredictionKey.CLASSES),
    ""f1score"":
        tf.contrib.learn.MetricSpec(
            metric_fn=metric_fn,
            prediction_key=tf.contrib.learn.PredictionKey.CLASSES)
}
</code></pre>

<p>However, although I get correct precision and recall values, <code>f1score</code> is always <code>nan</code>:</p>

<pre class=""lang-none prettyprint-override""><code>INFO:tensorflow:Saving dict for global step 151: accuracy = 0.982456, accuracy/baseline_label_mean = 0.397661, accuracy/threshold_0.500000_mean = 0.982456, auc = 0.982867, f1score = nan, global_step = 151, labels/actual_label_mean = 0.397661, labels/prediction_mean = 0.406118, loss = 0.310612, precision = 0.971014, precision/positive_threshold_0.500000_mean = 0.971014, recall = 0.985294, recall/positive_threshold_0.500000_mean = 0.985294
</code></pre>

<p>Something is wrong with my <code>metric_fn</code>, but I can't figure it out. 
The values <code>P</code> and <code>R</code> obtained by <code>metric_fn</code> are of the form
<code>Tensor(""precision/value:0"", shape=(), dtype=float32)</code>. I find this a bit strange. I was expecting a scalar tensor.</p>

<p>Any help is appreciated. </p>
",
Compute pairwise distance in a batch without replicating tensor in Tensorflow?,"<p>I want to compute the pairwise square distance of a batch of feature in Tensorflow. I have a simple implementation using + and * operations by
tiling the original tensor :</p>

<pre><code>def pairwise_l2_norm2(x, y, scope=None):
    with tf.op_scope([x, y], scope, 'pairwise_l2_norm2'):
        size_x = tf.shape(x)[0]
        size_y = tf.shape(y)[0]
        xx = tf.expand_dims(x, -1)
        xx = tf.tile(xx, tf.pack([1, 1, size_y]))

        yy = tf.expand_dims(y, -1)
        yy = tf.tile(yy, tf.pack([1, 1, size_x]))
        yy = tf.transpose(yy, perm=[2, 1, 0])

        diff = tf.sub(xx, yy)
        square_diff = tf.square(diff)

        square_dist = tf.reduce_sum(square_diff, 1)

        return square_dist
</code></pre>

<p>This function takes as input two matrices of size (m,d) and (n,d) and compute the squared distance between each row vector. The output is a matrix of size (m,n) with element 'd_ij = dist(x_i, y_j)'.</p>

<p>The problem is that I have a large batch and high dim features 'm, n, d' replicating the tensor consume a lot of memory. 
I'm looking for another way to implement this without increasing the memory usage and just only store the final distance tensor. Kind of double looping the original tensor.</p>
",
Using Keras & Tensorflow with AMD GPU,"<p>I'm starting to learn Keras, which I believe is a layer on top of Tensorflow and Theano.  However, I only have access to AMD GPUs such as the AMD R9 280X.</p>

<p>How can I setup my Python environment such that I can make use of my AMD GPUs through Keras/Tensorflow support for OpenCL?</p>

<p>I'm running on OSX.</p>
",
Tensorflow set CUDA_VISIBLE_DEVICES within jupyter,"<p>I have two GPUs and would like to run two different networks via ipynb simultaneously, however the first notebook always allocates both GPUs. </p>

<p>Using CUDA_VISIBLE_DEVICES, I can hide devices for python files, however I am unsure of how to do so within a notebook.</p>

<p>Is there anyway to hide different GPUs in to notebooks running on the same server?</p>
",
In tensorflow what is the difference between tf.add and operator (+)?,"<p>In tensorflow tutorials, I see both codes like <code>tf.add(tf.matmul(X, W), b)</code> and <code>tf.matmul(X, W) + b</code>, what is the difference between using the math function <code>tf.add()</code>, <code>tf.assign()</code>, etc and the operators <code>+</code> and <code>=</code>, etc, in precision or other aspects? </p>
",
What is num_units in tensorflow BasicLSTMCell?,"<p>In MNIST LSTM examples, I don't understand what ""hidden layer"" means. Is it the imaginary-layer formed when you represent an unrolled RNN over time? </p>

<p>Why is the <code>num_units = 128</code> in most cases ? </p>

<p>I know I should read colah's blog in detail to understand this, but, before that, I just want to get some code working with a sample time series data I have.</p>
",
How to manually create a tf.Summary(),"<p>I often want to log python variables --as opposed to tf tensors.</p>

<p>In the docs it says that ""you can pass a <code>tf.Summary</code> protocol buffer that you populate with your own data"" but there is no docs for <code>tf.Summary</code> and i could not figure out how to use it.</p>

<p>Anyone knows how to create a Scalar summary this way?</p>
",
How can I run Tensorboard on a remote server?,"<p>I'm new to Tensorflow and would greatly benefit from some visualizations of what I'm doing. I understand that Tensorboard is a useful visualization tool, but how do I run it on my remote Ubuntu machine?</p>
",
Trouble with TensorFlow in Jupyter Notebook,"<p>I installed Jupyter notebooks in Ubuntu 14.04 via Anaconda earlier, and just now I installed TensorFlow. I would like TensorFlow to work regardless of whether I am working in a notebook or simply scripting. In my attempt to achieve this, I ended up installing TensorFlow twice, once using Anaconda, and once using pip. The Anaconda install works, but I need to preface any call to python with ""source activate tensorflow"". And the pip install works nicely, if start python the standard way (in the terminal) then tensorflow loads just fine. </p>

<p>My question is: how can I also have it work in the Jupyter notebooks? </p>

<p>This leads me to a more general question: it seems that my python kernel in Jupyter/Anaconda is separate from the python kernel (or environment? not sure about the terminology here) used system wide. It would be nice if these coincided, so that if I install a new python library, it becomes accessible to all the varied ways I have of running python.</p>
",
how does tensorflow indexing work,"<p>I'm having trouble understanding a basic concept with tensorflow. How does indexing work for tensor read/write operations? In order to make this specific, how can the following numpy examples be translated to tensorflow (using tensors for the arrays, indices and values being assigned):</p>

<pre><code>x = np.zeros((3, 4))
row_indices = np.array([1, 1, 2])
col_indices = np.array([0, 2, 3])
x[row_indices, col_indices] = 2
x
</code></pre>

<p>with output:</p>

<pre><code>array([[ 0.,  0.,  0.,  0.],
       [ 2.,  0.,  2.,  0.],
       [ 0.,  0.,  0.,  2.]])
</code></pre>

<p>... and ...</p>

<pre><code>x[row_indices, col_indices] = np.array([5, 4, 3])
x
</code></pre>

<p>with output:</p>

<pre><code>array([[ 0.,  0.,  0.,  0.],
       [ 5.,  0.,  4.,  0.],
       [ 0.,  0.,  0.,  3.]])
</code></pre>

<p>... and finally ...</p>

<pre><code>y = x[row_indices, col_indices]
y
</code></pre>

<p>with output:</p>

<pre><code>array([ 5.,  4.,  3.])
</code></pre>
",
"TensorFlow - regularization with L2 loss, how to apply to all weights, not just last one?","<p>I am playing with a ANN which is part of Udacity DeepLearning course.</p>

<p>I have an assignment which involves introducing generalization to the network with one hidden ReLU layer using L2 loss. I wonder how to properly introduce it so that ALL weights are penalized, not only weights of the output layer.</p>

<p>Code for network <em>without</em> generalization is at the bottom of the post (code to actually run the training is out of the scope of the question).</p>

<p>Obvious way of introducing the L2 is to replace the loss calculation with something like this (if beta is 0.01):</p>

<pre><code>loss = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(out_layer, tf_train_labels) + 0.01*tf.nn.l2_loss(out_weights))
</code></pre>

<p>But in such case it will take into account values of output layer's weights. I am not sure, how do we properly penalize the weights which come INTO the hidden ReLU layer. Is it needed at all or introducing penalization of output layer will somehow keep the hidden weights in check also?</p>

<pre><code>#some importing
from __future__ import print_function
import numpy as np
import tensorflow as tf
from six.moves import cPickle as pickle
from six.moves import range

#loading data
pickle_file = '/home/maxkhk/Documents/Udacity/DeepLearningCourse/SourceCode/tensorflow/examples/udacity/notMNIST.pickle'

with open(pickle_file, 'rb') as f:
  save = pickle.load(f)
  train_dataset = save['train_dataset']
  train_labels = save['train_labels']
  valid_dataset = save['valid_dataset']
  valid_labels = save['valid_labels']
  test_dataset = save['test_dataset']
  test_labels = save['test_labels']
  del save  # hint to help gc free up memory
  print('Training set', train_dataset.shape, train_labels.shape)
  print('Validation set', valid_dataset.shape, valid_labels.shape)
  print('Test set', test_dataset.shape, test_labels.shape)


#prepare data to have right format for tensorflow
#i.e. data is flat matrix, labels are onehot

image_size = 28
num_labels = 10

def reformat(dataset, labels):
  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)
  # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...]
  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)
  return dataset, labels
train_dataset, train_labels = reformat(train_dataset, train_labels)
valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)
test_dataset, test_labels = reformat(test_dataset, test_labels)
print('Training set', train_dataset.shape, train_labels.shape)
print('Validation set', valid_dataset.shape, valid_labels.shape)
print('Test set', test_dataset.shape, test_labels.shape)


#now is the interesting part - we are building a network with
#one hidden ReLU layer and out usual output linear layer

#we are going to use SGD so here is our size of batch
batch_size = 128

#building tensorflow graph
graph = tf.Graph()
with graph.as_default():
      # Input data. For the training data, we use a placeholder that will be fed
  # at run time with a training minibatch.
  tf_train_dataset = tf.placeholder(tf.float32,
                                    shape=(batch_size, image_size * image_size))
  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))
  tf_valid_dataset = tf.constant(valid_dataset)
  tf_test_dataset = tf.constant(test_dataset)

  #now let's build our new hidden layer
  #that's how many hidden neurons we want
  num_hidden_neurons = 1024
  #its weights
  hidden_weights = tf.Variable(
    tf.truncated_normal([image_size * image_size, num_hidden_neurons]))
  hidden_biases = tf.Variable(tf.zeros([num_hidden_neurons]))

  #now the layer itself. It multiplies data by weights, adds biases
  #and takes ReLU over result
  hidden_layer = tf.nn.relu(tf.matmul(tf_train_dataset, hidden_weights) + hidden_biases)

  #time to go for output linear layer
  #out weights connect hidden neurons to output labels
  #biases are added to output labels  
  out_weights = tf.Variable(
    tf.truncated_normal([num_hidden_neurons, num_labels]))  

  out_biases = tf.Variable(tf.zeros([num_labels]))  

  #compute output  
  out_layer = tf.matmul(hidden_layer,out_weights) + out_biases
  #our real output is a softmax of prior result
  #and we also compute its cross-entropy to get our loss
  loss = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(out_layer, tf_train_labels))

  #now we just minimize this loss to actually train the network
  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)

  #nice, now let's calculate the predictions on each dataset for evaluating the
  #performance so far
  # Predictions for the training, validation, and test data.
  train_prediction = tf.nn.softmax(out_layer)
  valid_relu = tf.nn.relu(  tf.matmul(tf_valid_dataset, hidden_weights) + hidden_biases)
  valid_prediction = tf.nn.softmax( tf.matmul(valid_relu, out_weights) + out_biases) 

  test_relu = tf.nn.relu( tf.matmul( tf_test_dataset, hidden_weights) + hidden_biases)
  test_prediction = tf.nn.softmax(tf.matmul(test_relu, out_weights) + out_biases)
</code></pre>
",
How to add new embeddings for unknown words in Tensorflow (training & pre-set for testing),"<p>I am curious as to how I can add a normal-randomized 300 dimension vector (elements' type = tf.float32) whenever a word unknown to the pre-trained vocabulary is encountered. I am using pre-trained GloVe word embeddings, but in some cases, I realize I encounter unknown words, and I want to create a normal-randomized word vector for this new found unknown word. </p>

<p>The problem is that with my current set up, I use <a href=""https://www.tensorflow.org/api_docs/python/tf/contrib/lookup/index_table_from_tensor"" rel=""nofollow noreferrer"">tf.contrib.lookup.index_table_from_tensor</a> to convert from words to integers based on the known vocabulary. This function can create new tokens and hash them for some predefined number of out of vocabulary words, but my <code>embed</code> will not contain an embedding for this new unknown hash value. I am uncertain if I can simply append a randomized embedding to the end of the <code>embed</code> list.</p>

<p>I also would like to do this in an efficient way, so pre-built tensorflow function or method involving tensorflow functions would probably be the most efficient. I define pre-known special tokens such as an end of sentence token and a default unknown as the empty string ("""" at index 0), but this is limited in its power to learn for various different unknown words. I currently use <a href=""https://www.tensorflow.org/api_docs/python/tf/nn/embedding_lookup"" rel=""nofollow noreferrer"">tf.nn.embedding_lookup()</a> as the final embedding step.</p>

<p>I would like to be able to add new random 300d vectors for each unknown word in the training data, and I would also like to add pre-made random word vectors for any unknown tokens not seen in training that are possibly encountered during testing. What is the most efficient way of doing this?</p>

<pre><code>def embed_tensor(string_tensor, trainable=True):
    """"""    
    Convert List of strings into list of indicies then into 300d vectors
    """"""
    # ordered lists of vocab and corresponding (by index) 300d vector
    vocab, embed = load_pretrained_glove()

    # Set up tensorflow look up from string word to unique integer
    vocab_lookup = tf.contrib.lookup.index_table_from_tensor(
        mapping=tf.constant(vocab),
        default_value = 0)
    string_tensor = vocab_lookup.lookup(string_tensor)

    # define the word embedding 
    embedding_init = tf.Variable(tf.constant(np.asarray(embed),
                                 dtype=tf.float32),
                                 trainable=trainable,
                                 name=""embed_init"")

    # return the word embedded version of the sentence (300d vectors/word)
    return tf.nn.embedding_lookup(embedding_init, string_tensor)
</code></pre>
",
TensorFlow - introducing both L2 regularization and dropout into the network. Does it makes any sense?,"<p>I am currently playing with ANN which is part of Udactity DeepLearning course.</p>

<p>I successful built and train network and introduced the L2 regularization on all weights and biases. Right now I am trying out the dropout for hidden layer in order to improve generalization. I wonder, does it makes sense to both introduce the L2 regularization into the hidden layer and dropout on that same layer? If so, how to do this properly?</p>

<p>During dropout we literally switch off half of the activations of hidden layer and double the amount outputted by rest of the neurons. While using the L2 we compute the L2 norm on all hidden weights. But I am not sure how  to compute L2 in case we use dropout. We switch off some activations, shouldn't we remove the weights which are 'not used' now from the L2 calculation? Any references on that matter will be useful, I haven't found any info.</p>

<p>Just in case you are interested, my code for ANN with L2 regularization is below:</p>

<pre class=""lang-py prettyprint-override""><code>#for NeuralNetwork model code is below
#We will use SGD for training to save our time. Code is from Assignment 2
#beta is the new parameter - controls level of regularization. Default is 0.01
#but feel free to play with it
#notice, we introduce L2 for both biases and weights of all layers

beta = 0.01

#building tensorflow graph
graph = tf.Graph()
with graph.as_default():
      # Input data. For the training data, we use a placeholder that will be fed
  # at run time with a training minibatch.
  tf_train_dataset = tf.placeholder(tf.float32,
                                    shape=(batch_size, image_size * image_size))
  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))
  tf_valid_dataset = tf.constant(valid_dataset)
  tf_test_dataset = tf.constant(test_dataset)

  #now let's build our new hidden layer
  #that's how many hidden neurons we want
  num_hidden_neurons = 1024
  #its weights
  hidden_weights = tf.Variable(
    tf.truncated_normal([image_size * image_size, num_hidden_neurons]))
  hidden_biases = tf.Variable(tf.zeros([num_hidden_neurons]))

  #now the layer itself. It multiplies data by weights, adds biases
  #and takes ReLU over result
  hidden_layer = tf.nn.relu(tf.matmul(tf_train_dataset, hidden_weights) + hidden_biases)

  #time to go for output linear layer
  #out weights connect hidden neurons to output labels
  #biases are added to output labels  
  out_weights = tf.Variable(
    tf.truncated_normal([num_hidden_neurons, num_labels]))  

  out_biases = tf.Variable(tf.zeros([num_labels]))  

  #compute output  
  out_layer = tf.matmul(hidden_layer,out_weights) + out_biases
  #our real output is a softmax of prior result
  #and we also compute its cross-entropy to get our loss
  #Notice - we introduce our L2 here
  loss = (tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(
    out_layer, tf_train_labels) +
    beta*tf.nn.l2_loss(hidden_weights) +
    beta*tf.nn.l2_loss(hidden_biases) +
    beta*tf.nn.l2_loss(out_weights) +
    beta*tf.nn.l2_loss(out_biases)))

  #now we just minimize this loss to actually train the network
  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)

  #nice, now let's calculate the predictions on each dataset for evaluating the
  #performance so far
  # Predictions for the training, validation, and test data.
  train_prediction = tf.nn.softmax(out_layer)
  valid_relu = tf.nn.relu(  tf.matmul(tf_valid_dataset, hidden_weights) + hidden_biases)
  valid_prediction = tf.nn.softmax( tf.matmul(valid_relu, out_weights) + out_biases) 

  test_relu = tf.nn.relu( tf.matmul( tf_test_dataset, hidden_weights) + hidden_biases)
  test_prediction = tf.nn.softmax(tf.matmul(test_relu, out_weights) + out_biases)



#now is the actual training on the ANN we built
#we will run it for some number of steps and evaluate the progress after 
#every 500 steps

#number of steps we will train our ANN
num_steps = 3001

#actual training
with tf.Session(graph=graph) as session:
  tf.initialize_all_variables().run()
  print(""Initialized"")
  for step in range(num_steps):
    # Pick an offset within the training data, which has been randomized.
    # Note: we could use better randomization across epochs.
    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)
    # Generate a minibatch.
    batch_data = train_dataset[offset:(offset + batch_size), :]
    batch_labels = train_labels[offset:(offset + batch_size), :]
    # Prepare a dictionary telling the session where to feed the minibatch.
    # The key of the dictionary is the placeholder node of the graph to be fed,
    # and the value is the numpy array to feed to it.
    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}
    _, l, predictions = session.run(
      [optimizer, loss, train_prediction], feed_dict=feed_dict)
    if (step % 500 == 0):
      print(""Minibatch loss at step %d: %f"" % (step, l))
      print(""Minibatch accuracy: %.1f%%"" % accuracy(predictions, batch_labels))
      print(""Validation accuracy: %.1f%%"" % accuracy(
        valid_prediction.eval(), valid_labels))
      print(""Test accuracy: %.1f%%"" % accuracy(test_prediction.eval(), test_labels))
</code></pre>
",
"TensorFlow/TFLearn: ValueError: Cannot feed value of shape (64,) for Tensor u'target/Y:0', which has shape '(?, 10)'","<p>I have been trying to perform regression using <a href=""https://github.com/tflearn/tflearn"" rel=""noreferrer"">tflearn</a> and my own dataset.</p>

<p>Using tflearn I have been trying to implement a convolutional network based off an <a href=""https://github.com/tflearn/tflearn/blob/master/examples/images/convnet_mnist.py"" rel=""noreferrer"">example</a> using the MNIST dataset. Instead of using the MNIST dataset I have tried replacing the training and test data with my own. My data is read in from a csv file and is a different shape to the MNIST data. I have 255 features which represent a 15*15 grid and a target value. In the example I replaced the lines 24-30 with (and included import numpy as np):</p>

<pre><code>#read in train and test csv's where there are 255 features (15*15) and a target
csvTrain = np.genfromtxt('train.csv', delimiter="","")
X = np.array(csvTrain[:, :225]) #225, 15
Y = csvTrain[:,225]

csvTest = np.genfromtxt('test.csv', delimiter="","")
testX = np.array(csvTest[:, :225])
testY = csvTest[:,225]

#reshape features for each instance in to 15*15, targets are just a single number
X = X.reshape([-1,15,15,1])
testX = testX.reshape([-1,15,15,1])

## Building convolutional network
network = input_data(shape=[None, 15, 15, 1], name='input')
</code></pre>

<p>I get the following error:</p>

<blockquote>
  <p>ValueError: Cannot feed value of shape (64,) for Tensor u'target/Y:0',
  which has shape '(?, 10)'</p>
</blockquote>

<p>I have tried various combinations and have seen a <a href=""https://stackoverflow.com/questions/37238653/tensorflow-tflearn-valueerror-cannot-feed-value-of-shape-256-400-400-for-t"">similar question</a> in stackoverflow but have not had success. The example in this page does not work for me and throws a similar error and I do not understand the answer provided or those provided by similar questions.</p>

<p>How do I use my own data?</p>
",
How to use tf.while_loop() in tensorflow,"<p>This is a generic question. I found that in the tensorflow, after we build the graph, fetch data into the graph, the output from graph is a tensor. but in many cases, we need to do some computation based on this output (which is a <code>tensor</code>), which is not allowed in tensorflow. </p>

<p>for example, I'm trying to implement a RNN, which loops times based on data self property. That is, I need use a <code>tensor</code> to judge whether I should stop (I am not using dynamic_rnn since in my design, the rnn is highly customized). I find <code>tf.while_loop(cond,body.....)</code> might be a candidate for my implementation. But the official tutorial is too simple. I don't know how to add more functionalities into the 'body'. Can anyone give me few more complex example? </p>

<p>Also, in such case that if the future computation is based on the tensor output (ex: the RNN stop based on the output criterion), which is very common case. Is there an elegant way or better way instead of dynamic graph?</p>
",
What is the difference between steps and epochs in TensorFlow?,"<p>In most of the models, there is a <em>steps</em> parameter indicating the <em>number of steps to run over data</em>. But yet I see in most practical usage, we also execute the fit function N <em>epochs</em>. </p>

<p>What is the difference between running 1000 steps with 1 epoch and running 100 steps with 10 epoch? Which one is better in practice? Any logic changes between consecutive epochs? Data shuffling?</p>
",
Faster RCNN for TensorFlow,"<p>Has anyone implement the FRCNN for TensorFlow version?
I found some related repos as following:</p>

<ol>
<li><a href=""https://github.com/yuxng/tensorflow"">Implement roi pool layer</a></li>
<li><a href=""https://github.com/ExonRen/tf-fast-rcnn"">Implement fast RCNN based on py-faster-rcnn repo</a></li>
</ol>

<p>but for 1: assume the roi pooling layer works (I haven't tried), and there are something need to be implemented as following:</p>

<ul>
<li>ROI data layer e.g. <a href=""https://github.com/rbgirshick/py-faster-rcnn/blob/master/lib/roi_data_layer/roidb.py"">roidb</a>.</li>
<li>Linear Regression e.g. <a href=""https://github.com/rbgirshick/caffe-fast-rcnn/blob/0dcd397b29507b8314e252e850518c5695efbb83/src/caffe/layers/smooth_L1_loss_layer.cu"">SmoothL1Loss</a></li>
<li>ROI pool layer post-processing for end-to-end training which should convert the ROI pooling layer's results to feed into CNN for classifier.</li>
</ul>

<p>For 2: em...., it seems based on py-faster-rcnn which based on Caffe to prepared pre-processing (e.g. roidb) and feed data into Tensorflow to train the model, it seems weird, so I may not tried it.</p>

<p>So what I want to know is that, will <a href=""https://github.com/tensorflow/tensorflow/issues/739"">Tensorflow support Faster RCNN in the future</a>?. If not, do I have any mis-understand which mentioned above? or has any repo or someone support that?</p>
",
Tensorflow: Can't understand ctc_beam_search_decoder() output sequence,"<p>I am using Tensorflow's <code>tf.nn.ctc_beam_search_decoder()</code> to decode the output of a RNN doing some many-to-many mapping (i.e., multiple softmax outputs for each network cell).</p>

<p>A simplified version of the network's output and the Beam search decoder is:</p>

<pre><code>import numpy as np
import tensorflow as tf

batch_size = 4
sequence_max_len = 5
num_classes = 3

y_pred = tf.placeholder(tf.float32, shape=(batch_size, sequence_max_len, num_classes))
y_pred_transposed = tf.transpose(y_pred,
                                 perm=[1, 0, 2])  # TF expects dimensions [max_time, batch_size, num_classes]
logits = tf.log(y_pred_transposed)
sequence_lengths = tf.to_int32(tf.fill([batch_size], sequence_max_len))
decoded, log_probabilities = tf.nn.ctc_beam_search_decoder(logits,
                                                           sequence_length=sequence_lengths,
                                                           beam_width=3,
                                                           merge_repeated=False, top_paths=1)

decoded = decoded[0]
decoded_paths = tf.sparse_tensor_to_dense(decoded)  # Shape: [batch_size, max_sequence_len]

with tf.Session() as session:
    tf.global_variables_initializer().run()

    softmax_outputs = np.array([[[0.1, 0.1, 0.8], [0.8, 0.1, 0.1], [0.8, 0.1, 0.1], [0.8, 0.1, 0.1], [0.8, 0.1, 0.1]],
                                [[0.1, 0.2, 0.7], [0.1, 0.2, 0.7], [0.1, 0.2, 0.7], [0.1, 0.2, 0.7], [0.1, 0.2, 0.7]],
                                [[0.1, 0.7, 0.2], [0.1, 0.2, 0.7], [0.1, 0.2, 0.7], [0.1, 0.2, 0.7], [0.1, 0.2, 0.7]],
                                [[0.1, 0.2, 0.7], [0.1, 0.2, 0.7], [0.1, 0.2, 0.7], [0.1, 0.2, 0.7], [0.1, 0.2, 0.7]]])

    decoded_paths = session.run(decoded_paths, feed_dict = {y_pred: softmax_outputs})
    print(decoded_paths)
</code></pre>

<p>The output in this case is:</p>

<pre><code>[[0]
 [1]
 [1]
 [1]]
</code></pre>

<p>My understanding is that the output tensor should be of dimensions <code>[batch_size, max_sequence_len]</code>, with each row containing the indices of the relevant classes in the found path.</p>

<p>In this case I would expect the output to be similar to:</p>

<pre><code>[[2, 0, 0, 0, 0],
 [2, 2, 2, 2, 2],
 [1, 2, 2, 2, 2],
 [2, 2, 2, 2, 2]]
</code></pre>

<p>What am I not understanding about how <code>ctc_beam_search_decoder</code> works?</p>
",
What does opt.apply_gradients() do in TensorFlow?,"<p>The documentation is not quite clear about this. I suppose the gradients one can obtain by <code>opt.compute_gradients(E, [v])</code> contain the <code>∂E/∂x = g(x)</code> for each element <code>x</code> of the tensor that <code>v</code> stores. Does <code>opt.apply_gradients(grads_and_vars)</code> essentially execute <code>x ← -η·g(x)</code>, where <code>η</code> is the learning rate? That would imply that if I want to add a positive additive change <code>p</code> to the variable, I would need to need to change <code>g(x) ← g(x) - (1/η)p</code>, e.g. like this:</p>

<pre class=""lang-py prettyprint-override""><code>opt = tf.train.GradientDescentOptimizer(learning_rate=l)
grads_and_vars = opt.compute_gradients(loss, var_list)

for l, gv in enumerate(grads_and_vars):
    grads_and_vars[l] = (gv[0] - (1/l) * p, gv[1])

train_op = opt.apply_gradients(grads_and_vars)
</code></pre>

<p>Is there a better way to do this?</p>
",
Using deep learning models from TensorFlow in other language environments,"<p>I have a decent amount of experience with TensorFlow, and I am about to embark on a project which will ultimately culminate in using a TensorFlow trained model in a C# production environment. Essentially, I will have live data which will come into the C# environment, and I will ultimately need to output decisions / take certain actions based on the output of my model in TensorFlow. This is basically just a constraint of the existing infrastructure.</p>

<p>I can think of a couple of potentially bad ways to implement this, such as writing the data to disk and then calling the Python part of the application and then finally reading the result output by the Python application and taking some action based on it. This is slow, however.</p>

<p>Are there faster ways to accomplish this same integrated relationship between C# and the Python-based Tensorflow. I see that <a href=""https://tensorflow.github.io/serving/serving_basic"">there appear to be some ways</a> to do this with C++ and TensorFlow, but what about C#? </p>
",
Saving and reading variable size list from TFRecord,"

<p>What would be the best way to store sparse vector to TFRecord? My sparse vector only contains ones and zeros so I decided I'll just save indexes where 'ones' are located like this:</p>

<pre class=""lang-py prettyprint-override""><code>example = tf.train.Example(
        features=tf.train.Features(
            feature={
                'label': self._int64_feature(label),
                'features' : self._int64_feature_list(values)
            }
        )
    )
</code></pre>

<p>Here, <code>values</code> is list containing indexes of 'ones'. This <code>values</code> array sometimes contains hundreds of elements, sometimes none at all. After that I simply save the serialized example to tfrecord. Later, I'm reading tfrecord like this:</p>

<pre class=""lang-py prettyprint-override""><code>features = tf.parse_single_example(
    serialized_example,
    features={
        # We know the length of both fields. If not the
        # tf.VarLenFeature could be used
        'label': tf.FixedLenFeature([], dtype=tf.int64),
        'features': tf.VarLenFeature(dtype=tf.int64)
    }
)

label = features['label']
values = features['features']
</code></pre>

<p>This doesn't work because <code>values</code> array is recognized as a sparse array and I don't get data that I have saved. What is the best way to store sparse tensor in tfrecords and how to read it?</p>
",
Tensorflow: restoring a graph and model then running evaluation on a single image,"<p>I think it would be immensely helpful to the Tensorflow community if there was a well-documented solution to the crucial task of testing a single new image against the model created by the <a href=""https://www.tensorflow.org/versions/r0.8/tutorials/deep_cnn/index.html"" rel=""noreferrer"">convnet in the CIFAR-10 tutorial</a>. </p>

<p>I may be wrong, but this critical step that makes the trained model usable in practice seems to be lacking. There is a ""missing link"" in that tutorial—a script that would directly load a single image (as array or binary), compare it against the trained model, and return a classification.</p>

<p>Prior answers give partial solutions that explain the overall approach, but none of which I've been able to implement successfully. Other bits and pieces can be found here and there, but unfortunately haven't added up to a working solution. Kindly consider the research I've done, before tagging this as duplicate or already answered.</p>

<p><a href=""https://stackoverflow.com/questions/33759623/tensorflow-how-to-restore-a-previously-saved-model-python"">Tensorflow: how to save/restore a model?</a></p>

<p><a href=""https://stackoverflow.com/questions/34982492/restoring-tensorflow-model"">Restoring TensorFlow model</a></p>

<p><a href=""https://stackoverflow.com/questions/37187597/unable-to-restore-models-in-tensorflow-v0-8"">Unable to restore models in tensorflow v0.8</a></p>

<p><a href=""https://gist.github.com/nikitakit/6ef3b72be67b86cb7868"" rel=""noreferrer"">https://gist.github.com/nikitakit/6ef3b72be67b86cb7868</a></p>

<p>The most popular answer is the first, in which @RyanSepassi and @YaroslavBulatov describe the problem and an approach: one needs to ""manually construct a graph with identical node names, and use Saver to load the weights into it"". Although both answers are helpful, it is not apparent how one would go about plugging this into the CIFAR-10 project.</p>

<p>A fully functional solution would be highly desirable so we could port it to other single image classification problems. There are several questions on SO in this regard that ask for this, but still no full answer (for example <a href=""https://stackoverflow.com/questions/37058236/load-checkpoint-and-evaluate-single-image-with-tensorflow-dnn"">Load checkpoint and evaluate single image with tensorflow DNN</a>).</p>

<p>I hope we can converge on a working script that everyone could use.</p>

<p>The below script is not yet functional, and I'd be happy to hear from you on how this can be improved to provide a solution for single-image classification using the CIFAR-10 TF tutorial trained model.</p>

<p>Assume all variables, file names etc. are untouched from the original tutorial.</p>

<p>New file: <strong>cifar10_eval_single.py</strong></p>

<pre><code>import cv2
import tensorflow as tf

FLAGS = tf.app.flags.FLAGS

tf.app.flags.DEFINE_string('eval_dir', './input/eval',
                           """"""Directory where to write event logs."""""")
tf.app.flags.DEFINE_string('checkpoint_dir', './input/train',
                           """"""Directory where to read model checkpoints."""""")

def get_single_img():
    file_path = './input/data/single/test_image.tif'
    pixels = cv2.imread(file_path, 0)
    return pixels

def eval_single_img():

    # below code adapted from @RyanSepassi, however not functional
    # among other errors, saver throws an error that there are no
    # variables to save
    with tf.Graph().as_default():

        # Get image.
        image = get_single_img()

        # Build a Graph.
        # TODO

        # Create dummy variables.
        x = tf.placeholder(tf.float32)
        w = tf.Variable(tf.zeros([1, 1], dtype=tf.float32))
        b = tf.Variable(tf.ones([1, 1], dtype=tf.float32))
        y_hat = tf.add(b, tf.matmul(x, w))

        saver = tf.train.Saver()

        with tf.Session() as sess:
            sess.run(tf.initialize_all_variables())
            ckpt = tf.train.get_checkpoint_state(FLAGS.checkpoint_dir)

            if ckpt and ckpt.model_checkpoint_path:
                saver.restore(sess, ckpt.model_checkpoint_path)
                print('Checkpoint found')
            else:
                print('No checkpoint found')

            # Run the model to get predictions
            predictions = sess.run(y_hat, feed_dict={x: image})
            print(predictions)

def main(argv=None):
    if tf.gfile.Exists(FLAGS.eval_dir):
        tf.gfile.DeleteRecursively(FLAGS.eval_dir)
    tf.gfile.MakeDirs(FLAGS.eval_dir)
    eval_single_img()

if __name__ == '__main__':
    tf.app.run()
</code></pre>
",
Machine Learning (tensorflow / sklearn) in Django?,"<p>I have a django form, which is collecting user response. I also have a tensorflow sentences classification model. What is the best/standard way to  put these two together.
Details: </p>

<ol>
<li>tensorflow model was trained on the Movie Review data from Rotten Tomatoes.</li>
<li>Everytime a new row is made in my response model , i want the tensorflow code to classify it( + or - ).</li>
<li>Basically I have a django project directory and two .py files for classification. Before going ahead myself , i wanted to know what is the standard way to implement machine learning algorithms to a web app.</li>
</ol>

<p>It'd be awesome if you could suggest a tutorial or a repo.
Thank you !</p>
",
Custom padding for convolutions in TensorFlow,"<p>In tensorflow function <a href=""https://www.tensorflow.org/versions/r0.9/api_docs/python/nn.html#conv2d"" rel=""noreferrer"">tf.nn.conv2d</a>, the padding option just has 'SAME' and 'VALID'. </p>

<p>But in the conv layer of Caffe, there is <a href=""http://caffe.berkeleyvision.org/tutorial/layers.html"" rel=""noreferrer"">pad option</a> can define the number of pixels to (implicitly) add to each side of the input.</p>

<p>How to achieve that in Tensorflow?</p>

<p>Thank you very much.</p>
",
Run Tensorflow on CPU,"<p>I have installed the GPU version of tensorflow on an Ubuntu 14.04.</p>

<p>I am on a GPU server where tensorflow can access the available GPUs.</p>

<p>I want to run tensorflow on the CPUs.</p>

<p>Normally I can use <code>env CUDA_VISABLE_DEVICE=0</code> to run on GPU no. 0.</p>

<p>How can I pick between the CPUs instead?</p>

<p>I am not intersted in rewritting my code with <code>with tf.device(""/cpu:0""):</code></p>
",
TensorFlow Object Detection API print objects found on image to console,"<p>I'm trying to return list of objects that have been found at image with <em>TF Object Detection API</em>. </p>

<p>To do that I'm using <code>print([category_index.get(i) for i in classes[0]])</code> to print list of objects that have been found or <code>print(num_detections)</code> to display number of found objects, but in both cases it gives me list  with 300 values or simply value <code>[300.]</code> correspondingly.</p>

<p>How it`s possible to return only that objects that are on image? Or if there is some mistake please help to figure out what is wrong.</p>

<p>I was using <em>Faster RCNN</em> models config file and checkpoints while training. Be sure it really detects few objects at image, here it is:</p>

<p><a href=""https://i.stack.imgur.com/5VZ7l.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5VZ7l.png"" alt=""enter image description here""></a></p>

<p>My code:</p>

<pre><code>import numpy as np
import os
import six.moves.urllib as urllib
import sys
import tarfile
import tensorflow as tf
import zipfile

from collections import defaultdict
from io import StringIO
from matplotlib import pyplot as plt
from PIL import Image

from object_detection.utils import label_map_util
from object_detection.utils import visualization_utils as vis_util

PATH_TO_CKPT = 'frozen_graph/frozen_inference_graph.pb'

PATH_TO_LABELS = 'object_detection/pascal_label_map.pbtxt'

NUM_CLASSES = 7

detection_graph = tf.Graph()
with detection_graph.as_default():
  od_graph_def = tf.GraphDef()
  with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:
    serialized_graph = fid.read()
    od_graph_def.ParseFromString(serialized_graph)
    tf.import_graph_def(od_graph_def, name='')

label_map = label_map_util.load_labelmap(PATH_TO_LABELS)
categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)
category_index = label_map_util.create_category_index(categories)

def load_image_into_numpy_array(image):
  (im_width, im_height) = image.size
  return np.array(image.getdata()).reshape(
      (im_height, im_width, 3)).astype(np.uint8)


PATH_TO_TEST_IMAGES_DIR = 'object_detection/test_images/'
TEST_IMAGE_PATHS = [ os.path.join(PATH_TO_TEST_IMAGES_DIR, 'image{}.jpg'.format(i)) for i in range(1, 2) ]

IMAGE_SIZE = (12, 8)

with detection_graph.as_default():
  with tf.Session(graph=detection_graph) as sess:
    sess.run(tf.global_variables_initializer())
    img = 1
    for image_path in TEST_IMAGE_PATHS:
      image = Image.open(image_path)
      image_np = load_image_into_numpy_array(image)
      # Expand dimensions since the model expects images to have shape: [1, None, None, 3]
      image_np_expanded = np.expand_dims(image_np, axis=0)
      image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')
      # Each box represents a part of the image where a particular object was detected.
      boxes = detection_graph.get_tensor_by_name('detection_boxes:0')
      scores = detection_graph.get_tensor_by_name('detection_scores:0')
      classes = detection_graph.get_tensor_by_name('detection_classes:0')
      num_detections = detection_graph.get_tensor_by_name('num_detections:0')

      (boxes, scores, classes, num_detections) = sess.run(
          [boxes, scores, classes, num_detections],
          feed_dict={image_tensor: image_np_expanded})

      vis_util.visualize_boxes_and_labels_on_image_array(
          image_np,
          np.squeeze(boxes),
          np.squeeze(classes).astype(np.int32),
          np.squeeze(scores),
          category_index,
          use_normalized_coordinates=True,
          line_thickness=8)
      plt.figure(figsize=IMAGE_SIZE)
      plt.imsave('RESULTS/' + str(img) + '.jpg', image_np)
      img += 1

      # Return found objects
      print([category_index.get(i) for i in classes[0]])
      print(boxes.shape)
      print(num_detections)
</code></pre>

<p>Which gives following result:</p>

<pre><code>[{'name': 'marlboro_red', 'id': 7}, {'name': 'marlboro_red', 'id': 7}, {'name': 'marlboro_red', 'id': 7}, {'name': 'chesterfield_blue', 'id': 1}, {'name': 'chesterfield_blue', 'id': 1}, {'name': 'marlboro_gold', 'id': 5}, {'name': 'marlboro_red', 'id': 7}, {'name': 'lucky_strike_red', 'id': 4}, {'name': 'marlboro_mentol', 'id': 6}, {'name': 'lucky_strike_blue', 'id': 3}, {'name': 'marlboro_mentol', 'id': 6}, {'name': 'marlboro_gold', 'id': 5}, {'name': 'lucky_strike_red', 'id': 4}, {'name': 'chesterfield_red', 'id': 2}, {'name': 'marlboro_mentol', 'id': 6}, {'name': 'marlboro_gold', 'id': 5}, {'name': 'marlboro_mentol', 'id': 6}, {'name': 'chesterfield_blue', 'id': 1}, {'name': 'chesterfield_blue', 'id': 1}, {'name': 'marlboro_gold', 'id': 5}, {'name': 'marlboro_gold', 'id': 5}, {'name': 'chesterfield_red', 'id': 2}, {'name': 'chesterfield_red', 'id': 2}, {'name': 'lucky_strike_red', 'id': 4}, {'name': 'marlboro_gold', 'id': 5}, {'name': 'marlboro_gold', 'id': 5}, {'name': 'marlboro_mentol', 'id': 6}, {'name': 'chesterfield_blue', 'id': 1}, {'name': 'marlboro_red', 'id': 7}, {'name': 'chesterfield_red', 'id': 2}, {'name': 'lucky_strike_red', 'id': 4}, {'name': 'chesterfield_red', 'id': 2}, {'name': 'marlboro_mentol', 'id': 6}, {'name': 'chesterfield_blue', 'id': 1}, {'name': 'marlboro_mentol', 'id': 6}, {'name': 'marlboro_gold', 'id': 5}, {'name': 'marlboro_red', 'id': 7}, {'name': 'lucky_strike_blue', 'id': 3}, {'name': 'chesterfield_red', 'id': 2}, {'name': 'lucky_strike_red', 'id': 4}, {'name': 'lucky_strike_blue', 'id': 3}, {'name': 'marlboro_red', 'id': 7}, {'name': 'chesterfield_blue', 'id': 1}, {'name': 'marlboro_gold', 'id': 5}, {'name': 'marlboro_mentol', 'id': 6}, {'name': 'lucky_strike_blue', 'id': 3}, {'name': 'marlboro_mentol', 'id': 6}, {'name': 'marlboro_mentol', 'id': 6}, {'name': 'chesterfield_red', 'id': 2}, {'name': 'chesterfield_blue', 'id': 1}, {'name': 'marlboro_red', 'id': 7}, {'name': 'lucky_strike_blue', 'id': 3}, {'name': 'chesterfield_red', 'id': 2}, {'name': 'lucky_strike_blue', 'id': 3}, {'name': 'lucky_strike_blue', 'id': 3}, {'name': 'chesterfield_blue', 'id': 1}, {'name': 'marlboro_red', 'id': 7}, {'name': 'chesterfield_red', 'id': 2}, {'name': 'chesterfield_blue', 'id': 1}, {'name': 'lucky_strike_red', 'id': 4}, {'name': 'marlboro_gold', 'id': 5}, {'name': 'marlboro_red', 'id': 7}, {'name': 'marlboro_red', 'id': 7}, {'name': 'lucky_strike_red', 'id': 4}, {'name': 'lucky_strike_blue', 'id': 3}, {'name': 'marlboro_red', 'id': 7}, {'name': 'chesterfield_blue', 'id': 1}, {'name': 'lucky_strike_red', 'id': 4}, {'name': 'marlboro_red', 'id': 7}, {'name': 'marlboro_mentol', 'id': 6}, {'name': 'marlboro_mentol', 'id': 6}, {'name': 'lucky_strike_blue', 'id': 3}, {'name': 'marlboro_red', 'id': 7}, {'name': 'marlboro_mentol', 'id': 6}, {'name': 'marlboro_red', 'id': 7}, {'name': 'marlboro_gold', 'id': 5}, {'name': 'lucky_strike_red', 'id': 4}, {'name': 'lucky_strike_red', 'id': 4}, {'name': 'lucky_strike_blue', 'id': 3}, {'name': 'marlboro_gold', 'id': 5}, {'name': 'marlboro_red', 'id': 7}, {'name': 'chesterfield_red', 'id': 2}, {'name': 'marlboro_red', 'id': 7}, {'name': 'marlboro_mentol', 'id': 6}, {'name': 'marlboro_red', 'id': 7}, {'name': 'chesterfield_blue', 'id': 1}, {'name': 'marlboro_mentol', 'id': 6}, {'name': 'marlboro_red', 'id': 7}, {'name': 'chesterfield_red', 'id': 2}, {'name': 'marlboro_red', 'id': 7}, {'name': 'lucky_strike_red', 'id': 4}, {'name': 'marlboro_gold', 'id': 5}, {'name': 'marlboro_red', 'id': 7}, {'name': 'chesterfield_blue', 'id': 1}, {'name': 'chesterfield_red', 'id': 2}, {'name': 'lucky_strike_blue', 'id': 3}, {'name': 'lucky_strike_red', 'id': 4}, {'name': 'lucky_strike_blue', 'id': 3}, {'name': 'lucky_strike_red', 'id': 4}, {'name': 'chesterfield_red', 'id': 2}, {'name': 'lucky_strike_red', 'id': 4}, {'name': 'marlboro_gold', 'id': 5}, {'name': 'marlboro_red', 'id': 7}, {'name': 'marlboro_mentol', 'id': 6}, {'name': 'marlboro_red', 'id': 7}, {'name': 'lucky_strike_blue', 'id': 3}, {'name': 'marlboro_mentol', 'id': 6}, {'name': 'marlboro_red', 'id': 7}, {'name': 'marlboro_mentol', 'id': 6}, {'name': 'chesterfield_red', 'id': 2}, {'name': 'lucky_strike_red', 'id': 4}, {'name': 'marlboro_red', 'id': 7}, {'name': 'chesterfield_blue', 'id': 1}, {'name': 'marlboro_mentol', 'id': 6}, {'name': 'lucky_strike_red', 'id': 4}, {'name': 'marlboro_mentol', 'id': 6}, {'name': 'marlboro_mentol', 'id': 6}, {'name': 'lucky_strike_blue', 'id': 3}, {'name': 'chesterfield_red', 'id': 2}, {'name': 'lucky_strike_red', 'id': 4}, {'name': 'chesterfield_blue', 'id': 1}, {'name': 'lucky_strike_red', 'id': 4}, {'name': 'chesterfield_red', 'id': 2}, {'name': 'lucky_strike_blue', 'id': 3}, {'name': 'marlboro_mentol', 'id': 6}, {'name': 'marlboro_mentol', 'id': 6}, {'name': 'lucky_strike_red', 'id': 4}, {'name': 'marlboro_red', 'id': 7}, {'name': 'lucky_strike_blue', 'id': 3}, {'name': 'marlboro_gold', 'id': 5}, {'name': 'marlboro_gold', 'id': 5}, {'name': 'lucky_strike_blue', 'id': 3}, {'name': 'marlboro_mentol', 'id': 6}, {'name': 'chesterfield_blue', 'id': 1}, {'name': 'marlboro_gold', 'id': 5}, {'name': 'chesterfield_red', 'id': 2}, {'name': 'marlboro_mentol', 'id': 6}, {'name': 'marlboro_mentol', 'id': 6}, {'name': 'marlboro_gold', 'id': 5}, {'name': 'marlboro_red', 'id': 7}, {'name': 'marlboro_mentol', 'id': 6}, {'name': 'marlboro_gold', 'id': 5}, {'name': 'marlboro_gold', 'id': 5}, {'name': 'chesterfield_red', 'id': 2}, {'name': 'marlboro_gold', 'id': 5}, {'name': 'lucky_strike_red', 'id': 4}, {'name': 'chesterfield_red', 'id': 2}, {'name': 'lucky_strike_blue', 'id': 3}, {'name': 'chesterfield_blue', 'id': 1}, {'name': 'chesterfield_blue', 'id': 1}, {'name': 'marlboro_mentol', 'id': 6}, {'name': 'marlboro_gold', 'id': 5}, {'name': 'lucky_strike_red', 'id': 4}, {'name': 'chesterfield_blue', 'id': 1}, {'name': 'marlboro_red', 'id': 7}, {'name': 'lucky_strike_red', 'id': 4}, {'name': 'lucky_strike_blue', 'id': 3}, {'name': 'marlboro_gold', 'id': 5}, {'name': 'chesterfield_blue', 'id': 1}, {'name': 'marlboro_gold', 'id': 5}, {'name': 'marlboro_red', 'id': 7}, {'name': 'marlboro_mentol', 'id': 6}, {'name': 'lucky_strike_blue', 'id': 3}, {'name': 'lucky_strike_red', 'id': 4}, {'name': 'lucky_strike_blue', 'id': 3}, {'name': 'lucky_strike_blue', 'id': 3}, {'name': 'marlboro_gold', 'id': 5}, {'name': 'lucky_strike_blue', 'id': 3}, {'name': 'marlboro_mentol', 'id': 6}, {'name': 'marlboro_mentol', 'id': 6}, {'name': 'lucky_strike_blue', 'id': 3}, {'name': 'marlboro_mentol', 'id': 6}, {'name': 'marlboro_mentol', 'id': 6}, {'name': 'chesterfield_red', 'id': 2}, {'name': 'lucky_strike_red', 'id': 4}, {'name': 'marlboro_gold', 'id': 5}, {'name': 'marlboro_red', 'id': 7}, {'name': 'marlboro_gold', 'id': 5}, {'name': 'marlboro_red', 'id': 7}, {'name': 'lucky_strike_blue', 'id': 3}, {'name': 'marlboro_gold', 'id': 5}, {'name': 'marlboro_mentol', 'id': 6}, {'name': 'lucky_strike_red', 'id': 4}, {'name': 'marlboro_gold', 'id': 5}, {'name': 'marlboro_mentol', 'id': 6}, {'name': 'lucky_strike_blue', 'id': 3}, {'name': 'marlboro_gold', 'id': 5}, {'name': 'chesterfield_blue', 'id': 1}, {'name': 'lucky_strike_blue', 'id': 3}, {'name': 'chesterfield_blue', 'id': 1}, {'name': 'lucky_strike_red', 'id': 4}, {'name': 'chesterfield_blue', 'id': 1}, {'name': 'lucky_strike_blue', 'id': 3}, {'name': 'lucky_strike_blue', 'id': 3}, {'name': 'lucky_strike_blue', 'id': 3}, {'name': 'lucky_strike_blue', 'id': 3}, {'name': 'lucky_strike_red', 'id': 4}, {'name': 'marlboro_gold', 'id': 5}, {'name': 'chesterfield_red', 'id': 2}, {'name': 'marlboro_red', 'id': 7}, {'name': 'lucky_strike_red', 'id': 4}, {'name': 'chesterfield_red', 'id': 2}, {'name': 'chesterfield_red', 'id': 2}, {'name': 'lucky_strike_blue', 'id': 3}, {'name': 'marlboro_gold', 'id': 5}, {'name': 'lucky_strike_blue', 'id': 3}, {'name': 'marlboro_red', 'id': 7}, {'name': 'chesterfield_blue', 'id': 1}, {'name': 'marlboro_red', 'id': 7}, {'name': 'lucky_strike_blue', 'id': 3}, {'name': 'marlboro_gold', 'id': 5}, {'name': 'marlboro_mentol', 'id': 6}, {'name': 'chesterfield_red', 'id': 2}, {'name': 'chesterfield_blue', 'id': 1}, {'name': 'lucky_strike_red', 'id': 4}, {'name': 'chesterfield_blue', 'id': 1}, {'name': 'marlboro_red', 'id': 7}, {'name': 'marlboro_red', 'id': 7}, {'name': 'chesterfield_red', 'id': 2}, {'name': 'marlboro_red', 'id': 7}, {'name': 'chesterfield_red', 'id': 2}, {'name': 'marlboro_mentol', 'id': 6}, {'name': 'marlboro_mentol', 'id': 6}, {'name': 'marlboro_red', 'id': 7}, {'name': 'lucky_strike_red', 'id': 4}, {'name': 'chesterfield_blue', 'id': 1}, {'name': 'marlboro_red', 'id': 7}, {'name': 'lucky_strike_red', 'id': 4}, {'name': 'lucky_strike_red', 'id': 4}, {'name': 'marlboro_red', 'id': 7}, {'name': 'lucky_strike_red', 'id': 4}, {'name': 'chesterfield_blue', 'id': 1}, {'name': 'chesterfield_blue', 'id': 1}, {'name': 'marlboro_mentol', 'id': 6}, {'name': 'lucky_strike_blue', 'id': 3}, {'name': 'lucky_strike_red', 'id': 4}, {'name': 'lucky_strike_red', 'id': 4}, {'name': 'marlboro_mentol', 'id': 6}, {'name': 'lucky_strike_blue', 'id': 3}, {'name': 'chesterfield_blue', 'id': 1}, {'name': 'chesterfield_red', 'id': 2}, {'name': 'chesterfield_red', 'id': 2}, {'name': 'chesterfield_blue', 'id': 1}, {'name': 'chesterfield_blue', 'id': 1}, {'name': 'lucky_strike_red', 'id': 4}, {'name': 'chesterfield_blue', 'id': 1}, {'name': 'chesterfield_blue', 'id': 1}, {'name': 'chesterfield_red', 'id': 2}, {'name': 'marlboro_mentol', 'id': 6}, {'name': 'lucky_strike_red', 'id': 4}, {'name': 'chesterfield_red', 'id': 2}, {'name': 'chesterfield_blue', 'id': 1}, {'name': 'chesterfield_red', 'id': 2}, {'name': 'lucky_strike_blue', 'id': 3}, {'name': 'lucky_strike_blue', 'id': 3}, {'name': 'chesterfield_red', 'id': 2}, {'name': 'marlboro_gold', 'id': 5}, {'name': 'marlboro_red', 'id': 7}, {'name': 'marlboro_red', 'id': 7}, {'name': 'marlboro_gold', 'id': 5}, {'name': 'marlboro_mentol', 'id': 6}, {'name': 'marlboro_gold', 'id': 5}, {'name': 'chesterfield_red', 'id': 2}, {'name': 'marlboro_mentol', 'id': 6}, {'name': 'marlboro_mentol', 'id': 6}, {'name': 'marlboro_red', 'id': 7}, {'name': 'chesterfield_red', 'id': 2}, {'name': 'chesterfield_blue', 'id': 1}, {'name': 'marlboro_gold', 'id': 5}, {'name': 'chesterfield_red', 'id': 2}, {'name': 'marlboro_gold', 'id': 5}, {'name': 'lucky_strike_blue', 'id': 3}, {'name': 'marlboro_red', 'id': 7}, {'name': 'chesterfield_red', 'id': 2}, {'name': 'marlboro_red', 'id': 7}, {'name': 'lucky_strike_blue', 'id': 3}, {'name': 'marlboro_mentol', 'id': 6}, {'name': 'lucky_strike_blue', 'id': 3}, {'name': 'marlboro_mentol', 'id': 6}, {'name': 'chesterfield_blue', 'id': 1}, {'name': 'chesterfield_red', 'id': 2}, {'name': 'marlboro_mentol', 'id': 6}, {'name': 'marlboro_mentol', 'id': 6}, {'name': 'marlboro_gold', 'id': 5}, {'name': 'marlboro_gold', 'id': 5}, {'name': 'chesterfield_red', 'id': 2}, {'name': 'marlboro_gold', 'id': 5}, {'name': 'marlboro_red', 'id': 7}, {'name': 'lucky_strike_red', 'id': 4}, {'name': 'marlboro_red', 'id': 7}, {'name': 'chesterfield_red', 'id': 2}, {'name': 'marlboro_red', 'id': 7}, {'name': 'marlboro_mentol', 'id': 6}, {'name': 'marlboro_red', 'id': 7}, {'name': 'lucky_strike_blue', 'id': 3}, {'name': 'lucky_strike_red', 'id': 4}, {'name': 'lucky_strike_red', 'id': 4}, {'name': 'marlboro_mentol', 'id': 6}, {'name': 'marlboro_mentol', 'id': 6}, {'name': 'lucky_strike_red', 'id': 4}]
(1, 300, 4)
[ 300.]
</code></pre>

<p>Thanks in advance for any information!</p>

<p><strong>UPD:</strong></p>

<p>Thousand thanks for everyone who helped with this question.
Following line of code is exactly what I needed, it gives me list with objects that were found so I can do other operations on them.</p>

<pre><code>print [category_index.get(value) for index,value in enumerate(classes[0]) if scores[0,index] &gt; 0.5]
</code></pre>
",
What is the difference between 'SAME' and 'VALID' padding in tf.nn.max_pool of tensorflow?,"<p>What is the difference between 'SAME' and 'VALID' padding in <code>tf.nn.max_pool</code> of <code>tensorflow</code>?</p>

<p>In my opinion, 'VALID' means there will be no zero padding outside the edges when we do max pool. </p>

<p>According to <a href=""https://arxiv.org/pdf/1603.07285v1.pdf"">A guide to convolution arithmetic for deep learning</a>, it says that there will be no padding in pool operator, i.e. just use 'VALID' of <code>tensorflow</code>.
But what is 'SAME' padding of max pool in <code>tensorflow</code>?</p>
",
How to use tensorflow to implement deconvolution?,"<p>I want to take use of tensorflow to implement fully convolutional network. There is a function</p>

<pre><code>tf.nn.conv2d_transpose(value, filter, output_shape, strides, padding, name),
</code></pre>

<p>which could be used to take bilinear upsampling. However, I am confused as to how to use it? The input is an image with a single channel, and the output is also an image with a single channel, whose size is two times the one of the input. 
I tried to use the function as follows, but got an <code>IndexError: list index out of range</code>: </p>

<pre><code>  with tf.name_scope('deconv') as scope:
    deconv = tf.nn.conv2d_transpose(conv6, [3, 3, 1, 1], 
        [1, 26, 20, 1], 2, padding='SAME', name=None)
</code></pre>
",
Saving tf.trainable_variables() using convert_variables_to_constants,"<p>I have a Keras model that I would like to convert to a Tensorflow protobuf (e.g. <code>saved_model.pb</code>).</p>

<p>This model comes from transfer learning on the vgg-19 network in which and the head was cut-off and trained with fully-connected+softmax layers while the rest of the vgg-19 network was frozen</p>

<p>I can load the model in Keras, and then use <code>keras.backend.get_session()</code> to run the model in tensorflow, generating the correct predictions:</p>

<pre><code>frame = preprocess(cv2.imread(""path/to/img.jpg"")
keras_model = keras.models.load_model(""path/to/keras/model.h5"")

keras_prediction = keras_model.predict(frame)

print(keras_prediction)

with keras.backend.get_session() as sess:

    tvars = tf.trainable_variables()

    output = sess.graph.get_tensor_by_name('Softmax:0')
    input_tensor = sess.graph.get_tensor_by_name('input_1:0')

    tf_prediction = sess.run(output, {input_tensor: frame})
    print(tf_prediction) # this matches keras_prediction exactly
</code></pre>

<p>If I don't include the line <code>tvars = tf.trainable_variables()</code>, then the <code>tf_prediction</code> variable is completely wrong and doesn't match the output from <code>keras_prediction</code> at all. In fact all the values in the output (single array with 4 probability values) are exactly the same (~0.25, all adding to 1). This made me suspect that weights for the head are just initialized to 0 if <code>tf.trainable_variables()</code> is not called first, which was confirmed after inspecting the model variables. In any case, calling <code>tf.trainable_variables()</code> causes the tensorflow prediction to be correct.</p>

<p>The problem is that when I try to save this model, the variables from <code>tf.trainable_variables()</code> don't actually get saved to the <code>.pb</code> file:</p>

<pre><code>with keras.backend.get_session() as sess:
    tvars = tf.trainable_variables()

    constant_graph = graph_util.convert_variables_to_constants(sess, sess.graph.as_graph_def(), ['Softmax'])
    graph_io.write_graph(constant_graph, './', 'saved_model.pb', as_text=False)
</code></pre>

<p>What I am asking is, how can I save a Keras model as a Tensorflow protobuf with the <code>tf.training_variables()</code> intact? </p>

<p>Thanks so much!</p>
",
Convert between NHWC and NCHW in TensorFlow,"<p>What is the best way to convert a tensor from NHWC format to NCHW format, and vice versa?</p>

<p>Is there an op specifically that does this, or will I need to use some combination of the split/concat type operations?</p>
",
What is tensorflow.compat.as_str()?,"<p>In the <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/udacity/5_word2vec.ipynb"" rel=""noreferrer"">Google/Udemy Tensorflow tutorial</a> there is the following code:</p>

<pre><code>import tensorflow as tf
...
def read_data(filename):
    """"""Extract the first file enclosed in a zip file as a list of words""""""
    with zipfile.ZipFile(filename) as f:
    data = tf.compat.as_str(f.read(f.namelist()[0])).split()
return data
</code></pre>

<p>This executes fine but I cannot find <code>compat.as_str</code> in the Tensorflow documentation or anywhere else.</p>

<p>Q1: What does <code>compat.as_str</code> do?</p>

<p>Q2: Is this tensorflow <code>compat</code> library documented somewhere?</p>

<p>Q3: This is a call to the tensorflow library, so how and why does it work in normal python code, rather than inside a tensorflow graph? I.e. I thought tensorflow library calls had to be inside a tensorflow graph defintion block:</p>

<pre><code>graph = tf.Graph()
with graph.as_default()
    ... tensorflow function calls here ...
</code></pre>

<p>I am running python 2.7. </p>
",
"Tensorflow, multi label accuracy calculation","<p>I am working on a multi label problem and i am trying to determine the accuracy of my model.</p>

<p>My model:</p>

<pre class=""lang-py prettyprint-override""><code>NUM_CLASSES = 361

x = tf.placeholder(tf.float32, [None, IMAGE_PIXELS])
y_ = tf.placeholder(tf.float32, [None, NUM_CLASSES])

# create the network
pred = conv_net( x )

# loss
cost = tf.reduce_mean( tf.nn.sigmoid_cross_entropy_with_logits( pred, y_) )

# train step
train_step   = tf.train.AdamOptimizer().minimize( cost )
</code></pre>

<p>i want to calculate the accuracy in two different ways<br>
- % of all labels that are predicted correctly
- % of images where ALL labels are predicted correctly</p>

<p>unfortunately i am only able to calculate the % of all labels that are predicted correctly. </p>

<p>I thought this code would calculate % of images where ALL labels are predicted correctly</p>

<pre class=""lang-py prettyprint-override""><code>correct_prediction = tf.equal( tf.round( pred ), tf.round( y_ ) )

accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
</code></pre>

<p>and this code % of all labels that are predicted correctly</p>

<pre class=""lang-py prettyprint-override""><code>pred_reshape = tf.reshape( pred, [ BATCH_SIZE * NUM_CLASSES, 1 ] )
y_reshape = tf.reshape( y_, [ BATCH_SIZE * NUM_CLASSES, 1 ] )

correct_prediction_all = tf.equal( tf.round( pred_reshape ), tf.round( y_reshape ) )

accuracy_all = tf.reduce_mean( tf.cast(correct_prediction_all, tf.float32 ) )
</code></pre>

<p>somehow the coherency of the labels belonging to one image is lost and i am not sure why.</p>
",
Tensorflow - matmul of input matrix with batch data,"<p>I have some data represented by <code>input_x</code>. It is a tensor of unknown size (should be inputted by batch) and each item there is of size <code>n</code>. <code>input_x</code> undergoes <code>tf.nn.embedding_lookout</code>, so that <code>embed</code> now has dimensions <code>[?, n, m]</code> where <code>m</code> is the embedding size and <code>?</code> refers to the unknown batch size. </p>

<p>This is described here:</p>

<pre><code>input_x = tf.placeholder(tf.int32, [None, n], name=""input_x"") 
embed = tf.nn.embedding_lookup(W, input_x)
</code></pre>

<p>I'm now trying to multiply each sample in my input data (which is now an embedding matrix) by a matrix variable, <code>U</code>, and I can't seem to get how to do that.</p>

<p>I first tried using <code>tf.matmul</code> but it gives an error due to mismatch in shapes. I then tried the following, by expanding the dimension of <code>U</code> and applying <code>batch_matmul</code> (I also tried the function from <code>tf.nn.math_ops.</code>, the result was the same):</p>

<pre><code>U = tf.Variable( ... )    
U1 = tf.expand_dims(U,0)
h=tf.batch_matmul(embed, U1)
</code></pre>

<p>This passes the initial compilation, but then when actual data is applied, I get the following error:</p>

<p><code>In[0].dim(0) and In[1].dim(0) must be the same: [64,58,128] vs [1,128,128]</code></p>

<p>I also know why this is happening - I replicated the dimension of <code>U</code> and it is now <code>1</code>, but the minibatch size, <code>64</code>, doesn't fit. </p>

<p>How can I do that matrix multiplication on my tensor-matrix input correctly (for unknown batch size)?</p>
",
TensorFlow: Remember LSTM state for next batch (stateful LSTM),"<p>Given a trained LSTM model I want to perform inference for single timesteps, i.e. <code>seq_length = 1</code> in the example below. After each timestep the internal LSTM (memory and hidden) states need to be remembered for the next 'batch'. For the very beginning of the inference the internal LSTM states <code>init_c, init_h</code> are computed given the input. These are then stored in a <code>LSTMStateTuple</code> object which is passed to the LSTM. During training this state is updated every timestep. However for inference I want the <code>state</code> to be saved in between batches, i.e. the initial states only need to be computed at the very beginning and after that the LSTM states should be saved after each 'batch' (n=1). </p>

<p>I found this related StackOverflow question: <a href=""https://stackoverflow.com/questions/37969065/tensorflow-best-way-to-save-state-in-rnns"">Tensorflow, best way to save state in RNNs?</a>. However this only works if <code>state_is_tuple=False</code>, but this behavior is soon to be deprecated by TensorFlow (see <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn_cell.py#L265"" rel=""noreferrer"">rnn_cell.py</a>). Keras seems to have a nice wrapper to make <strong>stateful</strong> LSTMs possible but I don't know the best way to achieve this in TensorFlow. This issue on the TensorFlow GitHub is also related to my question: <a href=""https://github.com/tensorflow/tensorflow/issues/2838"" rel=""noreferrer"">https://github.com/tensorflow/tensorflow/issues/2838</a></p>

<p>Anyone good suggestions for building a stateful LSTM model? </p>

<pre><code>inputs  = tf.placeholder(tf.float32, shape=[None, seq_length, 84, 84], name=""inputs"")
targets = tf.placeholder(tf.float32, shape=[None, seq_length], name=""targets"")

num_lstm_layers = 2

with tf.variable_scope(""LSTM"") as scope:

    lstm_cell  = tf.nn.rnn_cell.LSTMCell(512, initializer=initializer, state_is_tuple=True)
    self.lstm  = tf.nn.rnn_cell.MultiRNNCell([lstm_cell] * num_lstm_layers, state_is_tuple=True)

    init_c = # compute initial LSTM memory state using contents in placeholder 'inputs'
    init_h = # compute initial LSTM hidden state using contents in placeholder 'inputs'
    self.state = [tf.nn.rnn_cell.LSTMStateTuple(init_c, init_h)] * num_lstm_layers

    outputs = []

    for step in range(seq_length):

        if step != 0:
            scope.reuse_variables()

        # CNN features, as input for LSTM
        x_t = # ... 

        # LSTM step through time
        output, self.state = self.lstm(x_t, self.state)
        outputs.append(output)
</code></pre>
",
Implementing contrastive loss and triplet loss in Tensorflow,"<p>I started to play with TensorFlow two days ago and I'm wondering if there is the triplet and the contrastive losses implemented.</p>

<p>I've been looking at <a href=""https://www.tensorflow.org/versions/r0.9/api_docs/python/nn.html#losses"" rel=""noreferrer"">the documentation</a>, but I haven't found any example or description about these things.</p>
",
TensorFlow Object Detection API Weird Behavior,"<p>I was playing with TensorFlow's brand new <a href=""https://github.com/tensorflow/models/tree/master/research/object_detection"" rel=""nofollow noreferrer"">Object Detection API</a> and decided to train it on some other publicly available datasets.</p>

<p>I happened to stumble upon <a href=""https://github.com/gulvarol/grocerydataset"" rel=""nofollow noreferrer"">this</a> grocery dataset which consists of images of various brands of cigarette boxes on the supermarket shelf along with a text file which lists out the bounding boxes of each cigarette box in each image. 10 major brands have been labeled in the dataset and all other brands fall into the 11th ""miscellaneous"" category.</p>

<p>I followed their <a href=""https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_pets.md"" rel=""nofollow noreferrer"">tutorial</a> and managed to train the model on this dataset. Due to limitations on processing power, I used only a third of the dataset and performed a 70:30 split for training and testing data. I used the faster_rcnn_resnet101 model. All parameters in my config file are the same as the default parameters provided by TF.</p>

<p>After 16491 global steps, I tested the model on some images but I am not too happy with the results -</p>

<p><a href=""https://i.stack.imgur.com/vJSm1.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vJSm1.png"" alt=""""></a>
Failed to detect the Camels in top-shelf whereas it detects the product in other images</p>

<p><a href=""https://i.stack.imgur.com/cc6hV.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/cc6hV.jpg"" alt=""""></a>
Why does it fail to detect the Marlboros in the top row?</p>

<p><a href=""https://i.stack.imgur.com/bsiCP.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/bsiCP.png"" alt=""""></a>
Another issue I had is that the model never detected any other label except for label 1</p>

<p><a href=""https://i.stack.imgur.com/AKKsx.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/AKKsx.png"" alt=""""></a></p>

<p>Doesn't detected a crop instance of the product from the training data</p>

<p><a href=""https://i.stack.imgur.com/oi4Ax.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/oi4Ax.png"" alt=""""></a></p>

<p>It detects cigarette boxes with 99% confidence even in negative images!</p>

<p>Can somebody help me with what is going wrong? What can I do to improve the accuracy? And why does it detect all products to belong in category 1 even though I have mentioned that there are 11 classes in total?</p>

<p><strong>Edit</strong> Added my label map:</p>

<pre><code>item {
  id: 1
  name: '1'
}

item {
  id: 2
  name: '2'
}

item {
  id: 3
  name: '3'
}

item {
  id: 4
  name: '4'
}

item {
  id: 5
  name: '5'
}

item {
  id: 6
  name: '6'
}

item {
  id: 7
  name: '7'
}

item {
  id: 8
  name: '8'
}

item {
  id: 9
  name: '9'
}

item {
  id: 10
  name: '10'
}

item {
  id: 11
  name: '11'
}
</code></pre>
",
Getting the current learning rate from a tf.train.AdamOptimizer,"<p>I'd like print out the learning rate for each training step of my nn.</p>

<p>I know that Adam has an adaptive learning rate, but is there a way i can see this (for visualization in tensorboard)</p>
",
TensorFlow - Importing data from a TensorBoard TFEvent file?,"<p>I've run several training sessions with different graphs in TensorFlow. The summaries I set up show interesting results in the training and validation. Now, I'd like to take the data I've saved in the summary logs and perform some statistical analysis and in general plot and look at the summary data in different ways. Is there any existing way to easily access this data?</p>

<p>More specifically, is there any built in way to read a TFEvent record back into Python?</p>

<p>If there is no simple way to do this, <a href=""https://www.tensorflow.org/versions/r0.8/how_tos/tool_developers/index.html#protocol-buffers"" rel=""noreferrer"">TensorFlow states that all its file formats are protobuf files</a>. From my understanding of protobufs (which is limited), I think I'd be able to extract this data if I have the TFEvent protocol specification. Is there an easy way to get ahold of this? Thank you much.</p>
",
TensorFlow: what's the difference between sparse_softmax_cross_entropy_with_logits and softmax_cross_entropy_with_logits?,"<p>I recently came across <a href=""https://www.tensorflow.org/api_docs/python/tf/nn/sparse_softmax_cross_entropy_with_logits"" rel=""noreferrer"">tf.nn.sparse_softmax_cross_entropy_with_logits</a> and I can not figure out what the difference is compared to <a href=""https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits"" rel=""noreferrer"">tf.nn.softmax_cross_entropy_with_logits</a>.</p>

<p>Is the only difference that training vectors <code>y</code> have to be <a href=""http://www.cs.toronto.edu/~guerzhoy/321/lec/W04/onehot.pdf"" rel=""noreferrer"">one-hot encoded</a> when using <code>sparse_softmax_cross_entropy_with_logits</code>?</p>

<p>Reading the API I were unable to find any other difference compared to <code>softmax_cross_entropy_with_logits</code>...but why do we need the extra function then?</p>

<p>Shouldn't <code>softmax_cross_entropy_with_logits</code> produce the same results as <code>sparse_softmax_cross_entropy_with_logits</code>, if it is supplied with one-hot encoded training data/vectors?  </p>
",
TensorFlow: Dst tensor is not initialized,"

<p>The <code>MNIST For ML Beginners</code> tutorial is giving me an error when I run <code>print(sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))</code>. Everything else runs fine. </p>

<p>Error and trace:</p>

<pre class=""lang-py prettyprint-override""><code>InternalErrorTraceback (most recent call last)
&lt;ipython-input-16-219711f7d235&gt; in &lt;module&gt;()
----&gt; 1 print(sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))

/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc in run(self, fetches, feed_dict, options, run_metadata)
    338     try:
    339       result = self._run(None, fetches, feed_dict, options_ptr,
--&gt; 340                          run_metadata_ptr)
    341       if run_metadata:
    342         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)

/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc in _run(self, handle, fetches, feed_dict, options, run_metadata)
    562     try:
    563       results = self._do_run(handle, target_list, unique_fetches,
--&gt; 564                              feed_dict_string, options, run_metadata)
    565     finally:
    566       # The movers are no longer used. Delete them.

/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)
    635     if handle is None:
    636       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,
--&gt; 637                            target_list, options, run_metadata)
    638     else:
    639       return self._do_call(_prun_fn, self._session, handle, feed_dict,

/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc in _do_call(self, fn, *args)
    657       # pylint: disable=protected-access
    658       raise errors._make_specific_exception(node_def, op, error_message,
--&gt; 659                                             e.code)
    660       # pylint: enable=protected-access
    661 

InternalError: Dst tensor is not initialized.
     [[Node: _recv_Placeholder_3_0/_1007 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=1, tensor_name=""edge_312__recv_Placeholder_3_0"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/gpu:0""]()]]
     [[Node: Mean_1/_1011 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_319_Mean_1"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]
</code></pre>

<p>I just switched to a more recent version of CUDA, so maybe this has something to do with that? Seems like this error is about copying a tensor to the GPU.</p>

<p>Stack: EC2 g2.8xlarge machine, Ubuntu 14.04 </p>

<p>UPDATE:</p>

<p><code>print(sess.run(accuracy, feed_dict={x: batch_xs, y_: batch_ys}))</code> runs fine. This leads me to suspect that the issue is that I'm trying to transfer a huge tensor to the GPU and it can't take it. Small tensors like a minibatch work just fine.</p>

<p>UPDATE 2:</p>

<p>I've figured out exactly how big the tensors have to be to cause this issue:</p>

<pre class=""lang-py prettyprint-override""><code>batch_size = 7509 #Works.
print(sess.run(accuracy, feed_dict={x: mnist.test.images[0:batch_size], y_: mnist.test.labels[0:batch_size]}))

batch_size = 7510 #Doesn't work. Gets the Dst error.
print(sess.run(accuracy, feed_dict={x: mnist.test.images[0:batch_size], y_: mnist.test.labels[0:batch_size]}))
</code></pre>
",
Is it possible to make a trainable variable not trainable?,"<p>I created a <em><strong>trainable</strong> variable</em> in a scope. Later, I entered the same scope, set the scope to <code>reuse_variables</code>, and used <code>get_variable</code> to retrieve the same variable. However, I cannot set the variable's trainable property to <code>False</code>. My <code>get_variable</code> line is like:</p>

<pre><code>weight_var = tf.get_variable('weights', trainable = False)
</code></pre>

<p>But the variable <code>'weights'</code> is still in the output of <code>tf.trainable_variables</code>.</p>

<p>Can I set a shared variable's <code>trainable</code> flag to <code>False</code> by using <code>get_variable</code>?</p>

<p>The reason I want to do this is that I'm trying to reuse the low-level filters pre-trained from VGG net in my model, and I want to build the graph like before, retrieve the weights variable, and assign VGG filter values to the weight variable, and then keep them fixed during the following training step. </p>
",
TensorFlow: InternalError: Blas SGEMM launch failed,"<p>When I run <code>sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})</code> I get <code>InternalError: Blas SGEMM launch failed</code>. Here is the full error and stack trace:</p>

<pre><code>InternalErrorTraceback (most recent call last)
&lt;ipython-input-9-a3261a02bdce&gt; in &lt;module&gt;()
      1 batch_xs, batch_ys = mnist.train.next_batch(100)
----&gt; 2 sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})

/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc in run(self, fetches, feed_dict, options, run_metadata)
    338     try:
    339       result = self._run(None, fetches, feed_dict, options_ptr,
--&gt; 340                          run_metadata_ptr)
    341       if run_metadata:
    342         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)

/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc in _run(self, handle, fetches, feed_dict, options, run_metadata)
    562     try:
    563       results = self._do_run(handle, target_list, unique_fetches,
--&gt; 564                              feed_dict_string, options, run_metadata)
    565     finally:
    566       # The movers are no longer used. Delete them.

/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)
    635     if handle is None:
    636       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,
--&gt; 637                            target_list, options, run_metadata)
    638     else:
    639       return self._do_call(_prun_fn, self._session, handle, feed_dict,

/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc in _do_call(self, fn, *args)
    657       # pylint: disable=protected-access
    658       raise errors._make_specific_exception(node_def, op, error_message,
--&gt; 659                                             e.code)
    660       # pylint: enable=protected-access
    661 

InternalError: Blas SGEMM launch failed : a.shape=(100, 784), b.shape=(784, 10), m=100, n=10, k=784
     [[Node: MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=""/job:localhost/replica:0/task:0/gpu:0""](_recv_Placeholder_0/_4, Variable/read)]]
Caused by op u'MatMul', defined at:
  File ""/usr/lib/python2.7/runpy.py"", line 162, in _run_module_as_main
    ""__main__"", fname, loader, pkg_name)
  File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code
    exec code in run_globals
  File ""/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py"", line 3, in &lt;module&gt;
    app.launch_new_instance()
  File ""/usr/local/lib/python2.7/dist-packages/traitlets/config/application.py"", line 596, in launch_instance
    app.start()
  File ""/usr/local/lib/python2.7/dist-packages/ipykernel/kernelapp.py"", line 442, in start
    ioloop.IOLoop.instance().start()
  File ""/usr/local/lib/python2.7/dist-packages/zmq/eventloop/ioloop.py"", line 162, in start
    super(ZMQIOLoop, self).start()
  File ""/usr/local/lib/python2.7/dist-packages/tornado/ioloop.py"", line 883, in start
    handler_func(fd_obj, events)
  File ""/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py"", line 275, in null_wrapper
    return fn(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py"", line 440, in _handle_events
    self._handle_recv()
  File ""/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py"", line 472, in _handle_recv
    self._run_callback(callback, msg)
  File ""/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py"", line 414, in _run_callback
    callback(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py"", line 275, in null_wrapper
    return fn(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py"", line 276, in dispatcher
    return self.dispatch_shell(stream, msg)
  File ""/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py"", line 228, in dispatch_shell
    handler(stream, idents, msg)
  File ""/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py"", line 391, in execute_request
    user_expressions, allow_stdin)
  File ""/usr/local/lib/python2.7/dist-packages/ipykernel/ipkernel.py"", line 199, in do_execute
    shell.run_cell(code, store_history=store_history, silent=silent)
  File ""/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py"", line 2723, in run_cell
    interactivity=interactivity, compiler=compiler, result=result)
  File ""/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py"", line 2825, in run_ast_nodes
    if self.run_code(code, result):
  File ""/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py"", line 2885, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""&lt;ipython-input-4-d7414c4b6213&gt;"", line 4, in &lt;module&gt;
    y = tf.nn.softmax(tf.matmul(x, W) + b)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.py"", line 1036, in matmul
    name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_math_ops.py"", line 911, in _mat_mul
    transpose_b=transpose_b, name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/op_def_library.py"", line 655, in apply_op
    op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 2154, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1154, in __init__
    self._traceback = _extract_stack()
</code></pre>

<p>Stack: EC2 g2.8xlarge machine, Ubuntu 14.04</p>
",
TensorFlow: training on my own image,"<p>I am new to TensorFlow. I am looking for the help on the image recognition where I can <strong>train my own image</strong> dataset.</p>

<p>Is there any example for training the new dataset?</p>
",
How to tell if tensorflow is using gpu acceleration from inside python shell?,"<p>I have installed tensorflow in my ubuntu 16.04 using the second answer <a href=""https://devtalk.nvidia.com/default/topic/936429/-solved-tensorflow-with-gpu-in-anaconda-env-ubuntu-16-04-cuda-7-5-cudnn-/"">here</a> with ubuntu's builtin apt cuda installation.</p>

<p>Now my question is how can I test if tensorflow is really using gpu? I have a gtx 960m gpu. When I <code>import tensorflow</code> this is the outp</p>

<pre><code>I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally
</code></pre>

<p>Is this output enough to check if tensorflow is using gpu ? </p>
",
Tensorflow sigmoid and cross entropy vs sigmoid_cross_entropy_with_logits,"<p>When trying to get cross entropy with sigmoid activation function, there is difference between </p>

<ol>
<li><code>loss1 = -tf.reduce_sum(p*tf.log(q), 1)</code></li>
<li><code>loss2 = tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(labels=p, logits=logit_q),1)</code></li>
</ol>

<p>But they are same when with softmax activation function.</p>

<p>Following is the sample code:</p>



<pre class=""lang-python prettyprint-override""><code>import tensorflow as tf

sess2 = tf.InteractiveSession()
p = tf.placeholder(tf.float32, shape=[None, 5])
logit_q = tf.placeholder(tf.float32, shape=[None, 5])
q = tf.nn.sigmoid(logit_q)
sess.run(tf.global_variables_initializer())

feed_dict = {p: [[0, 0, 0, 1, 0], [1,0,0,0,0]], logit_q: [[0.2, 0.2, 0.2, 0.2, 0.2], [0.3, 0.3, 0.2, 0.1, 0.1]]}
loss1 = -tf.reduce_sum(p*tf.log(q),1).eval(feed_dict)
loss2 = tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(labels=p, logits=logit_q),1).eval(feed_dict)

print(p.eval(feed_dict), ""\n"", q.eval(feed_dict))
print(""\n"",loss1, ""\n"", loss2)
</code></pre>
",
"Building a mutlivariate, multi-task LSTM with Keras","<p><strong>Preamble</strong></p>

<p>I am currently working on a Machine Learning problem where we are tasked with using past data on product sales in order to predict sales volumes going forward (so that shops can better plan their stocks). We essentially have time series data, where for each and every product we know how many units were sold on which days. We also have information like what the weather was like, whether there was a public holiday, if any of the products were on sales etc. </p>

<p>We've been able to model this with some success using an MLP with dense layers, and just using a sliding window approach to include sales volumes from the surrounding days. However, we believe we'll be able to get much better results with a time-series approach such as an LSTM.</p>

<p><strong>Data</strong></p>

<p>The data we have essentially is as follows:</p>

<p><a href=""https://i.stack.imgur.com/fOr4z.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/fOr4z.jpg"" alt=""enter image description here""></a></p>

<p>(<strong>EDIT:</strong> for clarity the ""Time"" column in the picture above is not correct. We have inputs once per day, not once per month. But otherwise the structure is the same!)</p>

<p>So the X data is of shape:</p>

<pre><code>(numProducts, numTimesteps, numFeatures) = (50 products, 1096 days, 90 features)
</code></pre>

<p>And the Y data is of shape:</p>

<pre><code>(numProducts, numTimesteps, numTargets) =  (50 products, 1096 days, 3 binary targets)
</code></pre>

<p><a href=""https://i.stack.imgur.com/uaSl5.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/uaSl5.jpg"" alt=""enter image description here""></a></p>

<p>So we have data for three years (2014, 2015, 2016) and want to train on this in order to make predictions for 2017. (That's of course not 100% true, since we actually have data up to Oct 2017, but let's just ignore that for now)</p>

<p><strong>Problem</strong></p>

<p>I would like to build an LSTM in Keras that allows me to make these predictions. There are a few places where I am getting stuck though. So I have six concrete questions (I know one is supposed to try to limit a Stackoverflow post to one question, but these are all intertwined).</p>

<p>Firstly, <strong>how would I slice up my data for the batches</strong>? Since I have three full years, does it make sense to simply push through three batches, each time of size one year? Or does it make more sense to make smaller batches (say 30 days) and also to using sliding windows? I.e. instead of 36 batches of 30 days each, I use 36 * 6 batches of 30 days each, each time sliding with 5 days? Or is this not really the way LSTMs should be used? (Note that there is quite a bit of seasonality in the data, to I need to catch that kind of long-term trend as well).</p>

<p>Secondly, <strong>does it make sense to use</strong> <code>return_sequences=True</code> here? In other words, I keep my Y data as is <code>(50, 1096, 3)</code> so that (as far as I've understood it) there is a prediction at every time step for which a loss can be calculated against the target data? Or would I be better off with <code>return_sequences=False</code>, so that only the final value of each batch is used to evaluate the loss (i.e. if using yearly batches, then in 2016 for product 1, we evaluate against the Dec 2016 value of <code>(1,1,1)</code>).</p>

<p>Thirdly <strong>how should I deal with the 50 different products?</strong> They are different, but still strongly correlated and we've seen with other approaches (for example an MLP with simple time-windows) that the results are better when all products are considered in the same model. Some ideas that are currently on the table are:</p>

<ul>
<li>change the target variable to be not just 3 variables, but 3 * 50 = 150; i.e. for each product there are three targets, all of which are trained simultaneously. </li>
<li>split up the results after the LSTM layer into 50 dense networks, which take as input the ouputs from the LSTM, plus some features that are specific to each product - i.e. we get a multi-task network with 50 loss functions, which we then optimise together. Would that be crazy?</li>
<li>consider a product as a single observation, and include product specific features already at the LSTM layer. Use just this one layer followed by an ouput layer of size 3 (for the three targets). Push through each product in a separate batch.</li>
</ul>

<p>Fourthly, <strong>how do I deal with validation data</strong>? Normally I would just keep out a randomly selected sample to validate against, but here we need to keep the time ordering in place. So I guess the best is to just keep a few months aside?</p>

<p>Fifthly, and this is the part that is probably the most unclear to me - <strong>how can I use the actual results to perform predictions</strong>? Let's  say I used <code>return_sequences=False</code> and I trained on all three years in three batches (each time up to Nov) with the goal of training the model to predict the next value (Dec 2014, Dec 2015, Dec 2016). If I want to use these results in 2017, how does this actually work? If I understood it correctly, the only thing I can do in this instance is to then feed the model all the data points for Jan to Nov 2017 and it will give me back a prediction for Dec 2017. Is that correct? However, if I were to use <code>return_sequences=True</code>, then trained on all data up to Dec 2016, would I then be able to get a prediction for Jan 2017 just by giving the model the features observed at Jan 2017? Or do I need to also give it the 12 months before Jan 2017? What about Feb 2017, do I in addition need to give the value for 2017, plus a further 11 months before that? (If it sounds like I'm confused, it's because I am!)</p>

<p>Lastly, depending on what structure I should use, <strong>how do I do this in Keras</strong>? What I have in mind at the moment is something along the following lines: (though this would be for only one product, so doesn't solve having all products in the same model):</p>

<p><strong>Keras code</strong></p>

<pre><code>trainX = trainingDataReshaped #Data for Product 1, Jan 2014 to Dec 2016
trainY = trainingTargetReshaped
validX = validDataReshaped #Data for Product 1, for ??? Maybe for a few months?
validY = validTargetReshaped    

numSequences = trainX.shape[0]
numTimeSteps = trainX.shape[1]
numFeatures = trainX.shape[2]

numTargets = trainY.shape[2]

model = Sequential()
model.add(LSTM(100, input_shape=(None, numFeatures), return_sequences=True)) 
model.add(Dense(numTargets, activation=""softmax""))    

model.compile(loss=stackEntry.params[""loss""],
      optimizer=""adam"",
      metrics=['accuracy'])

history = model.fit(trainX, trainY,
            batch_size=30,
            epochs=20,
            verbose=1,
            validation_data=(validX, validY))               

predictX  = predictionDataReshaped #Data for Product 1, Jan 2017 to Dec 2017

prediction=model.predict(predictX)
</code></pre>
",
How can I make tensorflow run on a GPU with capability 2.0?,"<p>I've successfully installed tensorflow (GPU) on Linux Ubuntu 16.04 and made some small changes in order to make it work with the new Ubuntu LTS release.</p>

<p>However, I thought (who knows why) that my GPU met the minimum requirement of a compute capability greater than 3.5. That was not the case since my <a href=""http://www.geforce.com/hardware/notebook-gpus/geforce-820m"" rel=""noreferrer"">GeForce 820M</a> has just 2.1. Is there a way of making tensorflow GPU version working with my GPU?</p>

<p>I am asking this question since apparently there was no way of making tensorflow GPU version working on Ubuntu 16.04 but by searching the internet I found out that was not the case and indeed I made it almost work were it not for this unsatisfied requirement. Now I am wondering if this issue with GPU compute capability could be fixed as well.</p>
",
Tensorflow variable scope: reuse if variable exists,"<p>I want a piece of code that creates a variable within a scope if it doesn't exist, and access the variable if it already exists. I need it to be the <strong>same</strong> code since it will be called multiple times. </p>

<p>However, Tensorflow needs me to specify whether I want to create or reuse the variable, like this:</p>

<pre><code>with tf.variable_scope(""foo""): #create the first time
    v = tf.get_variable(""v"", [1])

with tf.variable_scope(""foo"", reuse=True): #reuse the second time
    v = tf.get_variable(""v"", [1])
</code></pre>

<p>How can I get it to figure out whether to create or reuse it automatically? I.e., I want the above two blocks of code to be the <strong>same</strong> and have the program run. </p>
",
Tensorflow: How to Display Custom Images in Tensorboard (e.g. Matplotlib Plots),"<p>The <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tensorboard/README.md#image-dashboard"" rel=""noreferrer"">Image Dashboard</a> section of the Tensorboard ReadMe says:</p>

<blockquote>
  <p>Since the image dashboard supports arbitrary pngs, you can use this to embed custom visualizations (e.g. matplotlib scatterplots) into TensorBoard.</p>
</blockquote>

<p>I see how a pyplot image could be written to file, read back in as a tensor, and then used with tf.image_summary() to write it to TensorBoard, but this statement from the readme suggests there is a more direct way. Is there? If so, is there any further documentation and/or examples of how to do this efficiently?  </p>
",
How to find which version of TensorFlow is installed in my system?,"<p>The title says it all. I'm using Ubuntu 16.04 Long Term Support.  </p>
",
How to get current available GPUs in tensorflow?,"<p>I have a plan to use distributed TensorFlow, and I saw TensorFlow can use GPUs for training and testing. In a cluster environment, each machine could have 0 or 1 or more GPUs, and I want to run my TensorFlow graph into GPUs on as many machines as possible.</p>

<p>I found that when running <code>tf.Session()</code> TensorFlow gives information about GPU in the log messages like below:</p>

<pre><code>I tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 
I tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:838] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0)
</code></pre>

<p>My question is how do I get information about current available GPU from TensorFlow? I can get loaded GPU information from the log, but I want to do it in a more sophisticated, programmatic way.
I also could restrict GPUs intentionally using the CUDA_VISIBLE_DEVICES environment variable, so I don't want to know a way of getting GPU information from OS kernel.</p>

<p>In short, I want a function like <code>tf.get_available_gpus()</code> that will return <code>['/gpu:0', '/gpu:1']</code> if there are two GPUs available in the machine. How can I implement this?</p>
",
the usages of ksize in tf.nn.max_pool,"<p>In the definition of <code>tf.nn.max_pool</code>, what is <code>ksize</code> used for?</p>

<pre><code>tf.nn.max_pool(value, ksize, strides, padding, data_format='NHWC', name=None)

Performs the max pooling on the input.

Args:

value: A 4-D Tensor with shape [batch, height, width, channels] and type    tf.float32.
ksize: A list of ints that has length &gt;= 4. The size of the window for each dimension of the input tensor.
</code></pre>

<p>For instance, if an <code>input value</code> is of <code>tensor :  [1, 64, 64, 3]</code> and <code>ksize=3</code>.what does that mean?</p>
",
Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2,"

<p>I am new to tensorflow. I have recently installed it (Windows CPU version) and received the following message:</p>

<blockquote>
  <p>Successfully installed tensorflow-1.4.0 tensorflow-tensorboard-0.4.0rc2</p>
</blockquote>

<p>Then when I tried to run</p>

<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf
hello = tf.constant('Hello, TensorFlow!')
sess = tf.Session()
sess.run(hello)
'Hello, TensorFlow!'
a = tf.constant(10)
b = tf.constant(32)
sess.run(a + b)
42
sess.close()
</code></pre>

<p>(which I found through <a href=""https://github.com/tensorflow/tensorflow"" rel=""noreferrer"">https://github.com/tensorflow/tensorflow</a>)</p>

<p>I received the following error:</p>

<pre class=""lang-py prettyprint-override""><code>2017-11-02 01:56:21.698935: I C:\tf_jenkins\home\workspace\rel-win\M\windows\PY\36\tensorflow\core\platform\cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2
</code></pre>

<p>But when I ran </p>

<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf
hello = tf.constant('Hello, TensorFlow!')
sess = tf.Session()
print(sess.run(hello))
</code></pre>

<p>it compiled as it should and output <code>Hello, TensorFlow!</code>, which indicates that the installation was successful indeed but there is something else that is wrong.</p>

<p>Do you know what the problem is and how to fix it? Thanks.</p>
",
Tensorflow Relu Misunderstanding,"<p>I've recently been doing a Udacity Deep Learning course which is based around <code>TensorFlow</code>. I have a simple <code>MNIST</code> program which is about 92% accurate:</p>

<pre>
<code>
from tensorflow.examples.tutorials.mnist import input_data
import tensorflow as tf

mnist = input_data.read_data_sets(""MNIST_data/"", one_hot=True)

x = tf.placeholder(tf.float32, [None, 784])
W = tf.Variable(tf.zeros([784, 10]))
b = tf.Variable(tf.zeros([10]))

y = tf.nn.softmax(tf.matmul(x, W) + b)

y_ = tf.placeholder(tf.float32, [None, 10])
cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))

train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)

init = tf.initialize_all_variables()

sess = tf.Session()
sess.run(init)

for i in range(1000):
    batch_xs, batch_ys = mnist.train.next_batch(100)
    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})

correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))
accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
print(sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels})) 
</code>
</pre>

<p>My next assignment it to <code>Turn the logistic regression example with SGD into a 1-hidden layer neural network with rectified linear units nn.relu() and 1024 hidden nodes</code></p>

<p>I am having a mental block about this. Currently I have a 784 x 10 Matrix of weights, and a 10 element long bias vector. I don't understand how I connect the resulting 10 element vector from <code>WX + Bias</code> to 1024 <code>Relu</code>s.</p>

<p>If anyone could explain this to me I'd be very grateful.</p>
",
gensim Doc2Vec vs tensorflow Doc2Vec,"<p>I'm trying to compare my implementation of Doc2Vec (via tf) and gensims implementation. It seems atleast visually that the gensim ones are performing better.</p>

<p>I ran the following code to train the gensim model and the one below that for tensorflow model. My questions are as follows:</p>

<ol>
<li>Is my tf implementation of Doc2Vec correct. Basically is it supposed to be concatenating the word vectors and the document vector to predict the middle word in a certain context?</li>
<li>Does the <code>window=5</code> parameter in gensim mean that I am using two words on either side to predict the middle one? Or is it 5 on either side. Thing is there are quite a few documents that are smaller than length 10.</li>
<li>Any insights as to why Gensim is performing better? Is my model any different to how they implement it?</li>
<li>Considering that this is effectively a matrix factorisation problem, why is the TF model even getting an answer? There are infinite solutions to this since its a rank deficient problem. &lt;- This last question is simply a bonus.</li>
</ol>

<h3>Gensim</h3>

<pre><code>model = Doc2Vec(dm=1, dm_concat=1, size=100, window=5, negative=10, hs=0, min_count=2, workers=cores)
model.build_vocab(corpus)
epochs = 100
for i in range(epochs):
    model.train(corpus)
</code></pre>

<h3>TF</h3>

<pre><code>batch_size = 512
embedding_size = 100 # Dimension of the embedding vector.
num_sampled = 10 # Number of negative examples to sample.


graph = tf.Graph()

with graph.as_default(), tf.device('/cpu:0'):
    # Input data.
    train_word_dataset = tf.placeholder(tf.int32, shape=[batch_size])
    train_doc_dataset = tf.placeholder(tf.int32, shape=[batch_size/context_window])
    train_labels = tf.placeholder(tf.int32, shape=[batch_size/context_window, 1])

    # The variables   
    word_embeddings =  tf.Variable(tf.random_uniform([vocabulary_size,embedding_size],-1.0,1.0))
    doc_embeddings = tf.Variable(tf.random_uniform([len_docs,embedding_size],-1.0,1.0))
    softmax_weights = tf.Variable(tf.truncated_normal([vocabulary_size, (context_window+1)*embedding_size],
                             stddev=1.0 / np.sqrt(embedding_size)))
    softmax_biases = tf.Variable(tf.zeros([vocabulary_size]))

    ###########################
    # Model.
    ###########################
    # Look up embeddings for inputs and stack words side by side
    embed_words = tf.reshape(tf.nn.embedding_lookup(word_embeddings, train_word_dataset),
                            shape=[int(batch_size/context_window),-1])
    embed_docs = tf.nn.embedding_lookup(doc_embeddings, train_doc_dataset)
    embed = tf.concat(1,[embed_words, embed_docs])
    # Compute the softmax loss, using a sample of the negative labels each time.
    loss = tf.reduce_mean(tf.nn.sampled_softmax_loss(softmax_weights, softmax_biases, embed,
                                   train_labels, num_sampled, vocabulary_size))

    # Optimizer.
    optimizer = tf.train.AdagradOptimizer(1.0).minimize(loss)
</code></pre>

<h2>Update:</h2>

<p>Check out the jupyter notebook <a href=""https://github.com/sachinruk/doc2vec_tf"" rel=""noreferrer"">here</a> (I have both models working and tested in here). It still feels like the gensim model is performing better in this initial analysis.</p>
",
A3C in Tensorflow - Should I use threading or the distributed Tensorflow API,"<p>I want to implement the Asynchronous Advantage Actor Critic (A3C) model for reinforcement learning in my local machine (1 CPU, 1 cuda compatible GPU). In this algorithm, several ""learner"" networks interact with copies of an environment and update a central model periodically.</p>

<p>I've seen implementations that create n ""worker"" networks and one ""global"" network inside the same graph and use threading to run these. In these approaches, the global net is updated by applying gradients to the trainable parameters with a ""global"" scope.</p>

<p>However, I recently read a bit about distributed tensorflow and now I'm a bit confused. Would it be easier/faster/better to implement this using the distributed tensorflow API? In the documentation and talks they always make expicit mention of using it in multi-device environments. I don't know if it's an overkill to use it in a local async algorithm.</p>

<p>I would also like to ask, is there a way to batch the gradients calculated by every worker to be applied together after n steps?</p>
",
"dyld: Library not loaded: @rpath/libcudart.8.0.dylib, while building tensorflow on Mac OSX","<p>I am building tensorflow on my Mac(an hackintosh, so I have a GPU, and already installed CUDA8.0. It works fine with building caffe, so I am sure it works.) I have already set up the environment variables as following(I have put these in <code>.zshrc</code>,<code>.bash_profile</code> and <code>.bashrc</code>):</p>

<pre><code>export CUDA_HOME=/usr/local/cuda
export DYLD_LIBRARY_PATH=""$DYLD_LIBRARY_PATH:$CUDA_HOME/lib""
export PATH=""$CUDA_HOME/bin:$PATH""
export LD_LIBRARY_PATH=""$LD_LIBRARY_PATH:$CUDA_HOME/lib:$CUDA_HOME/extras/CUPTI/lib""
</code></pre>

<p><code>./configure</code> works fine. Then I start build using command <code>bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package</code>.
Then I got this error:</p>

<pre><code>    ERROR: /Development/tensorflow/tensorflow/python/BUILD:572:1: Executing genrule //tensorflow/python:array_ops_pygenrule failed: bash failed: error executing command /bin/bash -c ... (remaining 1 argument(s) skipped): com.google.devtools.build.lib.shell.AbnormalTerminationException: Process terminated by signal 5.
dyld: Library not loaded: @rpath/libcudart.8.0.dylib
  Referenced from: /private/var/tmp/_bazel_zarzen/bdf1cb43f3ff02468b610730bd03f348/execroot/tensorflow/bazel-out/host/bin/tensorflow/python/gen_array_ops_py_wrappers_cc
  Reason: image not found
/bin/bash: line 1: 92702 Trace/BPT trap: 5       bazel-out/host/bin/tensorflow/python/gen_array_ops_py_wrappers_cc @tensorflow/python/ops/hidden_ops.txt 1 &gt; bazel-out/local_darwin-opt/genfiles/tensorflow/python/ops/gen_array_ops.py
Target //tensorflow/tools/pip_package:build_pip_package failed to build
</code></pre>

<p>I can make sure the missed library is there. And I also tried install pre-built binary(I know it only support CUDA7.5, so I set up the PATH to point to CUDA7.5, but it doesn't work. when I try to <code>import tensorflow</code>, similar error <code>Library not loaded: @rpath/libcudart.7.5.dylib</code>, only version number changed).</p>

<p>I don't know why it cannot find the <code>lib</code>. Anyone can help? or any suggestions?</p>
",
"Keras uses way too much GPU memory when calling train_on_batch, fit, etc","<p>I've been messing with Keras, and like it so far. There's one big issue I have been having, when working with fairly deep networks: When calling model.train_on_batch, or model.fit etc., Keras allocates significantly more GPU memory than what the model itself should need. This is not caused by trying to train on some really large images, it's the network model itself that seems to require a lot of GPU memory. I have created this toy example to show what I mean. Here's essentially what's going on:</p>

<p>I first create a fairly deep network, and use model.summary() to get the total number of parameters needed for the network (in this case 206538153, which corresponds to about 826 MB). I then use nvidia-smi to see how much GPU memory Keras has allocated, and I can see that it makes perfect sense (849 MB).</p>

<p>I then compile the network, and can confirm that this does not increase GPU memory usage. And as we can see in this case, I have almost 1 GB of VRAM available at this point.</p>

<p>Then I try to feed a simple 16x16 image and a 1x1 ground truth to the network, and then everything blows up, because Keras starts allocating lots of memory again, for no reason that is obvious to me. Something about training the network seems to require a lot more memory than just having the model, which doesn't make sense to me. I have trained significantly deeper networks on this GPU in other frameworks, so that makes me think that I'm using Keras wrong (or there's something wrong in my setup, or in Keras, but of course that's hard to know for sure).</p>

<p>Here's the code:</p>



<pre class=""lang-python prettyprint-override""><code>from scipy import misc
import numpy as np
from keras.models import Sequential
from keras.layers import Dense, Activation, Convolution2D, MaxPooling2D, Reshape, Flatten, ZeroPadding2D, Dropout
import os

model = Sequential()

model.add(Convolution2D(256, 3, 3, border_mode='same', input_shape=(16,16,1)))
model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))
model.add(Convolution2D(512, 3, 3, border_mode='same'))
model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))
model.add(Convolution2D(1024, 3, 3, border_mode='same'))
model.add(Convolution2D(1024, 3, 3, border_mode='same'))
model.add(Convolution2D(1024, 3, 3, border_mode='same'))
model.add(Convolution2D(1024, 3, 3, border_mode='same'))
model.add(Convolution2D(1024, 3, 3, border_mode='same'))
model.add(Convolution2D(1024, 3, 3, border_mode='same'))
model.add(Convolution2D(1024, 3, 3, border_mode='same'))
model.add(Convolution2D(1024, 3, 3, border_mode='same'))
model.add(Convolution2D(1024, 3, 3, border_mode='same'))
model.add(Convolution2D(1024, 3, 3, border_mode='same'))
model.add(Convolution2D(1024, 3, 3, border_mode='same'))
model.add(Convolution2D(1024, 3, 3, border_mode='same'))
model.add(Convolution2D(1024, 3, 3, border_mode='same'))
model.add(Convolution2D(1024, 3, 3, border_mode='same'))
model.add(Convolution2D(1024, 3, 3, border_mode='same'))
model.add(Convolution2D(1024, 3, 3, border_mode='same'))
model.add(Convolution2D(1024, 3, 3, border_mode='same'))
model.add(Convolution2D(1024, 3, 3, border_mode='same'))
model.add(Convolution2D(1024, 3, 3, border_mode='same'))
model.add(Convolution2D(1024, 3, 3, border_mode='same'))
model.add(Convolution2D(1024, 3, 3, border_mode='same'))
model.add(Convolution2D(1024, 3, 3, border_mode='same'))
model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))
model.add(Convolution2D(256, 3, 3, border_mode='same'))
model.add(Convolution2D(32, 3, 3, border_mode='same'))
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Flatten())
model.add(Dense(4))
model.add(Dense(1))

model.summary()

os.system(""nvidia-smi"")
raw_input(""Press Enter to continue..."")    

model.compile(optimizer='sgd',
              loss='mse', 
              metrics=['accuracy'])

os.system(""nvidia-smi"")              
raw_input(""Compiled model. Press Enter to continue..."")

n_batches = 1
batch_size = 1
for ibatch in range(n_batches):
    x = np.random.rand(batch_size, 16,16,1)
    y = np.random.rand(batch_size, 1)

    os.system(""nvidia-smi"")
    raw_input(""About to train one iteration. Press Enter to continue..."")

    model.train_on_batch(x, y)         
    print(""Trained one iteration"")
</code></pre>

<p>Which gives the following output for me:</p>

<pre class=""lang-python prettyprint-override""><code>Using Theano backend.
Using gpu device 0: GeForce GTX 960 (CNMeM is disabled, cuDNN 5103)
/usr/local/lib/python2.7/dist-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.
  warnings.warn(warn)
____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
convolution2d_1 (Convolution2D)  (None, 16, 16, 256)   2560        convolution2d_input_1[0][0]      
____________________________________________________________________________________________________
maxpooling2d_1 (MaxPooling2D)    (None, 8, 8, 256)     0           convolution2d_1[0][0]            
____________________________________________________________________________________________________
convolution2d_2 (Convolution2D)  (None, 8, 8, 512)     1180160     maxpooling2d_1[0][0]             
____________________________________________________________________________________________________
maxpooling2d_2 (MaxPooling2D)    (None, 4, 4, 512)     0           convolution2d_2[0][0]            
____________________________________________________________________________________________________
convolution2d_3 (Convolution2D)  (None, 4, 4, 1024)    4719616     maxpooling2d_2[0][0]             
____________________________________________________________________________________________________
convolution2d_4 (Convolution2D)  (None, 4, 4, 1024)    9438208     convolution2d_3[0][0]            
____________________________________________________________________________________________________
convolution2d_5 (Convolution2D)  (None, 4, 4, 1024)    9438208     convolution2d_4[0][0]            
____________________________________________________________________________________________________
convolution2d_6 (Convolution2D)  (None, 4, 4, 1024)    9438208     convolution2d_5[0][0]            
____________________________________________________________________________________________________
convolution2d_7 (Convolution2D)  (None, 4, 4, 1024)    9438208     convolution2d_6[0][0]            
____________________________________________________________________________________________________
convolution2d_8 (Convolution2D)  (None, 4, 4, 1024)    9438208     convolution2d_7[0][0]            
____________________________________________________________________________________________________
convolution2d_9 (Convolution2D)  (None, 4, 4, 1024)    9438208     convolution2d_8[0][0]            
____________________________________________________________________________________________________
convolution2d_10 (Convolution2D) (None, 4, 4, 1024)    9438208     convolution2d_9[0][0]            
____________________________________________________________________________________________________
convolution2d_11 (Convolution2D) (None, 4, 4, 1024)    9438208     convolution2d_10[0][0]           
____________________________________________________________________________________________________
convolution2d_12 (Convolution2D) (None, 4, 4, 1024)    9438208     convolution2d_11[0][0]           
____________________________________________________________________________________________________
convolution2d_13 (Convolution2D) (None, 4, 4, 1024)    9438208     convolution2d_12[0][0]           
____________________________________________________________________________________________________
convolution2d_14 (Convolution2D) (None, 4, 4, 1024)    9438208     convolution2d_13[0][0]           
____________________________________________________________________________________________________
convolution2d_15 (Convolution2D) (None, 4, 4, 1024)    9438208     convolution2d_14[0][0]           
____________________________________________________________________________________________________
convolution2d_16 (Convolution2D) (None, 4, 4, 1024)    9438208     convolution2d_15[0][0]           
____________________________________________________________________________________________________
convolution2d_17 (Convolution2D) (None, 4, 4, 1024)    9438208     convolution2d_16[0][0]           
____________________________________________________________________________________________________
convolution2d_18 (Convolution2D) (None, 4, 4, 1024)    9438208     convolution2d_17[0][0]           
____________________________________________________________________________________________________
convolution2d_19 (Convolution2D) (None, 4, 4, 1024)    9438208     convolution2d_18[0][0]           
____________________________________________________________________________________________________
convolution2d_20 (Convolution2D) (None, 4, 4, 1024)    9438208     convolution2d_19[0][0]           
____________________________________________________________________________________________________
convolution2d_21 (Convolution2D) (None, 4, 4, 1024)    9438208     convolution2d_20[0][0]           
____________________________________________________________________________________________________
convolution2d_22 (Convolution2D) (None, 4, 4, 1024)    9438208     convolution2d_21[0][0]           
____________________________________________________________________________________________________
convolution2d_23 (Convolution2D) (None, 4, 4, 1024)    9438208     convolution2d_22[0][0]           
____________________________________________________________________________________________________
convolution2d_24 (Convolution2D) (None, 4, 4, 1024)    9438208     convolution2d_23[0][0]           
____________________________________________________________________________________________________
maxpooling2d_3 (MaxPooling2D)    (None, 2, 2, 1024)    0           convolution2d_24[0][0]           
____________________________________________________________________________________________________
convolution2d_25 (Convolution2D) (None, 2, 2, 256)     2359552     maxpooling2d_3[0][0]             
____________________________________________________________________________________________________
convolution2d_26 (Convolution2D) (None, 2, 2, 32)      73760       convolution2d_25[0][0]           
____________________________________________________________________________________________________
maxpooling2d_4 (MaxPooling2D)    (None, 1, 1, 32)      0           convolution2d_26[0][0]           
____________________________________________________________________________________________________
flatten_1 (Flatten)              (None, 32)            0           maxpooling2d_4[0][0]             
____________________________________________________________________________________________________
dense_1 (Dense)                  (None, 4)             132         flatten_1[0][0]                  
____________________________________________________________________________________________________
dense_2 (Dense)                  (None, 1)             5           dense_1[0][0]                    
====================================================================================================
Total params: 206538153
____________________________________________________________________________________________________
None
Thu Oct  6 09:05:42 2016       
+------------------------------------------------------+                       
| NVIDIA-SMI 352.63     Driver Version: 352.63         |                       
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GTX 960     Off  | 0000:01:00.0      On |                  N/A |
| 30%   37C    P2    28W / 120W |   1082MiB /  2044MiB |      9%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID  Type  Process name                               Usage      |
|=============================================================================|
|    0      1796    G   /usr/bin/X                                     155MiB |
|    0      2597    G   compiz                                          65MiB |
|    0      5966    C   python                                         849MiB |
+-----------------------------------------------------------------------------+
Press Enter to continue...
Thu Oct  6 09:05:44 2016       
+------------------------------------------------------+                       
| NVIDIA-SMI 352.63     Driver Version: 352.63         |                       
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GTX 960     Off  | 0000:01:00.0      On |                  N/A |
| 30%   38C    P2    28W / 120W |   1082MiB /  2044MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID  Type  Process name                               Usage      |
|=============================================================================|
|    0      1796    G   /usr/bin/X                                     155MiB |
|    0      2597    G   compiz                                          65MiB |
|    0      5966    C   python                                         849MiB |
+-----------------------------------------------------------------------------+
Compiled model. Press Enter to continue...
Thu Oct  6 09:05:44 2016       
+------------------------------------------------------+                       
| NVIDIA-SMI 352.63     Driver Version: 352.63         |                       
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GTX 960     Off  | 0000:01:00.0      On |                  N/A |
| 30%   38C    P2    28W / 120W |   1082MiB /  2044MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID  Type  Process name                               Usage      |
|=============================================================================|
|    0      1796    G   /usr/bin/X                                     155MiB |
|    0      2597    G   compiz                                          65MiB |
|    0      5966    C   python                                         849MiB |
+-----------------------------------------------------------------------------+
About to train one iteration. Press Enter to continue...
Error allocating 37748736 bytes of device memory (out of memory). Driver report 34205696 bytes free and 2144010240 bytes total 
Traceback (most recent call last):
  File ""memtest.py"", line 65, in &lt;module&gt;
    model.train_on_batch(x, y)         
  File ""/usr/local/lib/python2.7/dist-packages/keras/models.py"", line 712, in train_on_batch
    class_weight=class_weight)
  File ""/usr/local/lib/python2.7/dist-packages/keras/engine/training.py"", line 1221, in train_on_batch
    outputs = self.train_function(ins)
  File ""/usr/local/lib/python2.7/dist-packages/keras/backend/theano_backend.py"", line 717, in __call__
    return self.function(*inputs)
  File ""/usr/local/lib/python2.7/dist-packages/theano/compile/function_module.py"", line 871, in __call__
    storage_map=getattr(self.fn, 'storage_map', None))
  File ""/usr/local/lib/python2.7/dist-packages/theano/gof/link.py"", line 314, in raise_with_op
    reraise(exc_type, exc_value, exc_trace)
  File ""/usr/local/lib/python2.7/dist-packages/theano/compile/function_module.py"", line 859, in __call__
    outputs = self.fn()
MemoryError: Error allocating 37748736 bytes of device memory (out of memory).
Apply node that caused the error: GpuContiguous(GpuDimShuffle{3,2,0,1}.0)
Toposort index: 338
Inputs types: [CudaNdarrayType(float32, 4D)]
Inputs shapes: [(1024, 1024, 3, 3)]
Inputs strides: [(1, 1024, 3145728, 1048576)]
Inputs values: ['not shown']
Outputs clients: [[GpuDnnConv{algo='small', inplace=True}(GpuContiguous.0, GpuContiguous.0, GpuAllocEmpty.0, GpuDnnConvDesc{border_mode='half', subsample=(1, 1), conv_mode='conv', precision='float32'}.0, Constant{1.0}, Constant{0.0}), GpuDnnConvGradI{algo='none', inplace=True}(GpuContiguous.0, GpuContiguous.0, GpuAllocEmpty.0, GpuDnnConvDesc{border_mode='half', subsample=(1, 1), conv_mode='conv', precision='float32'}.0, Constant{1.0}, Constant{0.0})]]

HINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.
HINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.
</code></pre>

<p>A few things to note: </p>

<ul>
<li>I have tried both Theano and TensorFlow backends. Both have the same problems, and run out of memory at the same line. In TensorFlow, it seems that Keras preallocates a lot of memory (about 1.5 GB) so nvidia-smi doesn't help us track what's going on there, but I get the same out-of-memory exceptions. Again, this points towards an error in (my usage of) Keras (although it's hard to be certain about such things, it could be something with my setup).</li>
<li>I tried using CNMEM in Theano, which behaves like TensorFlow: It preallocates a large amount of memory (about 1.5 GB) yet crashes in the same place.</li>
<li>There are some warnings about the CudNN-version. I tried running the Theano backend with CUDA but not CudNN and I got the same errors, so that is not the source of the problem.</li>
<li>If you want to test this on your own GPU, you might want to make the network deeper/shallower depending on how much GPU memory you have to test this.</li>
<li>My configuration is as follows: Ubuntu 14.04, GeForce GTX 960, CUDA 7.5.18, CudNN 5.1.3, Python 2.7, Keras 1.1.0 (installed via pip)</li>
<li>I've tried changing the compilation of the model to use different optimizers and losses, but that doesn't seem to change anything.</li>
<li>I've tried changing the train_on_batch function to use fit instead, but it has the same problem.</li>
<li>I saw one similar question here on StackOverflow - <a href=""https://stackoverflow.com/questions/35757151/why-does-this-keras-model-require-over-6gb-of-memory"">Why does this Keras model require over 6GB of memory?</a> - but as far as I can tell, I don't have those issues in my configuration. I've never had multiple versions of CUDA installed, and I've double checked my PATH, LD_LIBRARY_PATH and CUDA_ROOT variables more times than I can count.</li>
<li>Julius suggested that the activation parameters themselves take up GPU memory. If this is true, can somebody explain it a bit more clearly? I have tried changing the activation function of my convolution layers to functions that are clearly hard-coded with no learnable parameters as far as I can tell, and that doesn't change anything. Also, it seems unlikely that these parameters would take up almost as much memory as the rest of the network itself.</li>
<li>After thorough testing, the largest network I can train is about 453 MB of parameters, out of my ~2 GB of GPU RAM. Is this normal? </li>
<li>After testing Keras on some smaller CNNs that do fit in my GPU, I can see that there are very sudden spikes in GPU RAM usage. If I run a network with about 100 MB of parameters, 99% of the time during training it'll be using less than 200 MB of GPU RAM. But every once in a while, memory usage spikes to about 1.3 GB. It seems safe to assume that it's these spikes that are causing my problems. I've never seen these spikes in other frameworks, but they might be there for a good reason? <strong>If anybody knows what causes them, and if there's a way to avoid them, please chime in!</strong></li>
</ul>
",
Recurrentshop and Keras: multi-dimensional RNN results in a dimensions mismatch error,"<p>I have an issue with Recurrentshop and Keras. I am trying to use Concatenate and multidimensional tensors in a Recurrent Model, and I get dimension issue regardless of how I arrange the  Input, shape and batch_shape.</p>

<p>Minimal code:</p>

<pre><code>from keras.layers import *
from keras.models import *
from recurrentshop import *
from keras.layers import Concatenate

input_shape=(128,128,3)

x_t = Input(shape=(128,128,3,))
h_tm1 = Input(shape=(128,128,3, ))

h_t1 = Concatenate()([x_t, h_tm1])
last = Conv2D(3, kernel_size=(3,3), strides=(1,1), padding='same',     name='conv2')(h_t1)

# Build the RNN
rnn = RecurrentModel(input=x_t, initial_states=[h_tm1], output=last,     final_states=[last], state_initializer=['zeros'])

x = Input(shape=(128,128,3, ))
y = rnn(x)

model = Model(x, y)

model.predict(np.random.random((1, 128, 128, 3)))
</code></pre>

<p>ErrorCode:</p>

<pre><code>ValueError: Shape must be rank 3 but it is rank 4 for 'recurrent_model_1/concatenate_1/concat' (op:ConcatV2) with input shapes: [?,128,3], [?,128,128,3], [].
</code></pre>

<p>Please help.</p>
",
How to make a custom activation function with only Python in Tensorflow?,"<p>Suppose you need to make an activation function which is not possible using only pre-defined tensorflow building-blocks, what can you do?</p>

<p>So in Tensorflow it is possible to make your own activation function. But it is quite complicated, you have to write it in C++ and recompile the whole of tensorflow <a href=""https://www.quora.com/Is-it-possible-to-add-new-activation-functions-to-TensorFlow-Theano-Torch-How"" rel=""noreferrer"">[1]</a> <a href=""https://www.tensorflow.org/versions/r0.11/how_tos/adding_an_op/index.html"" rel=""noreferrer"">[2]</a>.</p>

<p>Is there a simpler way?</p>
",
